{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3547, 0.7193],\n",
      "        [0.8262, 0.1727]])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as scio\n",
    "import hiddenlayer as h\n",
    "from visdom import Visdom\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchsummary import summary\n",
    "from torchviz import make_dot\n",
    "\n",
    "# 这里很普通的检查cuda可用性\n",
    "x = torch.rand(2,2)\n",
    "print(x)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n",
      "Setting up a new session...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021_12_16_15:17:33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n",
      "Setting up a new session...\n",
      "Setting up a new session...\n",
      "Setting up a new session...\n"
     ]
    }
   ],
   "source": [
    "ckpDir = './/ckp//c6addnewloss'\n",
    "if not os.path.exists(ckpDir):\n",
    "    os.makedirs(ckpDir)\n",
    "\n",
    "timeForSave = datetime.datetime.now().strftime('%Y_%m_%d_%H:%M:%S')\n",
    "print(timeForSave)\n",
    "vizx = 0\n",
    "viz = Visdom()\n",
    "viz1 = Visdom()\n",
    "viz2 = Visdom()\n",
    "viz3 = Visdom()\n",
    "viz4 = Visdom()\n",
    "viz5 = Visdom()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40, 10, 1, 2552)\n",
      "(2552, 1, 40, 10) \n",
      " (1106, 1, 40, 10) \n",
      " (1722, 1, 40, 10) \n",
      " (1361, 1, 40, 10)\n",
      "(2552, 1) \n",
      " (1106, 1) \n",
      " (1722, 1) \n",
      " (1361, 1)\n",
      "(2552, 1) \n",
      " 2552 0\n"
     ]
    }
   ],
   "source": [
    "# 以下两块是mat转numpy用的，训练时不需要运行，在此注掉了\n",
    "\n",
    "datas = scio.loadmat('../data/Forpytorch20211210T220441.mat')\n",
    "print(datas['segTR_X'].shape)\n",
    "Xtrain = np.transpose(datas['segTR_X'],[3,2,0,1])\n",
    "Xtest = np.transpose(datas['segTE_X'],[3,2,0,1])\n",
    "Xtest_o = np.transpose(datas['segTE_Xo'],[3,2,0,1])\n",
    "Xtrain_unknown = np.transpose(datas['segTR_Xo'],[3,2,0,1])\n",
    "Ytrain_unknown = datas['segTR_Yo']\n",
    "print(Xtrain.shape,'\\n',Xtest.shape,'\\n',\\\n",
    "    Xtest_o.shape,'\\n',Xtrain_unknown.shape)\n",
    "\n",
    "# 标签\n",
    "# print(datas1[0,0,:,:])\n",
    "Ytrain = datas['segTR_Y']\n",
    "Ytest = datas['segTE_Y']\n",
    "Ytest_o = datas['segTE_Yo']\n",
    "print(Ytrain.shape,'\\n',Ytest.shape,'\\n',\\\n",
    "    Ytest_o.shape,'\\n',Ytrain_unknown.shape)\n",
    "print(Ytrain.shape,'\\n',len(Ytrain),Ytrain[1,0])\n",
    "# print(len(datalb))\n",
    "# print(datas1[0,0,:,:])\n",
    "\n",
    "# print(datas['Y_TrainP'].ndim)\n",
    "# datalb = datas['Y_TrainP']\n",
    "# print(len(datalb))\n",
    "# print(datalb[1,0])\n",
    "# emg1 = datalb[1,0]\n",
    "# type(emg1)\n",
    "# emg1.astype(np.float32)\n",
    "# np.save('../data/trainY.npy',datalb)\n",
    "# print(datalb[1,0])\n",
    "dataset = {}\n",
    "dataset['Ytrain'] = Ytrain\n",
    "dataset['Xtrain'] = Xtrain\n",
    "dataset['Xtest'] = Xtest\n",
    "dataset['Xtest_o'] = Xtest_o\n",
    "dataset['Xtrain_unknown'] = Xtrain_unknown\n",
    "dataset['Ytest'] = Ytest\n",
    "dataset['Ytest_o'] = Ytest_o\n",
    "dataset['Ytrain_unknown'] = Ytrain_unknown\n",
    "np.save('../data/OpenSetDataSet1210.npy',dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('../data/trainX.npy',datas1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1Ytrain-20210721T193346.mat\n",
    "\n",
    "# datas = scio.loadmat('../data/Y_TrainP.mat')\n",
    "# print(datas['Y_TrainP'].ndim)\n",
    "# datalb = datas['Y_TrainP']\n",
    "# print(len(datalb))\n",
    "# print(datalb[1,0])\n",
    "# emg1 = datalb[1,0]\n",
    "# type(emg1)\n",
    "# emg1.astype(np.float32)\n",
    "# np.save('../data/trainY.npy',datalb)\n",
    "# print(datalb[1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里检查了数据形状与维度，训练时不需要，注掉了\n",
    "\n",
    "# tx = np.load('../data/trainX.npy')\n",
    "# txp = tx[1,:,:,:]\n",
    "# print(txp.shape)\n",
    "# print(np.squeeze(txp).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "(tensor([[[0.0269, 0.0049, 0.0024, 0.0024, 0.0024, 0.0024, 0.1147, 0.0806,\n",
      "          0.0024, 0.0146],\n",
      "         [0.0342, 0.0171, 0.0024, 0.0024, 0.0024, 0.0024, 0.1221, 0.0781,\n",
      "          0.0024, 0.0098],\n",
      "         [0.0391, 0.0171, 0.0024, 0.0024, 0.0024, 0.0024, 0.1147, 0.0732,\n",
      "          0.0024, 0.0049],\n",
      "         [0.0488, 0.0146, 0.0024, 0.0024, 0.0024, 0.0024, 0.1001, 0.0659,\n",
      "          0.0024, 0.0024],\n",
      "         [0.0464, 0.0122, 0.0024, 0.0024, 0.0049, 0.0024, 0.0830, 0.0610,\n",
      "          0.0024, 0.0024],\n",
      "         [0.0464, 0.0073, 0.0024, 0.0024, 0.0024, 0.0024, 0.0757, 0.0562,\n",
      "          0.0024, 0.0024],\n",
      "         [0.0439, 0.0244, 0.0024, 0.0024, 0.0024, 0.0024, 0.0732, 0.0562,\n",
      "          0.0024, 0.0073],\n",
      "         [0.0488, 0.0415, 0.0024, 0.0024, 0.0024, 0.0024, 0.0732, 0.0537,\n",
      "          0.0024, 0.0098],\n",
      "         [0.0586, 0.0635, 0.0024, 0.0024, 0.0024, 0.0024, 0.0952, 0.0562,\n",
      "          0.0024, 0.0122],\n",
      "         [0.0537, 0.1196, 0.0049, 0.0024, 0.0024, 0.0024, 0.1025, 0.0537,\n",
      "          0.0098, 0.0098],\n",
      "         [0.0488, 0.1538, 0.0073, 0.0024, 0.0024, 0.0024, 0.0952, 0.0513,\n",
      "          0.0610, 0.0073],\n",
      "         [0.0391, 0.1538, 0.0049, 0.0024, 0.0024, 0.0024, 0.0806, 0.0464,\n",
      "          0.0806, 0.0098],\n",
      "         [0.0317, 0.1367, 0.0024, 0.0024, 0.0024, 0.0024, 0.0732, 0.0415,\n",
      "          0.0806, 0.0073],\n",
      "         [0.0317, 0.1123, 0.0024, 0.0024, 0.0024, 0.0024, 0.0659, 0.0439,\n",
      "          0.0732, 0.0049],\n",
      "         [0.0269, 0.0928, 0.0024, 0.0024, 0.0024, 0.0024, 0.0586, 0.0439,\n",
      "          0.0586, 0.0024],\n",
      "         [0.0244, 0.0708, 0.0024, 0.0024, 0.0024, 0.0024, 0.0757, 0.0439,\n",
      "          0.0488, 0.0024],\n",
      "         [0.0220, 0.0537, 0.0122, 0.0024, 0.0024, 0.0024, 0.1025, 0.0464,\n",
      "          0.0366, 0.0024],\n",
      "         [0.0195, 0.0391, 0.0220, 0.0024, 0.0024, 0.0024, 0.1294, 0.0439,\n",
      "          0.0293, 0.0024],\n",
      "         [0.0146, 0.0269, 0.0195, 0.0024, 0.0024, 0.0024, 0.1294, 0.0464,\n",
      "          0.0171, 0.0024],\n",
      "         [0.0122, 0.0146, 0.0146, 0.0024, 0.0024, 0.0024, 0.1196, 0.0488,\n",
      "          0.0098, 0.0024],\n",
      "         [0.0073, 0.0024, 0.0049, 0.0024, 0.0024, 0.0024, 0.1099, 0.0464,\n",
      "          0.0024, 0.0024],\n",
      "         [0.0122, 0.0024, 0.0024, 0.0024, 0.0024, 0.0024, 0.0952, 0.0464,\n",
      "          0.0024, 0.0024],\n",
      "         [0.0146, 0.0024, 0.0024, 0.0024, 0.0024, 0.0024, 0.0903, 0.0439,\n",
      "          0.0024, 0.0024],\n",
      "         [0.0293, 0.0220, 0.0024, 0.0024, 0.0024, 0.0024, 0.0879, 0.0415,\n",
      "          0.0269, 0.0024],\n",
      "         [0.0366, 0.0684, 0.0049, 0.0024, 0.0024, 0.0024, 0.1270, 0.0439,\n",
      "          0.0952, 0.0024],\n",
      "         [0.0366, 0.1001, 0.0024, 0.0024, 0.0024, 0.0024, 0.1440, 0.0439,\n",
      "          0.1465, 0.0024],\n",
      "         [0.0415, 0.1123, 0.0146, 0.0024, 0.0024, 0.0024, 0.1538, 0.0464,\n",
      "          0.1563, 0.0024],\n",
      "         [0.0391, 0.1050, 0.0220, 0.0024, 0.0024, 0.0024, 0.1587, 0.0464,\n",
      "          0.1465, 0.0024],\n",
      "         [0.0537, 0.0928, 0.0366, 0.0024, 0.0024, 0.0024, 0.1489, 0.0464,\n",
      "          0.1270, 0.0024],\n",
      "         [0.0586, 0.0806, 0.0586, 0.0024, 0.0024, 0.0024, 0.1392, 0.0415,\n",
      "          0.1050, 0.0024],\n",
      "         [0.0537, 0.0684, 0.0708, 0.0024, 0.0024, 0.0024, 0.1343, 0.0415,\n",
      "          0.0830, 0.0024],\n",
      "         [0.0513, 0.0610, 0.0806, 0.0024, 0.0024, 0.0024, 0.1587, 0.0488,\n",
      "          0.0684, 0.0024],\n",
      "         [0.0464, 0.1074, 0.0806, 0.0024, 0.0024, 0.0024, 0.1636, 0.0586,\n",
      "          0.0635, 0.0024],\n",
      "         [0.0391, 0.1465, 0.0732, 0.0024, 0.0024, 0.0024, 0.1538, 0.0610,\n",
      "          0.0830, 0.0024],\n",
      "         [0.0464, 0.1514, 0.0635, 0.0024, 0.0024, 0.0024, 0.1367, 0.0586,\n",
      "          0.0854, 0.0024],\n",
      "         [0.0586, 0.1392, 0.0635, 0.0024, 0.0024, 0.0024, 0.1147, 0.0537,\n",
      "          0.0757, 0.0024],\n",
      "         [0.0610, 0.1196, 0.0708, 0.0024, 0.0024, 0.0024, 0.1392, 0.0562,\n",
      "          0.0659, 0.0049],\n",
      "         [0.0562, 0.1123, 0.0854, 0.0024, 0.0024, 0.0024, 0.1611, 0.0659,\n",
      "          0.0537, 0.0024],\n",
      "         [0.0513, 0.1074, 0.0928, 0.0024, 0.0024, 0.0024, 0.1758, 0.0684,\n",
      "          0.0439, 0.0024],\n",
      "         [0.0464, 0.0977, 0.1001, 0.0049, 0.0024, 0.0024, 0.1636, 0.0708,\n",
      "          0.0342, 0.0024]]], dtype=torch.float64), 0)\n",
      "(tensor([[[0.0171, 0.0073, 0.0024, 0.0024, 0.0024, 0.0024, 0.0757, 0.0317,\n",
      "          0.0024, 0.0146],\n",
      "         [0.0171, 0.0244, 0.0024, 0.0024, 0.0024, 0.0024, 0.0684, 0.0342,\n",
      "          0.0024, 0.0146],\n",
      "         [0.0146, 0.0269, 0.0024, 0.0024, 0.0024, 0.0024, 0.0610, 0.0342,\n",
      "          0.0024, 0.0146],\n",
      "         [0.0220, 0.0195, 0.0024, 0.0024, 0.0024, 0.0024, 0.0513, 0.0366,\n",
      "          0.0024, 0.0098],\n",
      "         [0.0269, 0.0171, 0.0024, 0.0024, 0.0024, 0.0024, 0.0488, 0.0415,\n",
      "          0.0024, 0.0073],\n",
      "         [0.0269, 0.0293, 0.0024, 0.0024, 0.0024, 0.0024, 0.0732, 0.0488,\n",
      "          0.0024, 0.0073],\n",
      "         [0.0244, 0.0317, 0.0024, 0.0024, 0.0024, 0.0024, 0.1001, 0.0635,\n",
      "          0.0024, 0.0098],\n",
      "         [0.0220, 0.0244, 0.0024, 0.0024, 0.0024, 0.0024, 0.1074, 0.0757,\n",
      "          0.0024, 0.0195],\n",
      "         [0.0171, 0.0146, 0.0024, 0.0024, 0.0024, 0.0024, 0.1172, 0.0854,\n",
      "          0.0024, 0.0220],\n",
      "         [0.0195, 0.0049, 0.0024, 0.0024, 0.0024, 0.0024, 0.1147, 0.0854,\n",
      "          0.0024, 0.0220],\n",
      "         [0.0269, 0.0049, 0.0024, 0.0024, 0.0024, 0.0024, 0.1147, 0.0806,\n",
      "          0.0024, 0.0146],\n",
      "         [0.0342, 0.0171, 0.0024, 0.0024, 0.0024, 0.0024, 0.1221, 0.0781,\n",
      "          0.0024, 0.0098],\n",
      "         [0.0391, 0.0171, 0.0024, 0.0024, 0.0024, 0.0024, 0.1147, 0.0732,\n",
      "          0.0024, 0.0049],\n",
      "         [0.0488, 0.0146, 0.0024, 0.0024, 0.0024, 0.0024, 0.1001, 0.0659,\n",
      "          0.0024, 0.0024],\n",
      "         [0.0464, 0.0122, 0.0024, 0.0024, 0.0049, 0.0024, 0.0830, 0.0610,\n",
      "          0.0024, 0.0024],\n",
      "         [0.0464, 0.0073, 0.0024, 0.0024, 0.0024, 0.0024, 0.0757, 0.0562,\n",
      "          0.0024, 0.0024],\n",
      "         [0.0439, 0.0244, 0.0024, 0.0024, 0.0024, 0.0024, 0.0732, 0.0562,\n",
      "          0.0024, 0.0073],\n",
      "         [0.0488, 0.0415, 0.0024, 0.0024, 0.0024, 0.0024, 0.0732, 0.0537,\n",
      "          0.0024, 0.0098],\n",
      "         [0.0586, 0.0635, 0.0024, 0.0024, 0.0024, 0.0024, 0.0952, 0.0562,\n",
      "          0.0024, 0.0122],\n",
      "         [0.0537, 0.1196, 0.0049, 0.0024, 0.0024, 0.0024, 0.1025, 0.0537,\n",
      "          0.0098, 0.0098],\n",
      "         [0.0488, 0.1538, 0.0073, 0.0024, 0.0024, 0.0024, 0.0952, 0.0513,\n",
      "          0.0610, 0.0073],\n",
      "         [0.0391, 0.1538, 0.0049, 0.0024, 0.0024, 0.0024, 0.0806, 0.0464,\n",
      "          0.0806, 0.0098],\n",
      "         [0.0317, 0.1367, 0.0024, 0.0024, 0.0024, 0.0024, 0.0732, 0.0415,\n",
      "          0.0806, 0.0073],\n",
      "         [0.0317, 0.1123, 0.0024, 0.0024, 0.0024, 0.0024, 0.0659, 0.0439,\n",
      "          0.0732, 0.0049],\n",
      "         [0.0269, 0.0928, 0.0024, 0.0024, 0.0024, 0.0024, 0.0586, 0.0439,\n",
      "          0.0586, 0.0024],\n",
      "         [0.0244, 0.0708, 0.0024, 0.0024, 0.0024, 0.0024, 0.0757, 0.0439,\n",
      "          0.0488, 0.0024],\n",
      "         [0.0220, 0.0537, 0.0122, 0.0024, 0.0024, 0.0024, 0.1025, 0.0464,\n",
      "          0.0366, 0.0024],\n",
      "         [0.0195, 0.0391, 0.0220, 0.0024, 0.0024, 0.0024, 0.1294, 0.0439,\n",
      "          0.0293, 0.0024],\n",
      "         [0.0146, 0.0269, 0.0195, 0.0024, 0.0024, 0.0024, 0.1294, 0.0464,\n",
      "          0.0171, 0.0024],\n",
      "         [0.0122, 0.0146, 0.0146, 0.0024, 0.0024, 0.0024, 0.1196, 0.0488,\n",
      "          0.0098, 0.0024],\n",
      "         [0.0073, 0.0024, 0.0049, 0.0024, 0.0024, 0.0024, 0.1099, 0.0464,\n",
      "          0.0024, 0.0024],\n",
      "         [0.0122, 0.0024, 0.0024, 0.0024, 0.0024, 0.0024, 0.0952, 0.0464,\n",
      "          0.0024, 0.0024],\n",
      "         [0.0146, 0.0024, 0.0024, 0.0024, 0.0024, 0.0024, 0.0903, 0.0439,\n",
      "          0.0024, 0.0024],\n",
      "         [0.0293, 0.0220, 0.0024, 0.0024, 0.0024, 0.0024, 0.0879, 0.0415,\n",
      "          0.0269, 0.0024],\n",
      "         [0.0366, 0.0684, 0.0049, 0.0024, 0.0024, 0.0024, 0.1270, 0.0439,\n",
      "          0.0952, 0.0024],\n",
      "         [0.0366, 0.1001, 0.0024, 0.0024, 0.0024, 0.0024, 0.1440, 0.0439,\n",
      "          0.1465, 0.0024],\n",
      "         [0.0415, 0.1123, 0.0146, 0.0024, 0.0024, 0.0024, 0.1538, 0.0464,\n",
      "          0.1563, 0.0024],\n",
      "         [0.0391, 0.1050, 0.0220, 0.0024, 0.0024, 0.0024, 0.1587, 0.0464,\n",
      "          0.1465, 0.0024],\n",
      "         [0.0537, 0.0928, 0.0366, 0.0024, 0.0024, 0.0024, 0.1489, 0.0464,\n",
      "          0.1270, 0.0024],\n",
      "         [0.0586, 0.0806, 0.0586, 0.0024, 0.0024, 0.0024, 0.1392, 0.0415,\n",
      "          0.1050, 0.0024]]], dtype=torch.float64), 0)\n"
     ]
    }
   ],
   "source": [
    "# 自定义数据集类\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # 将图片转换为Tensor,归一化至[0,1]\n",
    "])\n",
    "\n",
    "class EMGDataset(Dataset):\n",
    " \n",
    "    def __init__(self, data, label):\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "        self.transforms = transform\n",
    " \n",
    "    def __getitem__(self, index):\n",
    "        emgData = self.data[index,:,:,:]\n",
    "        emgData = np.squeeze(emgData)#似乎不应该压缩了\n",
    "        emglabel = self.label[index]\n",
    "        emglabel = emglabel.astype(np.int16)\n",
    "        emgData = self.transforms(emgData)      \n",
    "        \n",
    "        return emgData,emglabel\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    " \n",
    " \n",
    "# if __name__ == '__main__':\n",
    "dataarray = np.load('../data/OpenSetDataSet1210.npy',allow_pickle=True)\n",
    "CNNdataset = dataarray.item()\n",
    "print(type(CNNdataset))\n",
    "traindata = CNNdataset['Xtrain']\n",
    "trainlabel = CNNdataset['Ytrain']\n",
    "testdata = CNNdataset['Xtest']\n",
    "testlabel = CNNdataset['Ytest']\n",
    "trainunknown_data = CNNdataset['Xtrain_unknown']\n",
    "trainunknownc_label = CNNdataset['Ytrain_unknown']\n",
    "# # print(trainlabel[:,0])\n",
    "\n",
    "trainlabel = trainlabel[:,0]\n",
    "testlabel = testlabel[:,0]\n",
    "trainunknownc_label = trainunknownc_label[:,0]\n",
    "# print(type(trainlabel))\n",
    "train_set = EMGDataset(traindata, trainlabel)\n",
    "test_set = EMGDataset(testdata, testlabel)\n",
    "train_unknown = EMGDataset(trainunknown_data,trainunknownc_label)\n",
    "# train_loader = torch.utils.data.DataLoader(train_set, batch_size=1, shuffle=True, pin_memory=True,\n",
    "#                                             num_workers=3)\n",
    "\n",
    "sample = next(iter(train_set))\n",
    "print(sample)\n",
    "sample = next(iter(test_set))\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=4352, out_features=128, bias=True)\n",
      "  (out): Linear(in_features=128, out_features=6, bias=True)\n",
      "  (dr1): Dropout2d(p=0.2, inplace=False)\n",
      ")\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1             [1, 32, 38, 8]             320\n",
      "            Conv2d-2             [1, 32, 35, 5]           9,248\n",
      "            Linear-3                   [1, 128]         557,184\n",
      "         Dropout2d-4                   [1, 128]               0\n",
      "            Linear-5                     [1, 6]             774\n",
      "================================================================\n",
      "Total params: 567,526\n",
      "Trainable params: 567,526\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.12\n",
      "Params size (MB): 2.16\n",
      "Estimated Total Size (MB): 2.29\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Digraph.gv.png'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 自定义神经网络\n",
    "\n",
    "def get_num_correct(preds, labels):\n",
    "    return preds.argmax(dim=1).eq(labels).sum().item()\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3)\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3)\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=32 * 4 * 34, out_features=128)\n",
    "        self.out = nn.Linear(in_features=128, out_features=6)\n",
    "        self.dr1 = nn.Dropout2d(0.2)\n",
    "\n",
    "    def forward(self, t):\n",
    "        # (1) input layer\n",
    "        t = t\n",
    "\n",
    "        # (2) hidden conv layer\n",
    "        t = self.conv1(t)\n",
    "        t = F.relu(t)\n",
    "        t = F.max_pool2d(t, kernel_size=2, stride=1)\n",
    "\n",
    "        # (3) hidden conv layer\n",
    "        t = self.conv2(t)\n",
    "        t = F.relu(t)\n",
    "        # t = self.dr1(t)\n",
    "        t = F.max_pool2d(t, kernel_size=2, stride=1)\n",
    "\n",
    "        # (4) hidden linear layer\n",
    "        t = t.reshape(-1, 32 * 4 * 34)\n",
    "        t = self.fc1(t)\n",
    "        t = F.relu(t)\n",
    "        t = self.dr1(t)\n",
    "\n",
    "        # (5) output layer\n",
    "        t = self.out(t)\n",
    "\n",
    "        return t\n",
    "        \n",
    "net = Network()\n",
    "# 打印网络，检查输入输出 shape是否正确\n",
    "print(net)\n",
    "summary(net,(1,40,10),batch_size = 1,device = \"cpu\")\n",
    "\n",
    "# 可视化结构，hiddenlayer\n",
    "# vis_graph = h.build_graph(net, torch.zeros([1,1,40,10]))\n",
    "# vis_graph.theme = h.graph.THEMES[\"blue\"].copy()\n",
    "# vis_graph.save(\"./CNNtrynetframe\")\n",
    "# 可视化结构，torchviz\n",
    "sampleInput = torch.randn(1,1,40,10).requires_grad_(True)\n",
    "sampleOutput = net(sampleInput)\n",
    "framevision = make_dot(sampleOutput, params=dict(list(net.named_parameters()) + [('x',sampleInput)]))\n",
    "framevision.format = \"png\"\n",
    "framevision.direcory = \"./\"\n",
    "framevision.view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2552\n"
     ]
    }
   ],
   "source": [
    "# import datetime\n",
    "\n",
    "# timeForSave = datetime.datetime.now().strftime('%Y_%m_%d_%H:%M:%S')\n",
    "# print(timeForSave)\n",
    "print(trainlabel.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 total_train_acc: 0.7476489028213166 loss: 108599.28515625 test_loss: 247.7643280029297 test_acc: 0.7269439421338155\n",
      "epoch 1 total_train_acc: 0.7496081504702194 loss: 92231.353515625 test_loss: 2461.086669921875 test_acc: 0.72875226039783\n",
      "epoch 2 total_train_acc: 0.747257053291536 loss: 81486.6328125 test_loss: 3272.078369140625 test_acc: 0.7260397830018083\n",
      "epoch 3 total_train_acc: 0.7405956112852664 loss: 71442.57421875 test_loss: 458.6404724121094 test_acc: 0.716998191681736\n",
      "epoch 4 total_train_acc: 0.7323667711598746 loss: 65891.263671875 test_loss: 1485.3663330078125 test_acc: 0.7079566003616636\n",
      "epoch 5 total_train_acc: 0.7304075235109718 loss: 59069.28515625 test_loss: 3153.973876953125 test_acc: 0.701627486437613\n",
      "epoch 6 total_train_acc: 0.7300156739811913 loss: 52540.7236328125 test_loss: 3062.457275390625 test_acc: 0.6934900542495479\n",
      "epoch 7 total_train_acc: 0.7159090909090909 loss: 47022.294921875 test_loss: 5387.14404296875 test_acc: 0.6862567811934901\n",
      "epoch 8 total_train_acc: 0.7096394984326019 loss: 43150.3564453125 test_loss: 3942.824951171875 test_acc: 0.6862567811934901\n",
      "epoch 9 total_train_acc: 0.7139498432601881 loss: 39938.552734375 test_loss: 0.0 test_acc: 0.6916817359855335\n",
      "epoch 10 total_train_acc: 0.695141065830721 loss: 36128.6748046875 test_loss: 2820.185302734375 test_acc: 0.6862567811934901\n",
      "epoch 11 total_train_acc: 0.6884796238244514 loss: 33785.693359375 test_loss: 5690.08544921875 test_acc: 0.6871609403254972\n",
      "epoch 12 total_train_acc: 0.6916144200626959 loss: 31494.251953125 test_loss: 4507.43310546875 test_acc: 0.6889692585895117\n",
      "epoch 13 total_train_acc: 0.6927899686520376 loss: 28974.095703125 test_loss: 5600.17529296875 test_acc: 0.6835443037974683\n",
      "epoch 14 total_train_acc: 0.6865203761755486 loss: 27969.0126953125 test_loss: 4042.669189453125 test_acc: 0.6826401446654611\n",
      "epoch 15 total_train_acc: 0.6908307210031348 loss: 25756.67919921875 test_loss: 381.4851989746094 test_acc: 0.6835443037974683\n",
      "epoch 16 total_train_acc: 0.6857366771159875 loss: 24707.7353515625 test_loss: 559.2799682617188 test_acc: 0.6808318264014467\n",
      "epoch 17 total_train_acc: 0.6747648902821317 loss: 23598.658203125 test_loss: 1529.9793701171875 test_acc: 0.6799276672694394\n",
      "epoch 18 total_train_acc: 0.682601880877743 loss: 23792.9404296875 test_loss: 25213.263671875 test_acc: 0.6853526220614828\n",
      "epoch 19 total_train_acc: 0.6681034482758621 loss: 22500.3349609375 test_loss: 4804.8037109375 test_acc: 0.6889692585895117\n",
      "epoch 20 total_train_acc: 0.6669278996865203 loss: 20972.9833984375 test_loss: 1625.7149658203125 test_acc: 0.6871609403254972\n",
      "epoch 21 total_train_acc: 0.6728056426332288 loss: 20273.2060546875 test_loss: 2085.08056640625 test_acc: 0.6835443037974683\n",
      "epoch 22 total_train_acc: 0.6728056426332288 loss: 19926.74072265625 test_loss: 6084.59375 test_acc: 0.6853526220614828\n",
      "epoch 23 total_train_acc: 0.6704545454545454 loss: 18736.65771484375 test_loss: 4186.81396484375 test_acc: 0.6826401446654611\n",
      "epoch 24 total_train_acc: 0.6649686520376176 loss: 18846.84765625 test_loss: 3476.720703125 test_acc: 0.6835443037974683\n",
      "epoch 25 total_train_acc: 0.6688871473354232 loss: 18168.61279296875 test_loss: 0.0 test_acc: 0.6826401446654611\n",
      "epoch 26 total_train_acc: 0.6720219435736677 loss: 17062.3759765625 test_loss: 1709.4498291015625 test_acc: 0.6844484629294756\n",
      "epoch 27 total_train_acc: 0.6661442006269592 loss: 16610.67138671875 test_loss: 10480.4169921875 test_acc: 0.6844484629294756\n",
      "epoch 28 total_train_acc: 0.6641849529780565 loss: 15715.20703125 test_loss: 338.8751525878906 test_acc: 0.6907775768535263\n",
      "epoch 29 total_train_acc: 0.67358934169279 loss: 16329.64111328125 test_loss: 1807.5191650390625 test_acc: 0.6916817359855335\n",
      "epoch 30 total_train_acc: 0.6778996865203761 loss: 15672.67919921875 test_loss: 1616.8433837890625 test_acc: 0.689873417721519\n",
      "epoch 31 total_train_acc: 0.658307210031348 loss: 14739.55859375 test_loss: 525.1544799804688 test_acc: 0.689873417721519\n",
      "epoch 32 total_train_acc: 0.661833855799373 loss: 14410.12646484375 test_loss: 7704.22021484375 test_acc: 0.6871609403254972\n",
      "epoch 33 total_train_acc: 0.6814263322884012 loss: 13402.76904296875 test_loss: 6822.33447265625 test_acc: 0.6853526220614828\n",
      "epoch 34 total_train_acc: 0.6747648902821317 loss: 13558.484375 test_loss: 1472.6800537109375 test_acc: 0.6880650994575045\n",
      "epoch 35 total_train_acc: 0.677115987460815 loss: 13462.9609375 test_loss: 442.9129333496094 test_acc: 0.6943942133815552\n",
      "epoch 36 total_train_acc: 0.674373040752351 loss: 13117.636962890625 test_loss: 418.5608825683594 test_acc: 0.6952983725135624\n",
      "epoch 37 total_train_acc: 0.6763322884012539 loss: 12184.423583984375 test_loss: 351.218994140625 test_acc: 0.6952983725135624\n",
      "epoch 38 total_train_acc: 0.679858934169279 loss: 12364.7724609375 test_loss: 2199.25537109375 test_acc: 0.701627486437613\n",
      "epoch 39 total_train_acc: 0.6845611285266457 loss: 11927.853759765625 test_loss: 2376.03466796875 test_acc: 0.7007233273056058\n",
      "epoch 40 total_train_acc: 0.67358934169279 loss: 12205.9423828125 test_loss: 2253.361572265625 test_acc: 0.7025316455696202\n",
      "epoch 41 total_train_acc: 0.6731974921630094 loss: 11899.8759765625 test_loss: 509.7464294433594 test_acc: 0.7061482820976492\n",
      "epoch 42 total_train_acc: 0.6786833855799373 loss: 11010.900390625 test_loss: 10417.6806640625 test_acc: 0.7097649186256781\n",
      "epoch 43 total_train_acc: 0.6833855799373041 loss: 11459.52490234375 test_loss: 1086.1656494140625 test_acc: 0.7088607594936709\n",
      "epoch 44 total_train_acc: 0.6802507836990596 loss: 11106.1669921875 test_loss: 1199.8697509765625 test_acc: 0.7124773960216998\n",
      "epoch 45 total_train_acc: 0.6822100313479624 loss: 10747.166015625 test_loss: 691.9838256835938 test_acc: 0.7179023508137432\n",
      "epoch 46 total_train_acc: 0.6845611285266457 loss: 10532.07080078125 test_loss: 2652.174072265625 test_acc: 0.720614828209765\n",
      "epoch 47 total_train_acc: 0.6939655172413793 loss: 10593.687255859375 test_loss: 444.85693359375 test_acc: 0.7215189873417721\n",
      "epoch 48 total_train_acc: 0.6786833855799373 loss: 10010.825439453125 test_loss: 6314.58056640625 test_acc: 0.7278481012658228\n",
      "epoch 49 total_train_acc: 0.6982758620689655 loss: 9721.39990234375 test_loss: 1482.2144775390625 test_acc: 0.72875226039783\n",
      "epoch 50 total_train_acc: 0.6916144200626959 loss: 9580.801513671875 test_loss: 859.5134887695312 test_acc: 0.7278481012658228\n",
      "epoch 51 total_train_acc: 0.6657523510971787 loss: 9085.433837890625 test_loss: 4982.46923828125 test_acc: 0.7359855334538878\n",
      "epoch 52 total_train_acc: 0.6849529780564263 loss: 8957.0693359375 test_loss: 440.033203125 test_acc: 0.7414104882459313\n",
      "epoch 53 total_train_acc: 0.6967084639498433 loss: 8959.3095703125 test_loss: 7148.96484375 test_acc: 0.740506329113924\n",
      "epoch 54 total_train_acc: 0.6888714733542319 loss: 8854.220703125 test_loss: 237.4794921875 test_acc: 0.7423146473779385\n",
      "epoch 55 total_train_acc: 0.692398119122257 loss: 8828.95166015625 test_loss: 2073.103759765625 test_acc: 0.7459312839059674\n",
      "epoch 56 total_train_acc: 0.6931818181818182 loss: 8071.828125 test_loss: 375.001953125 test_acc: 0.7477396021699819\n",
      "epoch 57 total_train_acc: 0.6994514106583072 loss: 8129.014892578125 test_loss: 0.0 test_acc: 0.7504520795660036\n",
      "epoch 58 total_train_acc: 0.700626959247649 loss: 7443.681884765625 test_loss: 21.29361915588379 test_acc: 0.7513562386980108\n",
      "epoch 59 total_train_acc: 0.7021943573667712 loss: 7828.94873046875 test_loss: 430.9137268066406 test_acc: 0.7495479204339964\n",
      "epoch 60 total_train_acc: 0.6845611285266457 loss: 7839.9462890625 test_loss: 2239.173828125 test_acc: 0.7531645569620253\n",
      "epoch 61 total_train_acc: 0.7096394984326019 loss: 6955.343505859375 test_loss: 1128.9215087890625 test_acc: 0.7531645569620253\n",
      "epoch 62 total_train_acc: 0.70141065830721 loss: 7457.208984375 test_loss: 2176.6015625 test_acc: 0.7531645569620253\n",
      "epoch 63 total_train_acc: 0.6935736677115988 loss: 7496.263671875 test_loss: 2300.743896484375 test_acc: 0.7549728752260397\n",
      "epoch 64 total_train_acc: 0.7123824451410659 loss: 7049.759521484375 test_loss: 0.0 test_acc: 0.7585895117540687\n",
      "epoch 65 total_train_acc: 0.6982758620689655 loss: 7202.802001953125 test_loss: 401.1482849121094 test_acc: 0.7631103074141049\n",
      "epoch 66 total_train_acc: 0.7123824451410659 loss: 6884.28466796875 test_loss: 0.0 test_acc: 0.7649186256781193\n",
      "epoch 67 total_train_acc: 0.7092476489028213 loss: 6844.603759765625 test_loss: 383.0104064941406 test_acc: 0.7694394213381555\n",
      "epoch 68 total_train_acc: 0.7088557993730408 loss: 6489.53564453125 test_loss: 0.0 test_acc: 0.7730560578661845\n",
      "epoch 69 total_train_acc: 0.710423197492163 loss: 7115.60546875 test_loss: 6113.39306640625 test_acc: 0.7703435804701627\n",
      "epoch 70 total_train_acc: 0.7202194357366771 loss: 6486.27294921875 test_loss: 823.1640625 test_acc: 0.7730560578661845\n",
      "epoch 71 total_train_acc: 0.6955329153605015 loss: 6259.432373046875 test_loss: 0.0 test_acc: 0.7748643761301989\n",
      "epoch 72 total_train_acc: 0.7080721003134797 loss: 6532.6907958984375 test_loss: 0.0 test_acc: 0.7775768535262206\n",
      "epoch 73 total_train_acc: 0.7186520376175548 loss: 5947.569580078125 test_loss: 812.5994262695312 test_acc: 0.7784810126582279\n",
      "epoch 74 total_train_acc: 0.7155172413793104 loss: 6367.2086181640625 test_loss: 1777.7650146484375 test_acc: 0.7802893309222423\n",
      "epoch 75 total_train_acc: 0.7065047021943573 loss: 6301.7164306640625 test_loss: 185.7223358154297 test_acc: 0.783001808318264\n",
      "epoch 76 total_train_acc: 0.7210031347962382 loss: 5946.7772216796875 test_loss: 795.9697265625 test_acc: 0.783001808318264\n",
      "epoch 77 total_train_acc: 0.7213949843260188 loss: 6038.6837158203125 test_loss: 2664.194580078125 test_acc: 0.7848101265822784\n",
      "epoch 78 total_train_acc: 0.7249216300940439 loss: 5660.10791015625 test_loss: 509.7057189941406 test_acc: 0.7839059674502713\n",
      "epoch 79 total_train_acc: 0.7119905956112853 loss: 5721.0240478515625 test_loss: 578.9264526367188 test_acc: 0.7839059674502713\n",
      "epoch 80 total_train_acc: 0.7147335423197492 loss: 5377.7779541015625 test_loss: 262.068359375 test_acc: 0.783001808318264\n",
      "epoch 81 total_train_acc: 0.7182601880877743 loss: 5645.749267578125 test_loss: 320.9697265625 test_acc: 0.7839059674502713\n",
      "epoch 82 total_train_acc: 0.7100313479623824 loss: 5636.75537109375 test_loss: 1564.3328857421875 test_acc: 0.7893309222423146\n",
      "epoch 83 total_train_acc: 0.7174764890282131 loss: 5665.4483642578125 test_loss: 0.0 test_acc: 0.7893309222423146\n",
      "epoch 84 total_train_acc: 0.7323667711598746 loss: 5367.02685546875 test_loss: 0.0 test_acc: 0.7911392405063291\n",
      "epoch 85 total_train_acc: 0.725705329153605 loss: 5240.1104736328125 test_loss: 1221.6318359375 test_acc: 0.786618444846293\n",
      "epoch 86 total_train_acc: 0.7260971786833855 loss: 5242.2137451171875 test_loss: 854.5504760742188 test_acc: 0.7884267631103075\n",
      "epoch 87 total_train_acc: 0.731974921630094 loss: 5174.037109375 test_loss: 0.0 test_acc: 0.7884267631103075\n",
      "epoch 88 total_train_acc: 0.7245297805642633 loss: 4998.821533203125 test_loss: 1353.69287109375 test_acc: 0.7902350813743219\n",
      "epoch 89 total_train_acc: 0.7233542319749217 loss: 4681.877197265625 test_loss: 674.2131958007812 test_acc: 0.7902350813743219\n",
      "epoch 90 total_train_acc: 0.734717868338558 loss: 5153.8690185546875 test_loss: 1826.2681884765625 test_acc: 0.7929475587703436\n",
      "epoch 91 total_train_acc: 0.7241379310344828 loss: 4963.7781982421875 test_loss: 397.8230895996094 test_acc: 0.794755877034358\n",
      "epoch 92 total_train_acc: 0.7331504702194357 loss: 4716.92529296875 test_loss: 334.5826721191406 test_acc: 0.7920433996383364\n",
      "epoch 93 total_train_acc: 0.72923197492163 loss: 4355.8624267578125 test_loss: 81.08805084228516 test_acc: 0.7920433996383364\n",
      "epoch 94 total_train_acc: 0.7339341692789969 loss: 4745.7119140625 test_loss: 331.2421875 test_acc: 0.7965641952983725\n",
      "epoch 95 total_train_acc: 0.7421630094043887 loss: 4492.44775390625 test_loss: 596.1689453125 test_acc: 0.794755877034358\n",
      "epoch 96 total_train_acc: 0.7327586206896551 loss: 5050.227783203125 test_loss: 542.5379028320312 test_acc: 0.798372513562387\n",
      "epoch 97 total_train_acc: 0.7315830721003135 loss: 4217.0648193359375 test_loss: 90.99185943603516 test_acc: 0.798372513562387\n",
      "epoch 98 total_train_acc: 0.7358934169278997 loss: 4487.5765380859375 test_loss: 116.48079681396484 test_acc: 0.798372513562387\n",
      "epoch 99 total_train_acc: 0.7280564263322884 loss: 4580.0980224609375 test_loss: 1662.6131591796875 test_acc: 0.798372513562387\n",
      "epoch 100 total_train_acc: 0.7370689655172413 loss: 4015.75146484375 test_loss: 2939.634765625 test_acc: 0.8001808318264014\n",
      "epoch 101 total_train_acc: 0.7433385579937304 loss: 4051.7080078125 test_loss: 495.2989807128906 test_acc: 0.8001808318264014\n",
      "epoch 102 total_train_acc: 0.747257053291536 loss: 3844.086181640625 test_loss: 0.0 test_acc: 0.7992766726943942\n",
      "epoch 103 total_train_acc: 0.7311912225705329 loss: 4038.5703125 test_loss: 258.0354919433594 test_acc: 0.798372513562387\n",
      "epoch 104 total_train_acc: 0.7272727272727273 loss: 4161.0738525390625 test_loss: 1408.5439453125 test_acc: 0.7992766726943942\n",
      "epoch 105 total_train_acc: 0.7339341692789969 loss: 4272.4227294921875 test_loss: 2859.789306640625 test_acc: 0.7992766726943942\n",
      "epoch 106 total_train_acc: 0.7445141065830722 loss: 3957.3760986328125 test_loss: 112.96907806396484 test_acc: 0.8001808318264014\n",
      "epoch 107 total_train_acc: 0.7484326018808778 loss: 3784.640869140625 test_loss: 2167.5234375 test_acc: 0.8010849909584087\n",
      "epoch 108 total_train_acc: 0.7441222570532915 loss: 3864.3919677734375 test_loss: 11.6875 test_acc: 0.8019891500904159\n",
      "epoch 109 total_train_acc: 0.731974921630094 loss: 4102.3502197265625 test_loss: 0.0 test_acc: 0.8037974683544303\n",
      "epoch 110 total_train_acc: 0.7221786833855799 loss: 4154.5352783203125 test_loss: 0.0 test_acc: 0.8037974683544303\n",
      "epoch 111 total_train_acc: 0.7358934169278997 loss: 3513.32958984375 test_loss: 0.0 test_acc: 0.8001808318264014\n",
      "epoch 112 total_train_acc: 0.7476489028213166 loss: 3853.18212890625 test_loss: 659.4365234375 test_acc: 0.8019891500904159\n",
      "epoch 113 total_train_acc: 0.7484326018808778 loss: 3963.955322265625 test_loss: 0.0 test_acc: 0.8028933092224232\n",
      "epoch 114 total_train_acc: 0.7390282131661442 loss: 3632.423828125 test_loss: 0.0 test_acc: 0.8019891500904159\n",
      "epoch 115 total_train_acc: 0.7366771159874608 loss: 3718.9017333984375 test_loss: 991.4401245117188 test_acc: 0.8037974683544303\n",
      "epoch 116 total_train_acc: 0.7558777429467085 loss: 3378.508056640625 test_loss: 0.0 test_acc: 0.8047016274864376\n",
      "epoch 117 total_train_acc: 0.7378526645768025 loss: 3613.8275756835938 test_loss: 352.2835998535156 test_acc: 0.8047016274864376\n",
      "epoch 118 total_train_acc: 0.7468652037617555 loss: 3360.1066284179688 test_loss: 0.0 test_acc: 0.8047016274864376\n",
      "epoch 119 total_train_acc: 0.7519592476489029 loss: 3015.2581176757812 test_loss: 0.0 test_acc: 0.8010849909584087\n",
      "epoch 120 total_train_acc: 0.7492163009404389 loss: 3463.3449096679688 test_loss: 0.0 test_acc: 0.8074141048824593\n",
      "epoch 121 total_train_acc: 0.7464733542319749 loss: 3420.1407470703125 test_loss: 0.0 test_acc: 0.8065099457504521\n",
      "epoch 122 total_train_acc: 0.7382445141065831 loss: 3324.7052001953125 test_loss: 127.78597259521484 test_acc: 0.8056057866184448\n",
      "epoch 123 total_train_acc: 0.7449059561128527 loss: 3470.2960205078125 test_loss: 689.8678588867188 test_acc: 0.8056057866184448\n",
      "epoch 124 total_train_acc: 0.7574451410658307 loss: 2921.9256591796875 test_loss: 311.9908752441406 test_acc: 0.8074141048824593\n",
      "epoch 125 total_train_acc: 0.7558777429467085 loss: 2949.1859741210938 test_loss: 920.8135375976562 test_acc: 0.8092224231464737\n",
      "epoch 126 total_train_acc: 0.7554858934169278 loss: 2918.5457153320312 test_loss: 0.0 test_acc: 0.8092224231464737\n",
      "epoch 127 total_train_acc: 0.7402037617554859 loss: 3297.2879638671875 test_loss: 0.0 test_acc: 0.8119349005424955\n",
      "epoch 128 total_train_acc: 0.7617554858934169 loss: 3162.8609619140625 test_loss: 306.3396911621094 test_acc: 0.810126582278481\n",
      "epoch 129 total_train_acc: 0.7609717868338558 loss: 3016.0462036132812 test_loss: 1047.4759521484375 test_acc: 0.810126582278481\n",
      "epoch 130 total_train_acc: 0.771551724137931 loss: 2919.4263916015625 test_loss: 544.3112182617188 test_acc: 0.8110307414104883\n",
      "epoch 131 total_train_acc: 0.7664576802507836 loss: 2683.2874145507812 test_loss: 124.10189056396484 test_acc: 0.8092224231464737\n",
      "epoch 132 total_train_acc: 0.7503918495297806 loss: 3020.2529907226562 test_loss: 370.0693359375 test_acc: 0.8028933092224232\n",
      "epoch 133 total_train_acc: 0.7554858934169278 loss: 3081.76611328125 test_loss: 558.59912109375 test_acc: 0.8047016274864376\n",
      "epoch 134 total_train_acc: 0.7621473354231975 loss: 2589.156005859375 test_loss: 0.0 test_acc: 0.8110307414104883\n",
      "epoch 135 total_train_acc: 0.7535266457680251 loss: 2971.662109375 test_loss: 117.52181243896484 test_acc: 0.8119349005424955\n",
      "epoch 136 total_train_acc: 0.7594043887147336 loss: 2973.5980834960938 test_loss: 135.0193634033203 test_acc: 0.8128390596745028\n",
      "epoch 137 total_train_acc: 0.7586206896551724 loss: 2937.370361328125 test_loss: 0.0 test_acc: 0.8110307414104883\n",
      "epoch 138 total_train_acc: 0.7633228840125392 loss: 2722.9877319335938 test_loss: 650.5709838867188 test_acc: 0.8110307414104883\n",
      "epoch 139 total_train_acc: 0.7676332288401254 loss: 2735.319580078125 test_loss: 57.56721878051758 test_acc: 0.8092224231464737\n",
      "epoch 140 total_train_acc: 0.7672413793103449 loss: 2658.9955444335938 test_loss: 0.0 test_acc: 0.8146473779385172\n",
      "epoch 141 total_train_acc: 0.7594043887147336 loss: 2460.82177734375 test_loss: 781.05078125 test_acc: 0.8119349005424955\n",
      "epoch 142 total_train_acc: 0.7684169278996865 loss: 2718.9552001953125 test_loss: 281.2421875 test_acc: 0.8083182640144665\n",
      "epoch 143 total_train_acc: 0.7609717868338558 loss: 2545.278564453125 test_loss: 0.0 test_acc: 0.8110307414104883\n",
      "epoch 144 total_train_acc: 0.7613636363636364 loss: 2521.403076171875 test_loss: 311.8048400878906 test_acc: 0.8092224231464737\n",
      "epoch 145 total_train_acc: 0.7605799373040752 loss: 2498.8012084960938 test_loss: 610.0505981445312 test_acc: 0.8110307414104883\n",
      "epoch 146 total_train_acc: 0.7543103448275862 loss: 2761.785888671875 test_loss: 199.8125 test_acc: 0.8137432188065099\n",
      "epoch 147 total_train_acc: 0.7535266457680251 loss: 2561.1239624023438 test_loss: 0.0 test_acc: 0.8137432188065099\n",
      "epoch 148 total_train_acc: 0.7543103448275862 loss: 2456.841552734375 test_loss: 129.0811309814453 test_acc: 0.8128390596745028\n",
      "epoch 149 total_train_acc: 0.7550940438871473 loss: 2333.4533081054688 test_loss: 0.0 test_acc: 0.8164556962025317\n",
      "epoch 150 total_train_acc: 0.7582288401253918 loss: 2440.0250854492188 test_loss: 959.5794067382812 test_acc: 0.8155515370705244\n",
      "epoch 151 total_train_acc: 0.7699843260188087 loss: 2512.0821533203125 test_loss: 165.958251953125 test_acc: 0.8137432188065099\n",
      "epoch 152 total_train_acc: 0.7648902821316614 loss: 2398.8900146484375 test_loss: 590.6542358398438 test_acc: 0.8191681735985533\n",
      "epoch 153 total_train_acc: 0.7676332288401254 loss: 2563.4185180664062 test_loss: 0.0 test_acc: 0.8191681735985533\n",
      "epoch 154 total_train_acc: 0.7672413793103449 loss: 2241.1555786132812 test_loss: 0.0 test_acc: 0.8209764918625678\n",
      "epoch 155 total_train_acc: 0.7672413793103449 loss: 2267.7788696289062 test_loss: 812.9501342773438 test_acc: 0.8119349005424955\n",
      "epoch 156 total_train_acc: 0.7719435736677116 loss: 2346.1965942382812 test_loss: 449.7840270996094 test_acc: 0.8119349005424955\n",
      "epoch 157 total_train_acc: 0.7676332288401254 loss: 2225.0210571289062 test_loss: 491.689453125 test_acc: 0.8119349005424955\n",
      "epoch 158 total_train_acc: 0.7727272727272727 loss: 2182.947265625 test_loss: 646.5670776367188 test_acc: 0.8173598553345389\n",
      "epoch 159 total_train_acc: 0.7770376175548589 loss: 2085.046142578125 test_loss: 345.8106384277344 test_acc: 0.8155515370705244\n",
      "epoch 160 total_train_acc: 0.7731191222570533 loss: 2252.61767578125 test_loss: 74.52701568603516 test_acc: 0.8146473779385172\n",
      "epoch 161 total_train_acc: 0.7676332288401254 loss: 2286.9961547851562 test_loss: 0.0 test_acc: 0.8146473779385172\n",
      "epoch 162 total_train_acc: 0.7692006269592476 loss: 2134.0553588867188 test_loss: 386.76611328125 test_acc: 0.8146473779385172\n",
      "epoch 163 total_train_acc: 0.7672413793103449 loss: 2032.481689453125 test_loss: 0.0 test_acc: 0.8146473779385172\n",
      "epoch 164 total_train_acc: 0.7778213166144201 loss: 2013.11279296875 test_loss: 943.3571166992188 test_acc: 0.8164556962025317\n",
      "epoch 165 total_train_acc: 0.7739028213166145 loss: 2031.4178466796875 test_loss: 249.5309295654297 test_acc: 0.8164556962025317\n",
      "epoch 166 total_train_acc: 0.7496081504702194 loss: 2142.9945678710938 test_loss: 0.0 test_acc: 0.8182640144665461\n",
      "epoch 167 total_train_acc: 0.7739028213166145 loss: 1910.9232788085938 test_loss: 176.2161407470703 test_acc: 0.8155515370705244\n",
      "epoch 168 total_train_acc: 0.7774294670846394 loss: 1857.9437255859375 test_loss: 0.0 test_acc: 0.8173598553345389\n",
      "epoch 169 total_train_acc: 0.7789968652037618 loss: 1848.8045043945312 test_loss: 431.0050354003906 test_acc: 0.8209764918625678\n",
      "epoch 170 total_train_acc: 0.7821316614420063 loss: 1976.486572265625 test_loss: 108.9293212890625 test_acc: 0.8200723327305606\n",
      "epoch 171 total_train_acc: 0.7817398119122257 loss: 2010.0753173828125 test_loss: 455.9270935058594 test_acc: 0.8182640144665461\n",
      "epoch 172 total_train_acc: 0.7727272727272727 loss: 1790.7981567382812 test_loss: 688.1119995117188 test_acc: 0.8173598553345389\n",
      "epoch 173 total_train_acc: 0.7840909090909091 loss: 1674.5622863769531 test_loss: 615.7731323242188 test_acc: 0.8191681735985533\n",
      "epoch 174 total_train_acc: 0.7860501567398119 loss: 1806.2749633789062 test_loss: 0.0 test_acc: 0.8146473779385172\n",
      "epoch 175 total_train_acc: 0.7884012539184952 loss: 1823.924560546875 test_loss: 1166.9664306640625 test_acc: 0.8227848101265823\n",
      "epoch 176 total_train_acc: 0.7774294670846394 loss: 1798.6408081054688 test_loss: 595.5703125 test_acc: 0.8245931283905967\n",
      "epoch 177 total_train_acc: 0.7813479623824452 loss: 1858.3960571289062 test_loss: 666.136474609375 test_acc: 0.8209764918625678\n",
      "epoch 178 total_train_acc: 0.7825235109717869 loss: 1902.1204833984375 test_loss: 0.0 test_acc: 0.8182640144665461\n",
      "epoch 179 total_train_acc: 0.7887931034482759 loss: 1667.9476318359375 test_loss: 0.0 test_acc: 0.8128390596745028\n",
      "epoch 180 total_train_acc: 0.7817398119122257 loss: 1654.5020446777344 test_loss: 487.0882263183594 test_acc: 0.8155515370705244\n",
      "epoch 181 total_train_acc: 0.792319749216301 loss: 1543.5060119628906 test_loss: 1488.498046875 test_acc: 0.8236889692585895\n",
      "epoch 182 total_train_acc: 0.7840909090909091 loss: 1733.2919311523438 test_loss: 136.3688201904297 test_acc: 0.8137432188065099\n",
      "epoch 183 total_train_acc: 0.7801724137931034 loss: 1623.0153198242188 test_loss: 0.0 test_acc: 0.8182640144665461\n",
      "epoch 184 total_train_acc: 0.7821316614420063 loss: 1499.7065734863281 test_loss: 21.119384765625 test_acc: 0.8218806509945751\n",
      "epoch 185 total_train_acc: 0.7789968652037618 loss: 1771.6260375976562 test_loss: 0.0 test_acc: 0.8191681735985533\n",
      "epoch 186 total_train_acc: 0.7754702194357367 loss: 1802.6561889648438 test_loss: 543.9978637695312 test_acc: 0.8164556962025317\n",
      "epoch 187 total_train_acc: 0.7774294670846394 loss: 1671.1632995605469 test_loss: 137.7828826904297 test_acc: 0.8182640144665461\n",
      "epoch 188 total_train_acc: 0.7731191222570533 loss: 1681.4908447265625 test_loss: 391.2560119628906 test_acc: 0.8236889692585895\n",
      "epoch 189 total_train_acc: 0.7891849529780565 loss: 1506.4268493652344 test_loss: 0.0 test_acc: 0.8227848101265823\n",
      "epoch 190 total_train_acc: 0.7864420062695925 loss: 1494.6568298339844 test_loss: 184.7109375 test_acc: 0.825497287522604\n",
      "epoch 191 total_train_acc: 0.792319749216301 loss: 1515.0863342285156 test_loss: 0.0 test_acc: 0.8245931283905967\n",
      "epoch 192 total_train_acc: 0.7880094043887147 loss: 1589.0067138671875 test_loss: 674.5952758789062 test_acc: 0.8245931283905967\n",
      "epoch 193 total_train_acc: 0.7938871473354232 loss: 1528.0708312988281 test_loss: 457.18701171875 test_acc: 0.8245931283905967\n",
      "epoch 194 total_train_acc: 0.7907523510971787 loss: 1524.9573974609375 test_loss: 0.0 test_acc: 0.8273056057866185\n",
      "epoch 195 total_train_acc: 0.7840909090909091 loss: 1493.9875793457031 test_loss: 406.006103515625 test_acc: 0.8282097649186256\n",
      "epoch 196 total_train_acc: 0.7899686520376176 loss: 1499.2676696777344 test_loss: 397.9451599121094 test_acc: 0.8273056057866185\n",
      "epoch 197 total_train_acc: 0.795846394984326 loss: 1502.2762451171875 test_loss: 28.29508399963379 test_acc: 0.8318264014466547\n",
      "epoch 198 total_train_acc: 0.7934952978056427 loss: 1619.4950561523438 test_loss: 330.5216369628906 test_acc: 0.8182640144665461\n",
      "epoch 199 total_train_acc: 0.7919278996865203 loss: 1471.0847778320312 test_loss: 9.686808586120605 test_acc: 0.8218806509945751\n",
      "epoch 200 total_train_acc: 0.7978056426332288 loss: 1425.1892700195312 test_loss: 1227.5333251953125 test_acc: 0.8236889692585895\n",
      "epoch 201 total_train_acc: 0.7919278996865203 loss: 1364.0268249511719 test_loss: 142.7628631591797 test_acc: 0.833634719710669\n",
      "epoch 202 total_train_acc: 0.7856583072100314 loss: 1434.4285278320312 test_loss: 0.0 test_acc: 0.8318264014466547\n",
      "epoch 203 total_train_acc: 0.7934952978056427 loss: 1317.285888671875 test_loss: 703.4690551757812 test_acc: 0.8327305605786618\n",
      "epoch 204 total_train_acc: 0.8107366771159875 loss: 1255.8549194335938 test_loss: 0.0 test_acc: 0.8273056057866185\n",
      "epoch 205 total_train_acc: 0.7817398119122257 loss: 1246.1121826171875 test_loss: 546.265380859375 test_acc: 0.8309222423146474\n",
      "epoch 206 total_train_acc: 0.8032915360501567 loss: 1228.5375366210938 test_loss: 170.4647674560547 test_acc: 0.8309222423146474\n",
      "epoch 207 total_train_acc: 0.792319749216301 loss: 1249.7034606933594 test_loss: 0.0 test_acc: 0.8245931283905967\n",
      "epoch 208 total_train_acc: 0.7989811912225705 loss: 1301.1349487304688 test_loss: 31.888671875 test_acc: 0.8318264014466547\n",
      "epoch 209 total_train_acc: 0.7966300940438872 loss: 1238.0163879394531 test_loss: 0.0 test_acc: 0.8282097649186256\n",
      "epoch 210 total_train_acc: 0.7974137931034483 loss: 1221.3014526367188 test_loss: 374.0035705566406 test_acc: 0.8327305605786618\n",
      "epoch 211 total_train_acc: 0.7911442006269592 loss: 1123.3623352050781 test_loss: 80.3095703125 test_acc: 0.8345388788426763\n",
      "epoch 212 total_train_acc: 0.7997648902821317 loss: 1133.1976623535156 test_loss: 0.0 test_acc: 0.8327305605786618\n",
      "epoch 213 total_train_acc: 0.8044670846394985 loss: 1183.8753051757812 test_loss: 0.0 test_acc: 0.8318264014466547\n",
      "epoch 214 total_train_acc: 0.8001567398119123 loss: 1185.6278991699219 test_loss: 0.0 test_acc: 0.8345388788426763\n",
      "epoch 215 total_train_acc: 0.795846394984326 loss: 1137.9541015625 test_loss: 154.2757110595703 test_acc: 0.8318264014466547\n",
      "epoch 216 total_train_acc: 0.7962382445141066 loss: 1181.7786865234375 test_loss: 0.0 test_acc: 0.8318264014466547\n",
      "epoch 217 total_train_acc: 0.7966300940438872 loss: 1290.1673889160156 test_loss: 0.0 test_acc: 0.8327305605786618\n",
      "epoch 218 total_train_acc: 0.7989811912225705 loss: 1191.5501708984375 test_loss: 58.47542190551758 test_acc: 0.8354430379746836\n",
      "epoch 219 total_train_acc: 0.7981974921630094 loss: 1171.7793579101562 test_loss: 205.8116912841797 test_acc: 0.8399638336347197\n",
      "epoch 220 total_train_acc: 0.8036833855799373 loss: 1005.1305541992188 test_loss: 221.4842071533203 test_acc: 0.8399638336347197\n",
      "epoch 221 total_train_acc: 0.8017241379310345 loss: 1091.2704162597656 test_loss: 28.9443359375 test_acc: 0.8309222423146474\n",
      "epoch 222 total_train_acc: 0.7891849529780565 loss: 1124.9638061523438 test_loss: 0.0 test_acc: 0.8381555153707052\n",
      "epoch 223 total_train_acc: 0.8115203761755486 loss: 1072.585693359375 test_loss: 0.0 test_acc: 0.840867992766727\n",
      "epoch 224 total_train_acc: 0.7970219435736677 loss: 1194.1094055175781 test_loss: 1183.35693359375 test_acc: 0.8327305605786618\n",
      "epoch 225 total_train_acc: 0.8025078369905956 loss: 1120.8757934570312 test_loss: 29.22412109375 test_acc: 0.8363471971066908\n",
      "epoch 226 total_train_acc: 0.8028996865203761 loss: 1047.4984741210938 test_loss: 190.4670867919922 test_acc: 0.8363471971066908\n",
      "epoch 227 total_train_acc: 0.7989811912225705 loss: 1024.7334899902344 test_loss: 132.93701171875 test_acc: 0.8345388788426763\n",
      "epoch 228 total_train_acc: 0.8123040752351097 loss: 894.7019348144531 test_loss: 0.0 test_acc: 0.8372513562386981\n",
      "epoch 229 total_train_acc: 0.8138714733542319 loss: 910.8675231933594 test_loss: 1012.2763671875 test_acc: 0.8345388788426763\n",
      "epoch 230 total_train_acc: 0.8068181818181818 loss: 1053.5639038085938 test_loss: 0.0 test_acc: 0.8345388788426763\n",
      "epoch 231 total_train_acc: 0.8099529780564263 loss: 906.2448120117188 test_loss: 60.6649169921875 test_acc: 0.840867992766727\n",
      "epoch 232 total_train_acc: 0.8068181818181818 loss: 974.1141662597656 test_loss: 113.00716400146484 test_acc: 0.8372513562386981\n",
      "epoch 233 total_train_acc: 0.8056426332288401 loss: 955.5276184082031 test_loss: 18.69099998474121 test_acc: 0.840867992766727\n",
      "epoch 234 total_train_acc: 0.8068181818181818 loss: 1002.8321533203125 test_loss: 0.0 test_acc: 0.8399638336347197\n",
      "epoch 235 total_train_acc: 0.8115203761755486 loss: 923.7467346191406 test_loss: 0.0 test_acc: 0.8426763110307414\n",
      "epoch 236 total_train_acc: 0.8146551724137931 loss: 908.8269958496094 test_loss: 0.0 test_acc: 0.8390596745027125\n",
      "epoch 237 total_train_acc: 0.8158307210031348 loss: 928.6788330078125 test_loss: 7.6015625 test_acc: 0.8372513562386981\n",
      "epoch 238 total_train_acc: 0.8185736677115988 loss: 935.7962646484375 test_loss: 384.3567810058594 test_acc: 0.8453887884267631\n",
      "epoch 239 total_train_acc: 0.8119122257053292 loss: 1018.1080017089844 test_loss: 0.0 test_acc: 0.8444846292947559\n",
      "epoch 240 total_train_acc: 0.8185736677115988 loss: 878.7439880371094 test_loss: 544.7080078125 test_acc: 0.8462929475587704\n",
      "epoch 241 total_train_acc: 0.8126959247648903 loss: 904.7262573242188 test_loss: 0.0 test_acc: 0.8435804701627486\n",
      "epoch 242 total_train_acc: 0.8307210031347962 loss: 842.0832214355469 test_loss: 63.17317581176758 test_acc: 0.840867992766727\n",
      "epoch 243 total_train_acc: 0.8162225705329154 loss: 892.536376953125 test_loss: 0.0 test_acc: 0.8462929475587704\n",
      "epoch 244 total_train_acc: 0.8209247648902821 loss: 840.8989715576172 test_loss: 165.9117431640625 test_acc: 0.8499095840867993\n",
      "epoch 245 total_train_acc: 0.8181818181818182 loss: 1012.2680358886719 test_loss: 0.0 test_acc: 0.8435804701627486\n",
      "epoch 246 total_train_acc: 0.8142633228840125 loss: 740.1137084960938 test_loss: 0.0 test_acc: 0.8517179023508138\n",
      "epoch 247 total_train_acc: 0.8138714733542319 loss: 892.2641906738281 test_loss: 0.0 test_acc: 0.8444846292947559\n",
      "epoch 248 total_train_acc: 0.8248432601880877 loss: 823.115234375 test_loss: 0.0 test_acc: 0.8481012658227848\n",
      "epoch 249 total_train_acc: 0.8130877742946708 loss: 789.5928192138672 test_loss: 151.2408905029297 test_acc: 0.8462929475587704\n",
      "epoch 250 total_train_acc: 0.8224921630094044 loss: 824.6254272460938 test_loss: 850.2971801757812 test_acc: 0.849005424954792\n",
      "epoch 251 total_train_acc: 0.8083855799373041 loss: 867.5438537597656 test_loss: 265.1720275878906 test_acc: 0.8481012658227848\n",
      "epoch 252 total_train_acc: 0.8248432601880877 loss: 783.7482147216797 test_loss: 0.0 test_acc: 0.849005424954792\n",
      "epoch 253 total_train_acc: 0.8170062695924765 loss: 785.8163909912109 test_loss: 16.25203514099121 test_acc: 0.8481012658227848\n",
      "epoch 254 total_train_acc: 0.8158307210031348 loss: 753.3457946777344 test_loss: 0.0 test_acc: 0.8517179023508138\n",
      "epoch 255 total_train_acc: 0.822884012539185 loss: 729.4856872558594 test_loss: 220.78309631347656 test_acc: 0.8499095840867993\n",
      "epoch 256 total_train_acc: 0.8236677115987461 loss: 744.2819213867188 test_loss: 0.0 test_acc: 0.8508137432188065\n",
      "epoch 257 total_train_acc: 0.8181818181818182 loss: 775.417724609375 test_loss: 0.0 test_acc: 0.8471971066907775\n",
      "epoch 258 total_train_acc: 0.8295454545454546 loss: 748.3460540771484 test_loss: 0.0 test_acc: 0.8508137432188065\n",
      "epoch 259 total_train_acc: 0.8311128526645768 loss: 762.0765228271484 test_loss: 209.8140411376953 test_acc: 0.849005424954792\n",
      "epoch 260 total_train_acc: 0.8185736677115988 loss: 787.2651519775391 test_loss: 79.68936920166016 test_acc: 0.8526220614828209\n",
      "epoch 261 total_train_acc: 0.8221003134796239 loss: 824.372802734375 test_loss: 0.0 test_acc: 0.8481012658227848\n",
      "epoch 262 total_train_acc: 0.8087774294670846 loss: 748.4306640625 test_loss: 380.2568359375 test_acc: 0.8499095840867993\n",
      "epoch 263 total_train_acc: 0.8209247648902821 loss: 763.7497863769531 test_loss: 370.037841796875 test_acc: 0.8526220614828209\n",
      "epoch 264 total_train_acc: 0.8248432601880877 loss: 734.380615234375 test_loss: 169.4060821533203 test_acc: 0.8508137432188065\n",
      "epoch 265 total_train_acc: 0.8275862068965517 loss: 704.2763366699219 test_loss: 25.08553123474121 test_acc: 0.85623869801085\n",
      "epoch 266 total_train_acc: 0.8240595611285266 loss: 759.4119110107422 test_loss: 108.826171875 test_acc: 0.8535262206148282\n",
      "epoch 267 total_train_acc: 0.8240595611285266 loss: 779.6304931640625 test_loss: 244.33152770996094 test_acc: 0.8517179023508138\n",
      "epoch 268 total_train_acc: 0.832680250783699 loss: 694.2863464355469 test_loss: 0.0 test_acc: 0.849005424954792\n",
      "epoch 269 total_train_acc: 0.8385579937304075 loss: 726.1980895996094 test_loss: 501.3330078125 test_acc: 0.8535262206148282\n",
      "epoch 270 total_train_acc: 0.829153605015674 loss: 685.9288635253906 test_loss: 45.64982223510742 test_acc: 0.8535262206148282\n",
      "epoch 271 total_train_acc: 0.8268025078369906 loss: 655.9563598632812 test_loss: 0.0 test_acc: 0.8544303797468354\n",
      "epoch 272 total_train_acc: 0.8283699059561128 loss: 674.9152679443359 test_loss: 0.0 test_acc: 0.8535262206148282\n",
      "epoch 273 total_train_acc: 0.8268025078369906 loss: 744.0270690917969 test_loss: 202.4503631591797 test_acc: 0.8553345388788427\n",
      "epoch 274 total_train_acc: 0.8275862068965517 loss: 675.7568206787109 test_loss: 182.4319610595703 test_acc: 0.8535262206148282\n",
      "epoch 275 total_train_acc: 0.8279780564263323 loss: 651.6425476074219 test_loss: 198.008056640625 test_acc: 0.8544303797468354\n",
      "epoch 276 total_train_acc: 0.8330721003134797 loss: 659.4617156982422 test_loss: 153.0147705078125 test_acc: 0.8580470162748643\n",
      "epoch 277 total_train_acc: 0.8279780564263323 loss: 719.2170562744141 test_loss: 439.4779052734375 test_acc: 0.8553345388788427\n",
      "epoch 278 total_train_acc: 0.8252351097178683 loss: 693.1416168212891 test_loss: 576.8399047851562 test_acc: 0.8499095840867993\n",
      "epoch 279 total_train_acc: 0.8283699059561128 loss: 584.5614013671875 test_loss: 382.1353454589844 test_acc: 0.8526220614828209\n",
      "epoch 280 total_train_acc: 0.8185736677115988 loss: 643.2138061523438 test_loss: 0.0 test_acc: 0.8553345388788427\n",
      "epoch 281 total_train_acc: 0.8362068965517241 loss: 598.0585632324219 test_loss: 455.38330078125 test_acc: 0.8544303797468354\n",
      "epoch 282 total_train_acc: 0.8213166144200627 loss: 573.7420196533203 test_loss: 0.0 test_acc: 0.8580470162748643\n",
      "epoch 283 total_train_acc: 0.8252351097178683 loss: 662.1031188964844 test_loss: 0.0 test_acc: 0.8526220614828209\n",
      "epoch 284 total_train_acc: 0.829153605015674 loss: 569.8475036621094 test_loss: 0.0 test_acc: 0.85623869801085\n",
      "epoch 285 total_train_acc: 0.8271943573667712 loss: 607.9919281005859 test_loss: 214.4923858642578 test_acc: 0.8535262206148282\n",
      "epoch 286 total_train_acc: 0.8299373040752351 loss: 593.8726196289062 test_loss: 0.0 test_acc: 0.8526220614828209\n",
      "epoch 287 total_train_acc: 0.822884012539185 loss: 564.4670562744141 test_loss: 47.54716873168945 test_acc: 0.8508137432188065\n",
      "epoch 288 total_train_acc: 0.822884012539185 loss: 639.7642517089844 test_loss: 0.0 test_acc: 0.8598553345388789\n",
      "epoch 289 total_train_acc: 0.8224921630094044 loss: 644.1824493408203 test_loss: 333.0387878417969 test_acc: 0.8544303797468354\n",
      "epoch 290 total_train_acc: 0.8260188087774295 loss: 558.0459899902344 test_loss: 153.28453063964844 test_acc: 0.8580470162748643\n",
      "epoch 291 total_train_acc: 0.8330721003134797 loss: 606.5734710693359 test_loss: 258.1672668457031 test_acc: 0.8607594936708861\n",
      "epoch 292 total_train_acc: 0.8315047021943573 loss: 557.1411895751953 test_loss: 0.0 test_acc: 0.85623869801085\n",
      "epoch 293 total_train_acc: 0.8307210031347962 loss: 594.9787445068359 test_loss: 794.3961791992188 test_acc: 0.8553345388788427\n",
      "epoch 294 total_train_acc: 0.8365987460815048 loss: 512.3584899902344 test_loss: 0.0 test_acc: 0.8598553345388789\n",
      "epoch 295 total_train_acc: 0.8373824451410659 loss: 519.8286743164062 test_loss: 35.26716995239258 test_acc: 0.8553345388788427\n",
      "epoch 296 total_train_acc: 0.8373824451410659 loss: 600.0202026367188 test_loss: 0.0 test_acc: 0.8589511754068716\n",
      "epoch 297 total_train_acc: 0.841692789968652 loss: 550.7446594238281 test_loss: 266.1944885253906 test_acc: 0.8598553345388789\n",
      "epoch 298 total_train_acc: 0.844435736677116 loss: 547.5785827636719 test_loss: 147.228271484375 test_acc: 0.8571428571428571\n",
      "epoch 299 total_train_acc: 0.8420846394984326 loss: 508.50213623046875 test_loss: 1146.0823974609375 test_acc: 0.85623869801085\n",
      "epoch 300 total_train_acc: 0.8377742946708464 loss: 538.3012847900391 test_loss: 32.23478317260742 test_acc: 0.8589511754068716\n",
      "epoch 301 total_train_acc: 0.8397335423197492 loss: 448.16429138183594 test_loss: 42.7001953125 test_acc: 0.8598553345388789\n",
      "epoch 302 total_train_acc: 0.829153605015674 loss: 529.5031433105469 test_loss: 843.0709838867188 test_acc: 0.85623869801085\n",
      "epoch 303 total_train_acc: 0.8338557993730408 loss: 519.7087860107422 test_loss: 0.0 test_acc: 0.8634719710669078\n",
      "epoch 304 total_train_acc: 0.8283699059561128 loss: 468.69273376464844 test_loss: 110.26920318603516 test_acc: 0.8598553345388789\n",
      "epoch 305 total_train_acc: 0.8401253918495298 loss: 511.6286315917969 test_loss: 0.0 test_acc: 0.8598553345388789\n",
      "epoch 306 total_train_acc: 0.8311128526645768 loss: 535.2159881591797 test_loss: 0.0 test_acc: 0.8580470162748643\n",
      "epoch 307 total_train_acc: 0.8358150470219435 loss: 521.7452697753906 test_loss: 606.4954223632812 test_acc: 0.8571428571428571\n",
      "epoch 308 total_train_acc: 0.8401253918495298 loss: 467.1368179321289 test_loss: 0.0 test_acc: 0.8598553345388789\n",
      "epoch 309 total_train_acc: 0.8322884012539185 loss: 523.896484375 test_loss: 78.20999908447266 test_acc: 0.8580470162748643\n",
      "epoch 310 total_train_acc: 0.8420846394984326 loss: 482.63832092285156 test_loss: 247.65826416015625 test_acc: 0.8661844484629295\n",
      "epoch 311 total_train_acc: 0.8287617554858934 loss: 550.6575469970703 test_loss: 271.9269104003906 test_acc: 0.8453887884267631\n",
      "epoch 312 total_train_acc: 0.8315047021943573 loss: 530.1105499267578 test_loss: 0.0 test_acc: 0.8634719710669078\n",
      "epoch 313 total_train_acc: 0.8330721003134797 loss: 563.0720062255859 test_loss: 731.5227661132812 test_acc: 0.8616636528028933\n",
      "epoch 314 total_train_acc: 0.8311128526645768 loss: 435.8220520019531 test_loss: 0.0 test_acc: 0.8625678119349005\n",
      "epoch 315 total_train_acc: 0.8401253918495298 loss: 438.99835205078125 test_loss: 0.0 test_acc: 0.8616636528028933\n",
      "epoch 316 total_train_acc: 0.8432601880877743 loss: 443.99913024902344 test_loss: 0.0 test_acc: 0.8670886075949367\n",
      "epoch 317 total_train_acc: 0.8369905956112853 loss: 460.32862854003906 test_loss: 84.54500579833984 test_acc: 0.8652802893309223\n",
      "epoch 318 total_train_acc: 0.8279780564263323 loss: 500.6988983154297 test_loss: 0.0 test_acc: 0.8598553345388789\n",
      "epoch 319 total_train_acc: 0.838166144200627 loss: 480.7241516113281 test_loss: 62.48577880859375 test_acc: 0.8661844484629295\n",
      "epoch 320 total_train_acc: 0.8330721003134797 loss: 441.5294189453125 test_loss: 171.4498748779297 test_acc: 0.8679927667269439\n",
      "epoch 321 total_train_acc: 0.8275862068965517 loss: 461.7644500732422 test_loss: 530.0676879882812 test_acc: 0.8616636528028933\n",
      "epoch 322 total_train_acc: 0.8385579937304075 loss: 459.3363952636719 test_loss: 273.1537170410156 test_acc: 0.8625678119349005\n",
      "epoch 323 total_train_acc: 0.8287617554858934 loss: 454.3203811645508 test_loss: 1242.715576171875 test_acc: 0.8616636528028933\n",
      "epoch 324 total_train_acc: 0.8456112852664577 loss: 420.06478118896484 test_loss: 357.6941833496094 test_acc: 0.8661844484629295\n",
      "epoch 325 total_train_acc: 0.8424764890282131 loss: 417.4008255004883 test_loss: 0.0 test_acc: 0.8661844484629295\n",
      "epoch 326 total_train_acc: 0.8389498432601881 loss: 394.8482131958008 test_loss: 19.49544334411621 test_acc: 0.8661844484629295\n",
      "epoch 327 total_train_acc: 0.8330721003134797 loss: 437.9271545410156 test_loss: 0.0 test_acc: 0.8670886075949367\n",
      "epoch 328 total_train_acc: 0.8346394984326019 loss: 425.35475158691406 test_loss: 143.03965759277344 test_acc: 0.8616636528028933\n",
      "epoch 329 total_train_acc: 0.8460031347962382 loss: 382.0182800292969 test_loss: 13.681477546691895 test_acc: 0.8616636528028933\n",
      "epoch 330 total_train_acc: 0.8346394984326019 loss: 446.30631256103516 test_loss: 0.0 test_acc: 0.849005424954792\n",
      "epoch 331 total_train_acc: 0.8299373040752351 loss: 523.3798675537109 test_loss: 204.4287872314453 test_acc: 0.8634719710669078\n",
      "epoch 332 total_train_acc: 0.8401253918495298 loss: 459.35364532470703 test_loss: 210.250244140625 test_acc: 0.8471971066907775\n",
      "epoch 333 total_train_acc: 0.8350313479623824 loss: 461.5295715332031 test_loss: 0.0 test_acc: 0.8625678119349005\n",
      "epoch 334 total_train_acc: 0.829153605015674 loss: 410.05379486083984 test_loss: 0.0 test_acc: 0.8607594936708861\n",
      "epoch 335 total_train_acc: 0.8358150470219435 loss: 480.7077331542969 test_loss: 392.5784606933594 test_acc: 0.8616636528028933\n",
      "epoch 336 total_train_acc: 0.8397335423197492 loss: 401.0297317504883 test_loss: 142.23561096191406 test_acc: 0.8661844484629295\n",
      "epoch 337 total_train_acc: 0.844435736677116 loss: 382.1832809448242 test_loss: 241.7374725341797 test_acc: 0.8661844484629295\n",
      "epoch 338 total_train_acc: 0.8495297805642633 loss: 325.2593994140625 test_loss: 486.2763366699219 test_acc: 0.8688969258589512\n",
      "epoch 339 total_train_acc: 0.835423197492163 loss: 400.7430419921875 test_loss: 257.563720703125 test_acc: 0.8652802893309223\n",
      "epoch 340 total_train_acc: 0.8362068965517241 loss: 369.71788787841797 test_loss: 0.0 test_acc: 0.8661844484629295\n",
      "epoch 341 total_train_acc: 0.8401253918495298 loss: 417.4070587158203 test_loss: 0.0 test_acc: 0.8707052441229657\n",
      "epoch 342 total_train_acc: 0.832680250783699 loss: 369.17823028564453 test_loss: 20.12113380432129 test_acc: 0.8652802893309223\n",
      "epoch 343 total_train_acc: 0.8491379310344828 loss: 394.89825439453125 test_loss: 0.0 test_acc: 0.864376130198915\n",
      "epoch 344 total_train_acc: 0.8436520376175548 loss: 428.86280822753906 test_loss: 0.0 test_acc: 0.8499095840867993\n",
      "epoch 345 total_train_acc: 0.8275862068965517 loss: 446.33758544921875 test_loss: 0.0 test_acc: 0.8670886075949367\n",
      "epoch 346 total_train_acc: 0.8467868338557993 loss: 323.3142776489258 test_loss: 95.62740325927734 test_acc: 0.8634719710669078\n",
      "epoch 347 total_train_acc: 0.8346394984326019 loss: 350.32151794433594 test_loss: 15.78997802734375 test_acc: 0.8670886075949367\n",
      "epoch 348 total_train_acc: 0.8334639498432602 loss: 407.10901641845703 test_loss: 277.6499938964844 test_acc: 0.8652802893309223\n",
      "epoch 349 total_train_acc: 0.8483542319749217 loss: 303.99517822265625 test_loss: 4.382405757904053 test_acc: 0.8661844484629295\n",
      "epoch 350 total_train_acc: 0.8373824451410659 loss: 358.1451721191406 test_loss: 4.275125980377197 test_acc: 0.8670886075949367\n",
      "epoch 351 total_train_acc: 0.835423197492163 loss: 389.3116989135742 test_loss: 9.934104383546583e-08 test_acc: 0.8634719710669078\n",
      "epoch 352 total_train_acc: 0.8362068965517241 loss: 346.34588623046875 test_loss: 102.4698486328125 test_acc: 0.8625678119349005\n",
      "epoch 353 total_train_acc: 0.8522727272727273 loss: 317.52734375 test_loss: 38.59912109375 test_acc: 0.864376130198915\n",
      "epoch 354 total_train_acc: 0.8522727272727273 loss: 292.0406494140625 test_loss: 47.025634765625 test_acc: 0.8661844484629295\n",
      "epoch 355 total_train_acc: 0.8393416927899686 loss: 342.5059127807617 test_loss: 349.6640625 test_acc: 0.8679927667269439\n",
      "epoch 356 total_train_acc: 0.829153605015674 loss: 346.8279571533203 test_loss: 202.3658905029297 test_acc: 0.8698010849909584\n",
      "epoch 357 total_train_acc: 0.8467868338557993 loss: 342.1570816040039 test_loss: 0.0 test_acc: 0.864376130198915\n",
      "epoch 358 total_train_acc: 0.8432601880877743 loss: 334.0551223754883 test_loss: 0.0 test_acc: 0.8625678119349005\n",
      "epoch 359 total_train_acc: 0.844435736677116 loss: 351.95027923583984 test_loss: 77.671630859375 test_acc: 0.8652802893309223\n",
      "epoch 360 total_train_acc: 0.8456112852664577 loss: 334.6930389404297 test_loss: 0.0 test_acc: 0.8670886075949367\n",
      "epoch 361 total_train_acc: 0.8358150470219435 loss: 329.6983871459961 test_loss: 10.276958465576172 test_acc: 0.8679927667269439\n",
      "epoch 362 total_train_acc: 0.8456112852664577 loss: 320.67166900634766 test_loss: 0.0 test_acc: 0.8661844484629295\n",
      "epoch 363 total_train_acc: 0.8428683385579937 loss: 298.8049850463867 test_loss: 0.82362300157547 test_acc: 0.8688969258589512\n",
      "epoch 364 total_train_acc: 0.8428683385579937 loss: 311.3870849609375 test_loss: 65.4775390625 test_acc: 0.8707052441229657\n",
      "epoch 365 total_train_acc: 0.8452194357366771 loss: 301.5503387451172 test_loss: 104.50308990478516 test_acc: 0.8670886075949367\n",
      "epoch 366 total_train_acc: 0.8334639498432602 loss: 300.5211868286133 test_loss: 120.85990142822266 test_acc: 0.8698010849909584\n",
      "epoch 367 total_train_acc: 0.8463949843260188 loss: 296.21348571777344 test_loss: 0.0 test_acc: 0.8679927667269439\n",
      "epoch 368 total_train_acc: 0.8397335423197492 loss: 304.68797302246094 test_loss: 26.758188247680664 test_acc: 0.8652802893309223\n",
      "epoch 369 total_train_acc: 0.835423197492163 loss: 321.8183288574219 test_loss: 189.61737060546875 test_acc: 0.8634719710669078\n",
      "epoch 370 total_train_acc: 0.838166144200627 loss: 355.12081146240234 test_loss: 0.0 test_acc: 0.8264014466546112\n",
      "epoch 371 total_train_acc: 0.8287617554858934 loss: 410.41414642333984 test_loss: 26.05171775817871 test_acc: 0.864376130198915\n",
      "epoch 372 total_train_acc: 0.8365987460815048 loss: 350.1094741821289 test_loss: 43.61995315551758 test_acc: 0.864376130198915\n",
      "epoch 373 total_train_acc: 0.8405172413793104 loss: 339.3601379394531 test_loss: 56.92133712768555 test_acc: 0.8688969258589512\n",
      "epoch 374 total_train_acc: 0.8499216300940439 loss: 309.1850128173828 test_loss: 27.36197853088379 test_acc: 0.8652802893309223\n",
      "epoch 375 total_train_acc: 0.8405172413793104 loss: 306.46836853027344 test_loss: 75.3031005859375 test_acc: 0.8698010849909584\n",
      "epoch 376 total_train_acc: 0.8350313479623824 loss: 317.47606658935547 test_loss: 0.00047502704546786845 test_acc: 0.8652802893309223\n",
      "epoch 377 total_train_acc: 0.8428683385579937 loss: 293.6293258666992 test_loss: 0.0 test_acc: 0.8688969258589512\n",
      "epoch 378 total_train_acc: 0.8448275862068966 loss: 290.93299865722656 test_loss: 0.0 test_acc: 0.8679927667269439\n",
      "epoch 379 total_train_acc: 0.8577586206896551 loss: 261.63236236572266 test_loss: 10.755208015441895 test_acc: 0.8698010849909584\n",
      "epoch 380 total_train_acc: 0.8487460815047022 loss: 289.2042236328125 test_loss: 13.979403495788574 test_acc: 0.8716094032549728\n",
      "epoch 381 total_train_acc: 0.85423197492163 loss: 233.89948272705078 test_loss: 467.4195251464844 test_acc: 0.8707052441229657\n",
      "epoch 382 total_train_acc: 0.8479623824451411 loss: 261.8277587890625 test_loss: 0.0 test_acc: 0.864376130198915\n",
      "epoch 383 total_train_acc: 0.8491379310344828 loss: 280.61534881591797 test_loss: 0.0 test_acc: 0.8679927667269439\n",
      "epoch 384 total_train_acc: 0.8554075235109718 loss: 244.9389877319336 test_loss: 51.63279342651367 test_acc: 0.8661844484629295\n",
      "epoch 385 total_train_acc: 0.8385579937304075 loss: 281.42352294921875 test_loss: 0.0 test_acc: 0.8688969258589512\n",
      "epoch 386 total_train_acc: 0.850705329153605 loss: 263.77201080322266 test_loss: 0.0 test_acc: 0.8734177215189873\n",
      "epoch 387 total_train_acc: 0.8448275862068966 loss: 260.4716033935547 test_loss: 81.437744140625 test_acc: 0.8716094032549728\n",
      "epoch 388 total_train_acc: 0.8424764890282131 loss: 282.7917175292969 test_loss: 113.65169525146484 test_acc: 0.8652802893309223\n",
      "epoch 389 total_train_acc: 0.8491379310344828 loss: 265.15103912353516 test_loss: 0.0 test_acc: 0.8698010849909584\n",
      "epoch 390 total_train_acc: 0.8479623824451411 loss: 265.1453094482422 test_loss: 9.63878345489502 test_acc: 0.8679927667269439\n",
      "epoch 391 total_train_acc: 0.8546238244514106 loss: 241.6741943359375 test_loss: 33.872314453125 test_acc: 0.8698010849909584\n",
      "epoch 392 total_train_acc: 0.8491379310344828 loss: 256.90564727783203 test_loss: 0.0 test_acc: 0.864376130198915\n",
      "epoch 393 total_train_acc: 0.8413009404388715 loss: 231.67556381225586 test_loss: 0.0 test_acc: 0.8652802893309223\n",
      "epoch 394 total_train_acc: 0.8522727272727273 loss: 253.98660278320312 test_loss: 248.3887939453125 test_acc: 0.8707052441229657\n",
      "epoch 395 total_train_acc: 0.8491379310344828 loss: 241.84561157226562 test_loss: 49.50006103515625 test_acc: 0.8707052441229657\n",
      "epoch 396 total_train_acc: 0.8503134796238244 loss: 243.01764678955078 test_loss: 10.49846363067627 test_acc: 0.8688969258589512\n",
      "epoch 397 total_train_acc: 0.8499216300940439 loss: 249.549072265625 test_loss: 0.0 test_acc: 0.8652802893309223\n",
      "epoch 398 total_train_acc: 0.8475705329153606 loss: 241.7398223876953 test_loss: 0.0 test_acc: 0.8698010849909584\n",
      "epoch 399 total_train_acc: 0.8420846394984326 loss: 247.9547348022461 test_loss: 101.507080078125 test_acc: 0.8752260397830018\n",
      "epoch 400 total_train_acc: 0.8530564263322884 loss: 240.36615753173828 test_loss: 15.937907218933105 test_acc: 0.8679927667269439\n",
      "epoch 401 total_train_acc: 0.8471786833855799 loss: 232.22872161865234 test_loss: 0.0 test_acc: 0.8752260397830018\n",
      "epoch 402 total_train_acc: 0.8585423197492164 loss: 228.25367736816406 test_loss: 0.0 test_acc: 0.8734177215189873\n",
      "epoch 403 total_train_acc: 0.8510971786833855 loss: 215.79659271240234 test_loss: 0.0 test_acc: 0.8716094032549728\n",
      "epoch 404 total_train_acc: 0.8577586206896551 loss: 219.8318099975586 test_loss: 0.0 test_acc: 0.8761301989150091\n",
      "epoch 405 total_train_acc: 0.8499216300940439 loss: 199.26983261108398 test_loss: 0.0 test_acc: 0.8743218806509946\n",
      "epoch 406 total_train_acc: 0.8546238244514106 loss: 220.20809936523438 test_loss: 0.0 test_acc: 0.8707052441229657\n",
      "epoch 407 total_train_acc: 0.8557993730407524 loss: 267.38604736328125 test_loss: 23.51444435119629 test_acc: 0.8761301989150091\n",
      "epoch 408 total_train_acc: 0.8503134796238244 loss: 240.5552215576172 test_loss: 0.0 test_acc: 0.8752260397830018\n",
      "epoch 409 total_train_acc: 0.8448275862068966 loss: 232.68492889404297 test_loss: 155.2510528564453 test_acc: 0.8716094032549728\n",
      "epoch 410 total_train_acc: 0.8448275862068966 loss: 239.7223358154297 test_loss: 0.0 test_acc: 0.8716094032549728\n",
      "epoch 411 total_train_acc: 0.8491379310344828 loss: 231.46066284179688 test_loss: 90.23323822021484 test_acc: 0.8734177215189873\n",
      "epoch 412 total_train_acc: 0.8483542319749217 loss: 267.3449935913086 test_loss: 92.90792083740234 test_acc: 0.8707052441229657\n",
      "epoch 413 total_train_acc: 0.8440438871473355 loss: 233.43096160888672 test_loss: 0.0 test_acc: 0.8743218806509946\n",
      "epoch 414 total_train_acc: 0.8487460815047022 loss: 209.57273864746094 test_loss: 29.06192970275879 test_acc: 0.8743218806509946\n",
      "epoch 415 total_train_acc: 0.8510971786833855 loss: 219.82837677001953 test_loss: 85.08211517333984 test_acc: 0.8779385171790235\n",
      "epoch 416 total_train_acc: 0.8487460815047022 loss: 221.29460525512695 test_loss: 0.0 test_acc: 0.8779385171790235\n",
      "epoch 417 total_train_acc: 0.8487460815047022 loss: 186.74369430541992 test_loss: 25.55043601989746 test_acc: 0.8752260397830018\n",
      "epoch 418 total_train_acc: 0.8522727272727273 loss: 216.00463104248047 test_loss: 0.0 test_acc: 0.8743218806509946\n",
      "epoch 419 total_train_acc: 0.8546238244514106 loss: 210.26074600219727 test_loss: 13.802327156066895 test_acc: 0.8770343580470162\n",
      "epoch 420 total_train_acc: 0.8538401253918495 loss: 211.7321548461914 test_loss: 0.14232711493968964 test_acc: 0.8770343580470162\n",
      "epoch 421 total_train_acc: 0.8522727272727273 loss: 220.62506866455078 test_loss: 0.0 test_acc: 0.8743218806509946\n",
      "epoch 422 total_train_acc: 0.8538401253918495 loss: 213.7465476989746 test_loss: 32.356689453125 test_acc: 0.8698010849909584\n",
      "epoch 423 total_train_acc: 0.853448275862069 loss: 201.64625930786133 test_loss: 0.0 test_acc: 0.8688969258589512\n",
      "epoch 424 total_train_acc: 0.853448275862069 loss: 195.16418075561523 test_loss: 69.37662506103516 test_acc: 0.8743218806509946\n",
      "epoch 425 total_train_acc: 0.8432601880877743 loss: 188.62582397460938 test_loss: 0.0 test_acc: 0.8761301989150091\n",
      "epoch 426 total_train_acc: 0.8640282131661442 loss: 176.76956939697266 test_loss: 0.0 test_acc: 0.8743218806509946\n",
      "epoch 427 total_train_acc: 0.8440438871473355 loss: 226.81714630126953 test_loss: 0.0 test_acc: 0.8761301989150091\n",
      "epoch 428 total_train_acc: 0.8518808777429467 loss: 191.9983901977539 test_loss: 0.0 test_acc: 0.8779385171790235\n",
      "epoch 429 total_train_acc: 0.8503134796238244 loss: 207.70053100585938 test_loss: 22.94140625 test_acc: 0.8770343580470162\n",
      "epoch 430 total_train_acc: 0.8518808777429467 loss: 187.0650978088379 test_loss: 58.16023635864258 test_acc: 0.8770343580470162\n",
      "epoch 431 total_train_acc: 0.859717868338558 loss: 183.04985809326172 test_loss: 66.60282135009766 test_acc: 0.8779385171790235\n",
      "epoch 432 total_train_acc: 0.8514890282131662 loss: 210.37274169921875 test_loss: 0.0 test_acc: 0.8788426763110307\n",
      "epoch 433 total_train_acc: 0.8518808777429467 loss: 214.3788299560547 test_loss: 6.144327640533447 test_acc: 0.8788426763110307\n",
      "epoch 434 total_train_acc: 0.8479623824451411 loss: 195.86029815673828 test_loss: 47.38334274291992 test_acc: 0.8770343580470162\n",
      "epoch 435 total_train_acc: 0.8530564263322884 loss: 203.46551513671875 test_loss: 79.25528717041016 test_acc: 0.8779385171790235\n",
      "epoch 436 total_train_acc: 0.850705329153605 loss: 214.38136291503906 test_loss: 13.816447257995605 test_acc: 0.8734177215189873\n",
      "epoch 437 total_train_acc: 0.8554075235109718 loss: 206.85498809814453 test_loss: 0.0 test_acc: 0.8752260397830018\n",
      "epoch 438 total_train_acc: 0.8561912225705329 loss: 159.24840545654297 test_loss: 0.0 test_acc: 0.8761301989150091\n",
      "epoch 439 total_train_acc: 0.850705329153605 loss: 177.91426467895508 test_loss: 0.0 test_acc: 0.8770343580470162\n",
      "epoch 440 total_train_acc: 0.8577586206896551 loss: 190.9040756225586 test_loss: 33.21378707885742 test_acc: 0.8743218806509946\n",
      "epoch 441 total_train_acc: 0.8565830721003135 loss: 188.03885650634766 test_loss: 9.936279296875 test_acc: 0.8779385171790235\n",
      "epoch 442 total_train_acc: 0.8526645768025078 loss: 210.63833618164062 test_loss: 0.0 test_acc: 0.8752260397830018\n",
      "epoch 443 total_train_acc: 0.8620689655172413 loss: 172.03140258789062 test_loss: 0.0 test_acc: 0.8707052441229657\n",
      "epoch 444 total_train_acc: 0.8605015673981191 loss: 155.64723587036133 test_loss: 0.0 test_acc: 0.879746835443038\n",
      "epoch 445 total_train_acc: 0.8557993730407524 loss: 195.52061080932617 test_loss: 8.683802604675293 test_acc: 0.8752260397830018\n",
      "epoch 446 total_train_acc: 0.8612852664576802 loss: 166.80593490600586 test_loss: 125.71435546875 test_acc: 0.8761301989150091\n",
      "epoch 447 total_train_acc: 0.859717868338558 loss: 168.40035247802734 test_loss: 85.75982666015625 test_acc: 0.8716094032549728\n",
      "epoch 448 total_train_acc: 0.8522727272727273 loss: 198.4951515197754 test_loss: 68.80810546875 test_acc: 0.8725135623869801\n",
      "epoch 449 total_train_acc: 0.8601097178683386 loss: 167.70517349243164 test_loss: 0.0 test_acc: 0.8770343580470162\n",
      "epoch 450 total_train_acc: 0.8483542319749217 loss: 202.97705459594727 test_loss: 107.65592193603516 test_acc: 0.8824593128390597\n",
      "epoch 451 total_train_acc: 0.8620689655172413 loss: 174.23419570922852 test_loss: 81.65884399414062 test_acc: 0.8761301989150091\n",
      "epoch 452 total_train_acc: 0.8546238244514106 loss: 198.4670524597168 test_loss: 23.032470703125 test_acc: 0.879746835443038\n",
      "epoch 453 total_train_acc: 0.8593260188087775 loss: 153.74902725219727 test_loss: 0.0 test_acc: 0.8752260397830018\n",
      "epoch 454 total_train_acc: 0.8530564263322884 loss: 171.9676742553711 test_loss: 0.0 test_acc: 0.8752260397830018\n",
      "epoch 455 total_train_acc: 0.8546238244514106 loss: 187.16387939453125 test_loss: 110.13944244384766 test_acc: 0.8752260397830018\n",
      "epoch 456 total_train_acc: 0.8550156739811913 loss: 177.73548889160156 test_loss: 0.0 test_acc: 0.8743218806509946\n",
      "epoch 457 total_train_acc: 0.8467868338557993 loss: 193.9604377746582 test_loss: 318.3868408203125 test_acc: 0.8761301989150091\n",
      "epoch 458 total_train_acc: 0.8616771159874608 loss: 180.26426315307617 test_loss: 169.56849670410156 test_acc: 0.8779385171790235\n",
      "epoch 459 total_train_acc: 0.8561912225705329 loss: 175.35663986206055 test_loss: 0.0 test_acc: 0.8743218806509946\n",
      "epoch 460 total_train_acc: 0.8538401253918495 loss: 178.39191436767578 test_loss: 0.0 test_acc: 0.8806509945750453\n",
      "epoch 461 total_train_acc: 0.8636363636363636 loss: 154.50288772583008 test_loss: 0.0 test_acc: 0.8806509945750453\n",
      "epoch 462 total_train_acc: 0.8648119122257053 loss: 144.46237182617188 test_loss: 0.0 test_acc: 0.8806509945750453\n",
      "epoch 463 total_train_acc: 0.8581504702194357 loss: 181.97676849365234 test_loss: 0.0 test_acc: 0.8833634719710669\n",
      "epoch 464 total_train_acc: 0.859717868338558 loss: 181.26198196411133 test_loss: 29.51639747619629 test_acc: 0.8770343580470162\n",
      "epoch 465 total_train_acc: 0.8432601880877743 loss: 176.12592315673828 test_loss: 19.171142578125 test_acc: 0.8815551537070524\n",
      "epoch 466 total_train_acc: 0.8561912225705329 loss: 157.82999801635742 test_loss: 0.0 test_acc: 0.8815551537070524\n",
      "epoch 467 total_train_acc: 0.8526645768025078 loss: 179.78813552856445 test_loss: 38.57735061645508 test_acc: 0.879746835443038\n",
      "epoch 468 total_train_acc: 0.856974921630094 loss: 158.0857925415039 test_loss: 1.444374918937683 test_acc: 0.8806509945750453\n",
      "epoch 469 total_train_acc: 0.8605015673981191 loss: 151.18490600585938 test_loss: 0.0 test_acc: 0.8815551537070524\n",
      "epoch 470 total_train_acc: 0.8487460815047022 loss: 175.51372528076172 test_loss: 3.9929606914520264 test_acc: 0.8806509945750453\n",
      "epoch 471 total_train_acc: 0.859717868338558 loss: 167.12348175048828 test_loss: 0.0 test_acc: 0.8806509945750453\n",
      "epoch 472 total_train_acc: 0.8573667711598746 loss: 153.39780807495117 test_loss: 266.1938781738281 test_acc: 0.8788426763110307\n",
      "epoch 473 total_train_acc: 0.8648119122257053 loss: 163.97886657714844 test_loss: 0.0 test_acc: 0.879746835443038\n",
      "epoch 474 total_train_acc: 0.85423197492163 loss: 168.79262161254883 test_loss: 50.83382034301758 test_acc: 0.8806509945750453\n",
      "epoch 475 total_train_acc: 0.8608934169278997 loss: 143.87783813476562 test_loss: 0.0 test_acc: 0.879746835443038\n",
      "epoch 476 total_train_acc: 0.8526645768025078 loss: 147.2912712097168 test_loss: 0.0 test_acc: 0.8824593128390597\n",
      "epoch 477 total_train_acc: 0.8593260188087775 loss: 152.0148048400879 test_loss: 3.5635986328125 test_acc: 0.8770343580470162\n",
      "epoch 478 total_train_acc: 0.8605015673981191 loss: 163.6693115234375 test_loss: 0.0 test_acc: 0.8752260397830018\n",
      "epoch 479 total_train_acc: 0.8550156739811913 loss: 167.1862335205078 test_loss: 0.0 test_acc: 0.8806509945750453\n",
      "epoch 480 total_train_acc: 0.862460815047022 loss: 142.28336334228516 test_loss: 0.0 test_acc: 0.8815551537070524\n",
      "epoch 481 total_train_acc: 0.8577586206896551 loss: 142.08734512329102 test_loss: 0.0 test_acc: 0.8806509945750453\n",
      "epoch 482 total_train_acc: 0.8616771159874608 loss: 150.2802963256836 test_loss: 5.58575439453125 test_acc: 0.8761301989150091\n",
      "epoch 483 total_train_acc: 0.8612852664576802 loss: 143.7677001953125 test_loss: 0.0 test_acc: 0.8770343580470162\n",
      "epoch 484 total_train_acc: 0.8605015673981191 loss: 152.69269943237305 test_loss: 0.0 test_acc: 0.8806509945750453\n",
      "epoch 485 total_train_acc: 0.8487460815047022 loss: 163.70274353027344 test_loss: 13.218281745910645 test_acc: 0.8761301989150091\n",
      "epoch 486 total_train_acc: 0.8652037617554859 loss: 140.8433837890625 test_loss: 0.0 test_acc: 0.8779385171790235\n",
      "epoch 487 total_train_acc: 0.850705329153605 loss: 152.70746612548828 test_loss: 140.4666748046875 test_acc: 0.8806509945750453\n",
      "epoch 488 total_train_acc: 0.8522727272727273 loss: 149.33529663085938 test_loss: 32.66139602661133 test_acc: 0.8824593128390597\n",
      "epoch 489 total_train_acc: 0.8702978056426333 loss: 139.49090576171875 test_loss: 26.321298599243164 test_acc: 0.8806509945750453\n",
      "epoch 490 total_train_acc: 0.8554075235109718 loss: 142.98920059204102 test_loss: 59.26055908203125 test_acc: 0.8770343580470162\n",
      "epoch 491 total_train_acc: 0.8659874608150471 loss: 138.6706085205078 test_loss: 311.4267883300781 test_acc: 0.8815551537070524\n",
      "epoch 492 total_train_acc: 0.8663793103448276 loss: 145.18942642211914 test_loss: 39.78367233276367 test_acc: 0.8815551537070524\n",
      "epoch 493 total_train_acc: 0.8585423197492164 loss: 161.41876983642578 test_loss: 66.46405029296875 test_acc: 0.8806509945750453\n",
      "epoch 494 total_train_acc: 0.853448275862069 loss: 140.42942428588867 test_loss: 413.5615234375 test_acc: 0.8770343580470162\n",
      "epoch 495 total_train_acc: 0.856974921630094 loss: 162.7270965576172 test_loss: 48.7476806640625 test_acc: 0.8824593128390597\n",
      "epoch 496 total_train_acc: 0.8667711598746082 loss: 134.49496841430664 test_loss: 20.02701759338379 test_acc: 0.8815551537070524\n",
      "epoch 497 total_train_acc: 0.8557993730407524 loss: 137.18578720092773 test_loss: 0.0 test_acc: 0.8788426763110307\n",
      "epoch 498 total_train_acc: 0.8526645768025078 loss: 136.58359146118164 test_loss: 30.0008544921875 test_acc: 0.8779385171790235\n",
      "epoch 499 total_train_acc: 0.8632445141065831 loss: 132.79632568359375 test_loss: 0.0 test_acc: 0.8815551537070524\n",
      "epoch 500 total_train_acc: 0.8581504702194357 loss: 120.84606170654297 test_loss: 0.0 test_acc: 0.8815551537070524\n",
      "epoch 501 total_train_acc: 0.85423197492163 loss: 149.57645797729492 test_loss: 0.0 test_acc: 0.8842676311030742\n",
      "epoch 502 total_train_acc: 0.8616771159874608 loss: 125.22459411621094 test_loss: 0.0 test_acc: 0.8779385171790235\n",
      "epoch 503 total_train_acc: 0.8620689655172413 loss: 133.13771057128906 test_loss: 0.0 test_acc: 0.8779385171790235\n",
      "epoch 504 total_train_acc: 0.8510971786833855 loss: 136.33903884887695 test_loss: 47.94669723510742 test_acc: 0.8779385171790235\n",
      "epoch 505 total_train_acc: 0.8667711598746082 loss: 141.09425354003906 test_loss: 0.0 test_acc: 0.8806509945750453\n",
      "epoch 506 total_train_acc: 0.859717868338558 loss: 156.96963119506836 test_loss: 101.3545913696289 test_acc: 0.8806509945750453\n",
      "epoch 507 total_train_acc: 0.8632445141065831 loss: 130.43789672851562 test_loss: 473.5939025878906 test_acc: 0.8707052441229657\n",
      "epoch 508 total_train_acc: 0.8675548589341693 loss: 130.22809600830078 test_loss: 222.3739013671875 test_acc: 0.8752260397830018\n",
      "epoch 509 total_train_acc: 0.8577586206896551 loss: 138.6753807067871 test_loss: 44.73146438598633 test_acc: 0.8806509945750453\n",
      "epoch 510 total_train_acc: 0.8620689655172413 loss: 137.21817016601562 test_loss: 0.0 test_acc: 0.8743218806509946\n",
      "epoch 511 total_train_acc: 0.8691222570532915 loss: 117.6932373046875 test_loss: 36.96858596801758 test_acc: 0.8761301989150091\n",
      "epoch 512 total_train_acc: 0.8589341692789969 loss: 127.54298400878906 test_loss: 0.0 test_acc: 0.8806509945750453\n",
      "epoch 513 total_train_acc: 0.8648119122257053 loss: 127.38556671142578 test_loss: 101.99700927734375 test_acc: 0.8806509945750453\n",
      "epoch 514 total_train_acc: 0.8695141065830722 loss: 128.92394256591797 test_loss: 8.184306144714355 test_acc: 0.8806509945750453\n",
      "epoch 515 total_train_acc: 0.8620689655172413 loss: 128.6358299255371 test_loss: 1.229352355003357 test_acc: 0.8770343580470162\n",
      "epoch 516 total_train_acc: 0.8699059561128527 loss: 113.37756729125977 test_loss: 0.0 test_acc: 0.8806509945750453\n",
      "epoch 517 total_train_acc: 0.8636363636363636 loss: 122.5544204711914 test_loss: 116.2667007446289 test_acc: 0.8815551537070524\n",
      "epoch 518 total_train_acc: 0.8675548589341693 loss: 118.32360458374023 test_loss: 0.0 test_acc: 0.8833634719710669\n",
      "epoch 519 total_train_acc: 0.8726489028213166 loss: 119.19218826293945 test_loss: 9.10870361328125 test_acc: 0.8779385171790235\n",
      "epoch 520 total_train_acc: 0.8561912225705329 loss: 135.96561813354492 test_loss: 130.30618286132812 test_acc: 0.8815551537070524\n",
      "epoch 521 total_train_acc: 0.856974921630094 loss: 120.35660934448242 test_loss: 0.29387590289115906 test_acc: 0.8815551537070524\n",
      "epoch 522 total_train_acc: 0.8702978056426333 loss: 129.34210205078125 test_loss: 5.749104976654053 test_acc: 0.8770343580470162\n",
      "epoch 523 total_train_acc: 0.8710815047021944 loss: 110.38418197631836 test_loss: 13.791300773620605 test_acc: 0.8806509945750453\n",
      "epoch 524 total_train_acc: 0.8632445141065831 loss: 123.80248641967773 test_loss: 0.0 test_acc: 0.8833634719710669\n",
      "epoch 525 total_train_acc: 0.856974921630094 loss: 117.59223556518555 test_loss: 29.23728370666504 test_acc: 0.8752260397830018\n",
      "epoch 526 total_train_acc: 0.8659874608150471 loss: 105.07504272460938 test_loss: 0.0 test_acc: 0.8788426763110307\n",
      "epoch 527 total_train_acc: 0.8620689655172413 loss: 130.67026138305664 test_loss: 139.0313262939453 test_acc: 0.8851717902350814\n",
      "epoch 528 total_train_acc: 0.8636363636363636 loss: 124.36233139038086 test_loss: 0.0 test_acc: 0.8815551537070524\n",
      "epoch 529 total_train_acc: 0.8632445141065831 loss: 118.45307540893555 test_loss: 0.0 test_acc: 0.8824593128390597\n",
      "epoch 530 total_train_acc: 0.8628526645768025 loss: 108.67087173461914 test_loss: 0.0 test_acc: 0.8833634719710669\n",
      "epoch 531 total_train_acc: 0.8557993730407524 loss: 129.61734771728516 test_loss: 0.0 test_acc: 0.8761301989150091\n",
      "epoch 532 total_train_acc: 0.8616771159874608 loss: 132.03790664672852 test_loss: 3.56884765625 test_acc: 0.8815551537070524\n",
      "epoch 533 total_train_acc: 0.8636363636363636 loss: 114.75994873046875 test_loss: 0.0 test_acc: 0.8851717902350814\n",
      "epoch 534 total_train_acc: 0.8679467084639498 loss: 112.85127639770508 test_loss: 0.0 test_acc: 0.8788426763110307\n",
      "epoch 535 total_train_acc: 0.8718652037617555 loss: 120.2362289428711 test_loss: 0.0 test_acc: 0.8824593128390597\n",
      "epoch 536 total_train_acc: 0.8687304075235109 loss: 120.96254348754883 test_loss: 0.0 test_acc: 0.8824593128390597\n",
      "epoch 537 total_train_acc: 0.8659874608150471 loss: 98.86285209655762 test_loss: 0.0 test_acc: 0.8824593128390597\n",
      "epoch 538 total_train_acc: 0.8546238244514106 loss: 126.17804336547852 test_loss: 9.167724609375 test_acc: 0.8851717902350814\n",
      "epoch 539 total_train_acc: 0.8695141065830722 loss: 102.69710159301758 test_loss: 3.874256890412653e-06 test_acc: 0.8815551537070524\n",
      "epoch 540 total_train_acc: 0.8742163009404389 loss: 108.17435073852539 test_loss: 6.242431640625 test_acc: 0.8860759493670886\n",
      "epoch 541 total_train_acc: 0.859717868338558 loss: 108.88090515136719 test_loss: 22.641530990600586 test_acc: 0.8851717902350814\n",
      "epoch 542 total_train_acc: 0.8679467084639498 loss: 121.81331634521484 test_loss: 21.578140258789062 test_acc: 0.8815551537070524\n",
      "epoch 543 total_train_acc: 0.8659874608150471 loss: 100.66043090820312 test_loss: 35.6966552734375 test_acc: 0.8833634719710669\n",
      "epoch 544 total_train_acc: 0.8683385579937304 loss: 97.61123657226562 test_loss: 172.6622772216797 test_acc: 0.8806509945750453\n",
      "epoch 545 total_train_acc: 0.8769592476489029 loss: 90.58206748962402 test_loss: 55.71484375 test_acc: 0.8860759493670886\n",
      "epoch 546 total_train_acc: 0.8730407523510971 loss: 94.40559959411621 test_loss: 0.0 test_acc: 0.8851717902350814\n",
      "epoch 547 total_train_acc: 0.8632445141065831 loss: 105.85382843017578 test_loss: 0.0 test_acc: 0.879746835443038\n",
      "epoch 548 total_train_acc: 0.8702978056426333 loss: 103.84909057617188 test_loss: 43.6850700378418 test_acc: 0.8878842676311031\n",
      "epoch 549 total_train_acc: 0.8636363636363636 loss: 105.42206382751465 test_loss: 73.19722747802734 test_acc: 0.8869801084990958\n",
      "epoch 550 total_train_acc: 0.8687304075235109 loss: 110.92902946472168 test_loss: 24.912841796875 test_acc: 0.8860759493670886\n",
      "epoch 551 total_train_acc: 0.875 loss: 91.50725555419922 test_loss: 0.0 test_acc: 0.8878842676311031\n",
      "epoch 552 total_train_acc: 0.8710815047021944 loss: 107.21274185180664 test_loss: 0.0 test_acc: 0.8860759493670886\n",
      "epoch 553 total_train_acc: 0.8710815047021944 loss: 97.36341667175293 test_loss: 0.0 test_acc: 0.8887884267631103\n",
      "epoch 554 total_train_acc: 0.8679467084639498 loss: 102.44881439208984 test_loss: 22.871002197265625 test_acc: 0.8842676311030742\n",
      "epoch 555 total_train_acc: 0.8710815047021944 loss: 91.99566841125488 test_loss: 44.63454055786133 test_acc: 0.8869801084990958\n",
      "epoch 556 total_train_acc: 0.8691222570532915 loss: 103.5841178894043 test_loss: 9.643940925598145 test_acc: 0.8851717902350814\n",
      "epoch 557 total_train_acc: 0.8785266457680251 loss: 99.6093978881836 test_loss: 0.0 test_acc: 0.8842676311030742\n",
      "epoch 558 total_train_acc: 0.8702978056426333 loss: 103.5844612121582 test_loss: 0.0 test_acc: 0.8869801084990958\n",
      "epoch 559 total_train_acc: 0.8706896551724138 loss: 119.25459861755371 test_loss: 198.64939880371094 test_acc: 0.8860759493670886\n",
      "epoch 560 total_train_acc: 0.8695141065830722 loss: 90.19123268127441 test_loss: 0.0 test_acc: 0.8869801084990958\n",
      "epoch 561 total_train_acc: 0.8663793103448276 loss: 104.97469139099121 test_loss: 0.0 test_acc: 0.8869801084990958\n",
      "epoch 562 total_train_acc: 0.8687304075235109 loss: 104.93917846679688 test_loss: 110.028564453125 test_acc: 0.8887884267631103\n",
      "epoch 563 total_train_acc: 0.8695141065830722 loss: 100.35608863830566 test_loss: 41.587772369384766 test_acc: 0.8878842676311031\n",
      "epoch 564 total_train_acc: 0.8726489028213166 loss: 98.50713157653809 test_loss: 15.121418952941895 test_acc: 0.8869801084990958\n",
      "epoch 565 total_train_acc: 0.877742946708464 loss: 98.03380393981934 test_loss: 17.30735206604004 test_acc: 0.891500904159132\n",
      "epoch 566 total_train_acc: 0.8769592476489029 loss: 90.01742935180664 test_loss: 20.95591163635254 test_acc: 0.8896925858951176\n",
      "epoch 567 total_train_acc: 0.8753918495297806 loss: 83.47553634643555 test_loss: 36.70443344116211 test_acc: 0.8860759493670886\n",
      "epoch 568 total_train_acc: 0.8710815047021944 loss: 109.03999710083008 test_loss: 30.6824951171875 test_acc: 0.8896925858951176\n",
      "epoch 569 total_train_acc: 0.8699059561128527 loss: 123.05691146850586 test_loss: 0.16609452664852142 test_acc: 0.8896925858951176\n",
      "epoch 570 total_train_acc: 0.8765673981191222 loss: 100.58687782287598 test_loss: 5.9820556640625 test_acc: 0.8924050632911392\n",
      "epoch 571 total_train_acc: 0.8718652037617555 loss: 96.41606521606445 test_loss: 0.0003326392325107008 test_acc: 0.8851717902350814\n",
      "epoch 572 total_train_acc: 0.8718652037617555 loss: 98.27245330810547 test_loss: 166.3358917236328 test_acc: 0.891500904159132\n",
      "epoch 573 total_train_acc: 0.8718652037617555 loss: 82.7452335357666 test_loss: 48.83982467651367 test_acc: 0.8905967450271248\n",
      "epoch 574 total_train_acc: 0.8655956112852664 loss: 91.9922924041748 test_loss: 0.0 test_acc: 0.8851717902350814\n",
      "epoch 575 total_train_acc: 0.8695141065830722 loss: 81.91173553466797 test_loss: 18.10713768005371 test_acc: 0.8878842676311031\n",
      "epoch 576 total_train_acc: 0.8695141065830722 loss: 85.69035148620605 test_loss: 11.09144115447998 test_acc: 0.8942133815551537\n",
      "epoch 577 total_train_acc: 0.8710815047021944 loss: 80.5831527709961 test_loss: 0.0 test_acc: 0.8887884267631103\n",
      "epoch 578 total_train_acc: 0.8765673981191222 loss: 91.77224922180176 test_loss: 21.42673683166504 test_acc: 0.8905967450271248\n",
      "epoch 579 total_train_acc: 0.8628526645768025 loss: 100.7374496459961 test_loss: 3.6240437030792236 test_acc: 0.8924050632911392\n",
      "epoch 580 total_train_acc: 0.8765673981191222 loss: 90.01168251037598 test_loss: 11.051167488098145 test_acc: 0.8860759493670886\n",
      "epoch 581 total_train_acc: 0.8757836990595611 loss: 83.71383857727051 test_loss: 0.0 test_acc: 0.8878842676311031\n",
      "epoch 582 total_train_acc: 0.872257053291536 loss: 95.62063980102539 test_loss: 0.0 test_acc: 0.8924050632911392\n",
      "epoch 583 total_train_acc: 0.875 loss: 82.56632232666016 test_loss: 0.0 test_acc: 0.8896925858951176\n",
      "epoch 584 total_train_acc: 0.8734326018808778 loss: 81.65003967285156 test_loss: 1.0544568300247192 test_acc: 0.8942133815551537\n",
      "epoch 585 total_train_acc: 0.8734326018808778 loss: 84.65318298339844 test_loss: 27.81899070739746 test_acc: 0.8933092224231465\n",
      "epoch 586 total_train_acc: 0.8812695924764891 loss: 87.20645141601562 test_loss: 44.605804443359375 test_acc: 0.8942133815551537\n",
      "epoch 587 total_train_acc: 0.8710815047021944 loss: 89.49777221679688 test_loss: 145.7337188720703 test_acc: 0.8942133815551537\n",
      "epoch 588 total_train_acc: 0.8738244514106583 loss: 103.22370338439941 test_loss: 0.0 test_acc: 0.8860759493670886\n",
      "epoch 589 total_train_acc: 0.8710815047021944 loss: 88.65205383300781 test_loss: 14.549641609191895 test_acc: 0.8933092224231465\n",
      "epoch 590 total_train_acc: 0.8808777429467085 loss: 85.02557563781738 test_loss: 3.6634724140167236 test_acc: 0.8924050632911392\n",
      "epoch 591 total_train_acc: 0.8847962382445141 loss: 85.9393424987793 test_loss: 0.0 test_acc: 0.8933092224231465\n",
      "epoch 592 total_train_acc: 0.8851880877742947 loss: 67.18819427490234 test_loss: 0.0 test_acc: 0.8933092224231465\n",
      "epoch 593 total_train_acc: 0.875 loss: 72.14396667480469 test_loss: 28.92867088317871 test_acc: 0.891500904159132\n",
      "epoch 594 total_train_acc: 0.8714733542319749 loss: 93.13428115844727 test_loss: 4.763717879541218e-05 test_acc: 0.8960216998191681\n",
      "epoch 595 total_train_acc: 0.8675548589341693 loss: 97.57548904418945 test_loss: 5.449320316314697 test_acc: 0.8842676311030742\n",
      "epoch 596 total_train_acc: 0.8695141065830722 loss: 95.92681694030762 test_loss: 188.0961456298828 test_acc: 0.8942133815551537\n",
      "epoch 597 total_train_acc: 0.8769592476489029 loss: 81.1655216217041 test_loss: 6.9434814453125 test_acc: 0.8933092224231465\n",
      "epoch 598 total_train_acc: 0.8738244514106583 loss: 81.87044906616211 test_loss: 0.0 test_acc: 0.8924050632911392\n",
      "epoch 599 total_train_acc: 0.8797021943573667 loss: 75.88973999023438 test_loss: 0.0 test_acc: 0.8960216998191681\n",
      "epoch 600 total_train_acc: 0.8765673981191222 loss: 64.10792541503906 test_loss: 19.211395263671875 test_acc: 0.8942133815551537\n",
      "epoch 601 total_train_acc: 0.8773510971786834 loss: 87.46821022033691 test_loss: 5.731119632720947 test_acc: 0.8978300180831826\n",
      "epoch 602 total_train_acc: 0.8808777429467085 loss: 83.34545516967773 test_loss: 0.0 test_acc: 0.9005424954792043\n",
      "epoch 603 total_train_acc: 0.875 loss: 77.64312934875488 test_loss: 0.0 test_acc: 0.9005424954792043\n",
      "epoch 604 total_train_acc: 0.872257053291536 loss: 74.3405647277832 test_loss: 0.0 test_acc: 0.8978300180831826\n",
      "epoch 605 total_train_acc: 0.8769592476489029 loss: 79.8032341003418 test_loss: 6.469326019287109 test_acc: 0.8987341772151899\n",
      "epoch 606 total_train_acc: 0.8773510971786834 loss: 72.16792106628418 test_loss: 0.0 test_acc: 0.8942133815551537\n",
      "epoch 607 total_train_acc: 0.8859717868338558 loss: 83.63935470581055 test_loss: 73.55574798583984 test_acc: 0.8960216998191681\n",
      "epoch 608 total_train_acc: 0.8761755485893417 loss: 65.18355751037598 test_loss: 0.0 test_acc: 0.8960216998191681\n",
      "epoch 609 total_train_acc: 0.8797021943573667 loss: 80.67368698120117 test_loss: 14.737972259521484 test_acc: 0.8978300180831826\n",
      "epoch 610 total_train_acc: 0.8804858934169278 loss: 77.14372825622559 test_loss: 0.0 test_acc: 0.8978300180831826\n",
      "epoch 611 total_train_acc: 0.8793103448275862 loss: 67.22129440307617 test_loss: 0.00028619429212994874 test_acc: 0.8924050632911392\n",
      "epoch 612 total_train_acc: 0.8695141065830722 loss: 79.7732925415039 test_loss: 0.0 test_acc: 0.9005424954792043\n",
      "epoch 613 total_train_acc: 0.8824451410658307 loss: 68.29670524597168 test_loss: 42.87898635864258 test_acc: 0.8969258589511754\n",
      "epoch 614 total_train_acc: 0.8840125391849529 loss: 60.20047950744629 test_loss: 1.120968222618103 test_acc: 0.895117540687161\n",
      "epoch 615 total_train_acc: 0.8800940438871473 loss: 76.10744857788086 test_loss: 0.0 test_acc: 0.8905967450271248\n",
      "epoch 616 total_train_acc: 0.8800940438871473 loss: 82.91021728515625 test_loss: 47.82719039916992 test_acc: 0.8969258589511754\n",
      "epoch 617 total_train_acc: 0.8797021943573667 loss: 72.98431777954102 test_loss: 1.9868213740892315e-08 test_acc: 0.8969258589511754\n",
      "epoch 618 total_train_acc: 0.8824451410658307 loss: 69.9945182800293 test_loss: 3.361830472946167 test_acc: 0.8987341772151899\n",
      "epoch 619 total_train_acc: 0.8730407523510971 loss: 77.5209732055664 test_loss: 0.0 test_acc: 0.891500904159132\n",
      "epoch 620 total_train_acc: 0.8828369905956113 loss: 63.79055595397949 test_loss: 0.0 test_acc: 0.8987341772151899\n",
      "epoch 621 total_train_acc: 0.8855799373040752 loss: 70.64885711669922 test_loss: 3.973642392907095e-08 test_acc: 0.8987341772151899\n",
      "epoch 622 total_train_acc: 0.8785266457680251 loss: 81.16485214233398 test_loss: 4.940328121185303 test_acc: 0.8896925858951176\n",
      "epoch 623 total_train_acc: 0.8738244514106583 loss: 79.06045532226562 test_loss: 0.0 test_acc: 0.9005424954792043\n",
      "epoch 624 total_train_acc: 0.8687304075235109 loss: 74.13343048095703 test_loss: 237.2532501220703 test_acc: 0.8924050632911392\n",
      "epoch 625 total_train_acc: 0.8816614420062696 loss: 69.55617141723633 test_loss: 0.0 test_acc: 0.8978300180831826\n",
      "epoch 626 total_train_acc: 0.8828369905956113 loss: 55.64515495300293 test_loss: 0.0 test_acc: 0.903254972875226\n",
      "epoch 627 total_train_acc: 0.8765673981191222 loss: 71.5334701538086 test_loss: 34.393733978271484 test_acc: 0.8942133815551537\n",
      "epoch 628 total_train_acc: 0.8808777429467085 loss: 59.09007453918457 test_loss: 0.0 test_acc: 0.8969258589511754\n",
      "epoch 629 total_train_acc: 0.8797021943573667 loss: 66.33844184875488 test_loss: 0.00019699608674272895 test_acc: 0.8996383363471971\n",
      "epoch 630 total_train_acc: 0.8812695924764891 loss: 83.45559883117676 test_loss: 0.0 test_acc: 0.8960216998191681\n",
      "epoch 631 total_train_acc: 0.8816614420062696 loss: 71.91912460327148 test_loss: 0.0 test_acc: 0.8933092224231465\n",
      "epoch 632 total_train_acc: 0.8844043887147336 loss: 65.05158424377441 test_loss: 0.0 test_acc: 0.9014466546112115\n",
      "epoch 633 total_train_acc: 0.877742946708464 loss: 60.29714393615723 test_loss: 0.0 test_acc: 0.8942133815551537\n",
      "epoch 634 total_train_acc: 0.8832288401253918 loss: 63.33464050292969 test_loss: 10.108012199401855 test_acc: 0.8960216998191681\n",
      "epoch 635 total_train_acc: 0.8855799373040752 loss: 62.73838806152344 test_loss: 93.65833282470703 test_acc: 0.9014466546112115\n",
      "epoch 636 total_train_acc: 0.8800940438871473 loss: 67.51907730102539 test_loss: 54.70828628540039 test_acc: 0.9005424954792043\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9732/3774229964.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[0mtotal_correct\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;31m# Classify aux_dataset examples as open set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mbatch_unkn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_un_loader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m         \u001b[0mun_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_unkn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[0mun_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mun_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ml2\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    519\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 521\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ml2\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    559\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 561\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    562\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ml2\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\ml2\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m     82\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'each element in list of batch should be of equal size'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ml2\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     82\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'each element in list of batch should be of equal size'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ml2\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m     54\u001b[0m             \u001b[0mstorage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0melem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_new_shared\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0melem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0melem_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__module__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'numpy'\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'str_'\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'string_'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 视情况加载已保存文件\n",
    "# checkpoint_pretrain = torch.load('./ckp/c6c6_ep_1600_2021_12_05_16_38_46.pth')\n",
    "# net.load_state_dict(checkpoint_pretrain['model'])\n",
    "ckpDir = './/ckp//c6addnewloss'\n",
    "if not os.path.exists(ckpDir):\n",
    "    os.makedirs(ckpDir)\n",
    "timeForSave = datetime.datetime.now().strftime('%Y_%m_%d_%H:%M:%S')\n",
    "# 设置使用的训练设备\n",
    "device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "net = net.to(device)\n",
    "# 加载数据，设置优化器\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=1000,shuffle=True)\n",
    "train_un_loader = torch.utils.data.DataLoader(train_unknown, batch_size=100,shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=100,shuffle=True)\n",
    "optimizer = torch.optim.Adam(net.parameters(),\n",
    "        lr=0.0002)\n",
    "lr_schedule = torch.optim.lr_scheduler.StepLR(\\\n",
    "        optimizer, 400, gamma=0.5, last_epoch=-1)\n",
    "total_test_acc = 0\n",
    "total_test_correct = 0\n",
    "totaltest = 0\n",
    "vizx = 0\n",
    "epoch_num = 2000\n",
    "# 开始迭代\n",
    "for epoch in range(epoch_num):\n",
    "\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    curr_total_correct = 0\n",
    "    i = 0\n",
    "    net.train()\n",
    "    # Classify real examples into the correct K classes\n",
    "    total_traintnum = 0\n",
    "    for batch_kn in train_loader: # Get Batch\n",
    "        i+=1\n",
    "        images, labels = batch_kn\n",
    "        # 数据和标签转为所需数据类型\n",
    "        images = images.to(torch.float32)\n",
    "        labels = labels.long()\n",
    "        preds = net(images.to(device)) # Pass Batch\n",
    "        trainloss = F.cross_entropy(preds.to(device), labels.to(device)) # Calculate Loss\n",
    "        known_augmented_logits = F.pad(preds, (0,1))\n",
    "        known_log_soft_open = F.log_softmax(known_augmented_logits.to(device), dim=1)[:, -1]\n",
    "        # errknown =  known_log_soft_open.mean()\n",
    "        # trainloss = trainloss + errknown\n",
    "        optimizer.zero_grad()\n",
    "        trainloss.backward() # Calculate Gradients\n",
    "        optimizer.step() # Update Weights\n",
    "        total_loss += trainloss.item()\n",
    "        # total_correct += get_num_correct(preds.to(device), labels.to(device))\n",
    "        curr_total_correct = (get_num_correct(preds.to(device), labels.to(device)))\n",
    "        total_correct += curr_total_correct\n",
    "        total_traintnum += labels.size(0)\n",
    "    total_train_acc = total_correct/total_traintnum\n",
    "    total_correct = 0\n",
    "    # Classify aux_dataset examples as open set\n",
    "    for batch_unkn in train_un_loader:\n",
    "        un_data,_ = batch_unkn\n",
    "        un_data = un_data.to(torch.float32)\n",
    "        classifier_logits = net(un_data.to(device))\n",
    "        augmented_logits = F.pad(classifier_logits, (0,1))\n",
    "        log_soft_open = F.log_softmax(augmented_logits.to(device), dim=1)[:, -1]\n",
    "        errOpenSet = -log_soft_open.mean()\n",
    "        optimizer.zero_grad()\n",
    "        errOpenSet.backward()\n",
    "        optimizer.step() # Update Weights\n",
    "\n",
    "    # 可视化\n",
    "    vizx+=1\n",
    "    viz.line([float(trainloss)],[vizx],\\\n",
    "        win='trainloss', update='append',opts=dict(title='trainloss'))   \n",
    "    viz2.line([float(optimizer.state_dict()['param_groups'][0]['lr'])],[vizx],\\\n",
    "        win='lr', update='append',opts=dict(title='lr'))\n",
    "    viz3.line([float(total_train_acc)],[vizx],\\\n",
    "        win='train_acc', update='append',opts=dict(title='train_acc'))\n",
    "    viz5.line([float(errOpenSet)],[vizx],\\\n",
    "        win='errOpenSet', update='append',opts=dict(title='erropen'))   \n",
    "    # 测试\n",
    "    net.eval()\n",
    "    total_testnum = 0\n",
    "    for testemgdatas, testemglabels in test_loader: # Get Batch\n",
    "        testemgdatas = testemgdatas.to(torch.float32)\n",
    "        testemglabels = testemglabels.long()\n",
    "        predstest = net(testemgdatas.to(device))\n",
    "        testloss = F.cross_entropy(predstest.to(device), testemglabels.to(device)) # Calculate Loss\n",
    "        curr_test_correct = (get_num_correct(predstest.to(device), testemglabels.to(device)))\n",
    "        total_testnum += testemglabels.size(0)\n",
    "        total_test_correct += curr_test_correct\n",
    "        # totaltest += testemglabels.size(0)\n",
    "    # total_test_acc = total_test_correct/(trainlabel.size)\n",
    "    total_test_acc = total_test_correct/total_testnum\n",
    "    \n",
    "    viz1.line([float(testloss)],[vizx],win='testloss', update='append',opts=dict(title='testloss'))\n",
    "    viz4.line([float(total_test_acc)],[vizx],\\\n",
    "        win='test_acc', update='append',opts=dict(title='test_acc'))\n",
    "    total_test_correct = 0\n",
    "\n",
    "    print(\n",
    "        \"epoch\", epoch, \n",
    "        \"total_train_acc:\", total_train_acc, \n",
    "        \"loss:\", total_loss,\n",
    "        \"test_loss:\",float(testloss),\n",
    "        \"test_acc:\",total_test_acc\n",
    "    )\n",
    "   # 更新学习率\n",
    "    lr_schedule.step()\n",
    "   # 定期保存\n",
    "    if epoch%50 == 0:\n",
    "        timeForSave = datetime.datetime.now().strftime('%Y_%m_%d_%H_%M_%S')\n",
    "        checkpointPath = ckpDir+'//gap5_c6op_ep_'+str(epoch)+'_'+timeForSave+'.pth'\n",
    "        state = {'model': net.state_dict(), 'optimizer': optimizer.state_dict(), 'epoch': epoch}\n",
    "        torch.save(state, checkpointPath)\n",
    "FinalPath = 'c6s'+timeForSave+'.pth'\n",
    "torch.save(net.state_dict(),FinalPath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def openset_fuxin(dataloader, netC):\n",
    "#     openset_scores = []\n",
    "#     for i, (images, labels) in enumerate(dataloader):\n",
    "#         images = Variable(images, volatile=True)\n",
    "#         logits = netC(images)\n",
    "#         augmented_logits = F.pad(logits, pad=(0,1))\n",
    "#         # The implicit K+1th class (the open set class) is computed\n",
    "#         #  by assuming an extra linear output with constant value 0\n",
    "#         preds = F.softmax(augmented_logits)\n",
    "#         #preds = augmented_logits\n",
    "#         prob_unknown = preds[:, -1]\n",
    "#         prob_known = preds[:, :-1].max(dim=1)[0]\n",
    "#         prob_open = prob_unknown - prob_known\n",
    "\n",
    "#         openset_scores.extend(prob_open.data.cpu().numpy())\n",
    "#     return np.array(openset_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 重新加载模型\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3)\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3)\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=32 * 4 * 34, out_features=128)\n",
    "        self.out = nn.Linear(in_features=128, out_features=6)\n",
    "        self.dr1 = nn.Dropout2d(0.2)\n",
    "\n",
    "    def forward(self, t):\n",
    "        # (1) input layer\n",
    "        t = t\n",
    "\n",
    "        # (2) hidden conv layer\n",
    "        t = self.conv1(t)\n",
    "        t = F.relu(t)\n",
    "        t = F.max_pool2d(t, kernel_size=2, stride=1)\n",
    "\n",
    "        # (3) hidden conv layer\n",
    "        t = self.conv2(t)\n",
    "        t = F.relu(t)\n",
    "        # t = self.dr1(t)\n",
    "        t = F.max_pool2d(t, kernel_size=2, stride=1)\n",
    "\n",
    "        # (4) hidden linear layer\n",
    "        t = t.reshape(-1, 32 * 4 * 34)\n",
    "        t = self.fc1(t)\n",
    "        t = F.relu(t)\n",
    "        t = self.dr1(t)\n",
    "\n",
    "        # (5) output layer\n",
    "        t = self.out(t)\n",
    "\n",
    "        return t\n",
    "        \n",
    "net_eval = Network()\n",
    "\n",
    "checkpoint_eval = torch.load('./ckp/c6addnewloss/gap5_c6op_ep_600_2021_12_16_15_47_32.pth')\n",
    "net_eval.load_state_dict(checkpoint_eval['model'])\n",
    "# net_eval.load_state_dict(checkpoint_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "[tensor([[[[0.1587, 0.0293, 0.1221, 0.0439, 0.0024, 0.0024, 0.3979, 0.4346,\n",
      "           0.2100, 0.1123],\n",
      "          [0.1587, 0.0293, 0.1270, 0.0439, 0.0049, 0.0049, 0.3979, 0.4590,\n",
      "           0.2710, 0.1196],\n",
      "          [0.1636, 0.0317, 0.1270, 0.0439, 0.0049, 0.0049, 0.3882, 0.4810,\n",
      "           0.3247, 0.1221],\n",
      "          [0.1563, 0.0342, 0.1172, 0.0391, 0.0024, 0.0024, 0.3931, 0.4639,\n",
      "           0.3564, 0.1123],\n",
      "          [0.1514, 0.0317, 0.1025, 0.0342, 0.0024, 0.0024, 0.4053, 0.4517,\n",
      "           0.3491, 0.1074],\n",
      "          [0.1392, 0.0220, 0.0806, 0.0244, 0.0024, 0.0024, 0.3882, 0.4297,\n",
      "           0.3223, 0.0977],\n",
      "          [0.1196, 0.0098, 0.0610, 0.0171, 0.0024, 0.0024, 0.3491, 0.3906,\n",
      "           0.3101, 0.0854],\n",
      "          [0.1025, 0.0024, 0.0439, 0.0073, 0.0024, 0.0024, 0.3003, 0.3516,\n",
      "           0.2905, 0.0757],\n",
      "          [0.1050, 0.0024, 0.0366, 0.0024, 0.0024, 0.0024, 0.2686, 0.3784,\n",
      "           0.2588, 0.0732],\n",
      "          [0.1123, 0.0024, 0.0342, 0.0024, 0.0024, 0.0024, 0.3003, 0.4199,\n",
      "           0.2441, 0.0830],\n",
      "          [0.1123, 0.0024, 0.0317, 0.0024, 0.0024, 0.0024, 0.3223, 0.4199,\n",
      "           0.2417, 0.0854],\n",
      "          [0.1074, 0.0024, 0.0317, 0.0024, 0.0024, 0.0024, 0.3174, 0.4004,\n",
      "           0.2271, 0.0806],\n",
      "          [0.1050, 0.0024, 0.0293, 0.0024, 0.0024, 0.0024, 0.3125, 0.4272,\n",
      "           0.2173, 0.0830],\n",
      "          [0.1099, 0.0024, 0.0293, 0.0049, 0.0024, 0.0024, 0.3467, 0.4468,\n",
      "           0.2173, 0.0928],\n",
      "          [0.1050, 0.0024, 0.0415, 0.0073, 0.0024, 0.0024, 0.3467, 0.4297,\n",
      "           0.2417, 0.0879],\n",
      "          [0.1001, 0.0049, 0.0488, 0.0098, 0.0024, 0.0024, 0.3271, 0.4077,\n",
      "           0.2539, 0.0781],\n",
      "          [0.0928, 0.0024, 0.0464, 0.0073, 0.0024, 0.0024, 0.2979, 0.3857,\n",
      "           0.2368, 0.0635],\n",
      "          [0.0806, 0.0024, 0.0366, 0.0024, 0.0024, 0.0024, 0.2637, 0.3589,\n",
      "           0.2100, 0.0562],\n",
      "          [0.0708, 0.0024, 0.0586, 0.0024, 0.0024, 0.0024, 0.2271, 0.3442,\n",
      "           0.1831, 0.0537],\n",
      "          [0.0757, 0.0024, 0.0732, 0.0024, 0.0024, 0.0024, 0.2466, 0.3491,\n",
      "           0.1978, 0.0635],\n",
      "          [0.0757, 0.0024, 0.0732, 0.0024, 0.0024, 0.0024, 0.2637, 0.3345,\n",
      "           0.2319, 0.0684],\n",
      "          [0.0879, 0.0049, 0.0708, 0.0024, 0.0024, 0.0024, 0.2832, 0.3296,\n",
      "           0.2344, 0.0684],\n",
      "          [0.0977, 0.0024, 0.0635, 0.0024, 0.0024, 0.0024, 0.2759, 0.3271,\n",
      "           0.2148, 0.0635],\n",
      "          [0.0977, 0.0024, 0.0586, 0.0024, 0.0024, 0.0024, 0.2563, 0.3027,\n",
      "           0.1831, 0.0562],\n",
      "          [0.0977, 0.0024, 0.0537, 0.0049, 0.0024, 0.0024, 0.3076, 0.2905,\n",
      "           0.2295, 0.0513],\n",
      "          [0.0879, 0.0049, 0.0464, 0.0146, 0.0024, 0.0024, 0.3076, 0.2686,\n",
      "           0.2832, 0.0439],\n",
      "          [0.0781, 0.0098, 0.0439, 0.0171, 0.0024, 0.0024, 0.2905, 0.2417,\n",
      "           0.3125, 0.0366],\n",
      "          [0.0928, 0.0195, 0.0415, 0.0195, 0.0024, 0.0024, 0.2759, 0.2783,\n",
      "           0.3271, 0.0464],\n",
      "          [0.1099, 0.0244, 0.0391, 0.0195, 0.0024, 0.0024, 0.2563, 0.3052,\n",
      "           0.3101, 0.0586],\n",
      "          [0.1123, 0.0220, 0.0342, 0.0146, 0.0024, 0.0024, 0.2271, 0.3027,\n",
      "           0.2759, 0.0562],\n",
      "          [0.1123, 0.0146, 0.0342, 0.0098, 0.0024, 0.0024, 0.2051, 0.3052,\n",
      "           0.2368, 0.0537],\n",
      "          [0.1221, 0.0122, 0.0342, 0.0073, 0.0024, 0.0024, 0.2515, 0.3149,\n",
      "           0.2197, 0.0586],\n",
      "          [0.1245, 0.0146, 0.0366, 0.0049, 0.0024, 0.0024, 0.2637, 0.3198,\n",
      "           0.2417, 0.0537],\n",
      "          [0.1147, 0.0195, 0.0415, 0.0049, 0.0024, 0.0024, 0.2466, 0.3003,\n",
      "           0.2563, 0.0488],\n",
      "          [0.1001, 0.0146, 0.0464, 0.0049, 0.0024, 0.0024, 0.2246, 0.2734,\n",
      "           0.2466, 0.0415],\n",
      "          [0.0854, 0.0049, 0.0903, 0.0122, 0.0024, 0.0024, 0.2026, 0.2393,\n",
      "           0.2271, 0.0342],\n",
      "          [0.0977, 0.0073, 0.1123, 0.0220, 0.0024, 0.0049, 0.2051, 0.2515,\n",
      "           0.2075, 0.0317],\n",
      "          [0.1074, 0.0098, 0.1147, 0.0244, 0.0024, 0.0049, 0.2734, 0.2686,\n",
      "           0.1855, 0.0317],\n",
      "          [0.1074, 0.0049, 0.1025, 0.0195, 0.0024, 0.0024, 0.2881, 0.2832,\n",
      "           0.1563, 0.0269],\n",
      "          [0.1147, 0.0049, 0.0928, 0.0171, 0.0024, 0.0024, 0.2905, 0.3174,\n",
      "           0.1416, 0.0244]]]], dtype=torch.float64), tensor([7], dtype=torch.int16)]\n"
     ]
    }
   ],
   "source": [
    "# 自定义数据集类\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # 将图片转换为Tensor,归一化至[0,1]\n",
    "])\n",
    "\n",
    "class EMGDataset(Dataset):\n",
    " \n",
    "    def __init__(self, data, label):\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "        self.transforms = transform\n",
    " \n",
    "    def __getitem__(self, index):\n",
    "        emgData = self.data[index,:,:,:]\n",
    "        emgData = np.squeeze(emgData)#似乎不应该压缩了\n",
    "        emglabel = self.label[index]\n",
    "        emglabel = emglabel.astype(np.int16)\n",
    "        emgData = self.transforms(emgData)      \n",
    "        \n",
    "        return emgData,emglabel\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    " \n",
    " \n",
    "# if __name__ == '__main__':\n",
    "dataarray = np.load('../data/OpenSetDataSet.npy',allow_pickle=True)\n",
    "CNNdataset = dataarray.item()\n",
    "print(type(CNNdataset))\n",
    "opensetdata = CNNdataset['X_oo']\n",
    "opensetlabel = CNNdataset['Y_oo']\n",
    "# trainlabel = CNNdataset['Ytrain']\n",
    "# testdata = CNNdataset['Xtest']\n",
    "# testlabel = CNNdataset['Ytest']\n",
    "# # print(trainlabel[:,0])\n",
    "\n",
    "opensetlabel = opensetlabel[:,0]\n",
    "# testlabel = testlabel[:,0]\n",
    "# print(type(trainlabel))\n",
    "open_set = EMGDataset(opensetdata, opensetlabel)\n",
    "\n",
    "# train_loader = torch.utils.data.DataLoader(train_set, batch_size=1, shuffle=True, pin_memory=True,\n",
    "#                                             num_workers=3)\n",
    "openset_loader = torch.utils.data.DataLoader(open_set, batch_size=1,shuffle=True)\n",
    "sample = next(iter(openset_loader))\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n",
      "Setting up a new session...\n",
      "C:\\Users\\cwdbo\\AppData\\Local\\Temp/ipykernel_9732/2954200349.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  preds = F.softmax(augmented_logits)\n",
      "C:\\Users\\cwdbo\\AppData\\Local\\Temp/ipykernel_9732/2954200349.py:31: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  preds2 = F.softmax(augmented_logits2)\n"
     ]
    }
   ],
   "source": [
    "# 效果评估\n",
    "openset_loader = torch.utils.data.DataLoader(open_set, batch_size=1,shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=1,shuffle=True)\n",
    "openset_scores = []\n",
    "openset_scores2 = []\n",
    "# 可视化\n",
    "viz6 = Visdom()\n",
    "viz7 = Visdom()\n",
    "vizx = 0\n",
    "openscoresave = []\n",
    "for opensetdata, opensetlabel in openset_loader: # Get Batch\n",
    "        opensetdata = opensetdata.to(torch.float32)\n",
    "        opensetlabel = opensetlabel.long()\n",
    "        logits = net_eval(opensetdata)\n",
    "        augmented_logits = F.pad(logits, pad=(0,1))\n",
    "        preds = F.softmax(augmented_logits)\n",
    "        prob_unknown = preds[:, -1]\n",
    "        prob_known = preds[:, :-1].max(dim=1)[0]\n",
    "        prob_open = prob_unknown - prob_known\n",
    "        openset_scores.extend(prob_open.data.cpu().numpy())\n",
    "        prob_openforvis = prob_open.float()\n",
    "        viz6.line([float(prob_openforvis)],[vizx],\\\n",
    "                win='openscore', update='append',opts=dict(title='openscore for unknown'))\n",
    "        vizx += 1\n",
    "\n",
    "vizx = 0\n",
    "for testemgdatas, testemglabels in test_loader: # Get Batch\n",
    "        testemgdatas = testemgdatas.to(torch.float32)\n",
    "        testemglabels = testemglabels.long()\n",
    "        logits2 = net_eval(testemgdatas)\n",
    "        augmented_logits2 = F.pad(logits2, pad=(0,1))\n",
    "        preds2 = F.softmax(augmented_logits2)\n",
    "        prob_unknown2 = preds2[:, -1]\n",
    "        prob_known2 = preds2[:, :-1].max(dim=1)[0]\n",
    "        prob_open2 = prob_unknown2 - prob_known2\n",
    "        openset_scores2.extend(prob_open2.data.cpu().numpy())\n",
    "        prob_openforvis2 = prob_open2.float()\n",
    "        viz7.line([float(prob_openforvis2)],[vizx],\\\n",
    "                win='openscore2', update='append',opts=dict(title='openscore for known'))\n",
    "        vizx += 1\n",
    "# for i, (images, labels) in enumerate(dataloader):\n",
    "#     images = Variable(images, volatile=True)\n",
    "#     logits = netC(images)\n",
    "#     augmented_logits = F.pad(logits, pad=(0,1))\n",
    "#     # The implicit K+1th class (the open set class) is computed\n",
    "#     #  by assuming an extra linear output with constant value 0\n",
    "#     preds = F.softmax(augmented_logits)\n",
    "#     #preds = augmented_logits\n",
    "#     prob_unknown = preds[:, -1]\n",
    "#     prob_known = preds[:, :-1].max(dim=1)[0]\n",
    "#     prob_open = prob_unknown - prob_known\n",
    "#     openset_scores.extend(prob_open.data.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.11 ('ml2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "97f3b3761d073a7e2fe7bca6198a06e63bb287dd409b57ec6cef0ba2122fdc4d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
