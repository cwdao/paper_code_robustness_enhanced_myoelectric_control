{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## opg_t01_base_0519_npd1\n",
    "基于 opg_aftertraining0519.ipynb\n",
    "本代码主要完成ninapro db1 其他subject 的验证工作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "import datetime\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io as scio\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn import metrics\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchsummary import summary\n",
    "from torchvision import transforms\n",
    "from torchviz import make_dot\n",
    "# import hiddenlayer as h\n",
    "from visdom import Visdom\n",
    "\n",
    "# from sklearn.preprocessing import label_binarize\n",
    "# from sklearn import preprocessing\n",
    "# from sklearn import tree \n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from utils.networks import *\n",
    "from utils.reuse import *\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预备操作\n",
    "设置检查点、visdom 日志文件存储等日志性文件存储位置；\n",
    "初始化 visdom,记得先在命令行输入 visdom 运行（python环境下）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022_06_18_22_58_26\n"
     ]
    }
   ],
   "source": [
    "# 以下是检查点路径\n",
    "# 请在当前环境下 CMD 输入python -m visdom.server 或 visdom 启动监视器\n",
    "# 数据处理现在已移至 emgDataprocess.ipynb\n",
    "# 现在model_Dir 作为所有文件的父目录，不再分设开导致文件难寻\n",
    "model_Dir = \"..//model//opg_testops_220611a//\"\n",
    "if not os.path.exists(model_Dir):\n",
    "    os.makedirs(model_Dir)\n",
    "# 这是正常定期检查点存储位置\n",
    "ckpDir = model_Dir + \"ckp//\"\n",
    "if not os.path.exists(ckpDir):\n",
    "    os.makedirs(ckpDir)\n",
    "# 这里是特挑最佳AUC的位置\n",
    "ckpDir_auc = ckpDir + \"auc//\"\n",
    "if not os.path.exists(ckpDir_auc):\n",
    "    os.makedirs(ckpDir_auc)\n",
    "# 这是 visdom 日志文件存储位置，留待备用\n",
    "vislogDir = model_Dir + \"vislog//\"\n",
    "if not os.path.exists(vislogDir):\n",
    "    os.makedirs(vislogDir)\n",
    "\n",
    "def get_current_time():\n",
    "    return datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "\n",
    "\n",
    "print(get_current_time())\n",
    "\n",
    "timeForSave = datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "visdom has started\n"
     ]
    }
   ],
   "source": [
    "# 以下是 visdom 监视窗口初始化，实现每次启用时重新加载，这里只写了 NameError 以防其他错误不能被发现\n",
    "class visdom_account:\n",
    "    def __init__(self):\n",
    "        self.port = 8097\n",
    "        self.server = \"http://localhost\"\n",
    "        self.base_url = \"/\"\n",
    "        self.username = \"admin\"\n",
    "        self.passward = \"1234\"\n",
    "        self.evns = \"train\"\n",
    "\n",
    "\n",
    "viz_acnt = visdom_account()\n",
    "\n",
    "\n",
    "def viz_init():\n",
    "    try:\n",
    "        viz\n",
    "    except NameError:\n",
    "        viz = Visdom(\n",
    "            env=viz_acnt.evns, log_to_filename=vislogDir + \"vislog_\" + timeForSave\n",
    "        )\n",
    "        print(\"visdom has started\")\n",
    "    else:\n",
    "        viz.close()\n",
    "        del viz\n",
    "        print(\"last visdom session closed\")\n",
    "        viz = Visdom(\n",
    "            env=viz_acnt.evns, log_to_filename=vislogDir + \"vislog_\" + timeForSave\n",
    "        )\n",
    "        print(\"visdom has restarted\")\n",
    "    return viz\n",
    "\n",
    "\n",
    "viz = viz_init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义CNN神经网络结构\n",
    "CNN 训练部分，为了获得一个已知类分类器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [1, 32, 20, 10]             320\n",
      "            Conv2d-2            [1, 32, 20, 10]           9,248\n",
      "            Linear-3                   [1, 128]         819,328\n",
      "         Dropout2d-4                   [1, 128]               0\n",
      "            Linear-5                    [1, 10]           1,290\n",
      "================================================================\n",
      "Total params: 830,186\n",
      "Trainable params: 830,186\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.10\n",
      "Params size (MB): 3.17\n",
      "Estimated Total Size (MB): 3.27\n",
      "----------------------------------------------------------------\n",
      "Outputshape: torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "# 自定义神经网络,CNN\n",
    "def get_num_correct(preds, labels):\n",
    "    return preds.argmax(dim=1).eq(labels).sum().item()\n",
    "\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=32, out_channels=32, kernel_size=3, padding=1\n",
    "        )\n",
    "        self.conv3 = nn.Conv2d(\n",
    "            in_channels=32, out_channels=32, kernel_size=3, padding=0\n",
    "        )\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=32 * 10 * 20, out_features=128)\n",
    "        self.out = nn.Linear(in_features=128, out_features=10)\n",
    "        self.dr1 = nn.Dropout2d(0.2)\n",
    "\n",
    "    def forward(self, t):\n",
    "        # (1) input layer\n",
    "        t = t\n",
    "\n",
    "        # (2) hidden conv layer\n",
    "        t = self.conv1(t)\n",
    "        t = F.relu(t)\n",
    "        # t = F.max_pool2d(t, kernel_size=2, stride=1)\n",
    "\n",
    "        # (3) hidden conv layer\n",
    "        t = self.conv2(t)\n",
    "        t = F.relu(t)\n",
    "        # t = self.dr1(t)\n",
    "        # t = F.max_pool2d(t, kernel_size=2, stride=1)\n",
    "\n",
    "        # (4) hidden linear layer\n",
    "        t = t.reshape(-1, 32 * 10 * 20)\n",
    "        t = self.fc1(t)\n",
    "        t = F.relu(t)\n",
    "        t = self.dr1(t)\n",
    "\n",
    "        # (5) output layer\n",
    "        t = self.out(t)\n",
    "\n",
    "        return t\n",
    "\n",
    "\n",
    "net = Network()\n",
    "# 打印网络，检查输入输出 shape是否正确\n",
    "# print(net)\n",
    "summary(net, (1, 20, 10), batch_size=1, device=\"cpu\")\n",
    "# 可视化结构，torchviz\n",
    "sampleInput = torch.randn(1, 1, 20, 10).requires_grad_(True)\n",
    "sampleOutput = net(sampleInput)\n",
    "print(\"Outputshape:\", sampleOutput.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据集加载、构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "<iterator object at 0x000001F8E08903A0>\n"
     ]
    }
   ],
   "source": [
    "# CNN训练数据加载\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # 将图片转换为Tensor,归一化至[0,1]\n",
    "])\n",
    "\n",
    "class EMGDataset(Dataset):\n",
    " \n",
    "    def __init__(self, data, label):\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "        self.transforms = transform\n",
    " \n",
    "    def __getitem__(self, index):\n",
    "        emgData = self.data[index,:,:,:]\n",
    "        emgData = np.squeeze(emgData)\n",
    "        emglabel = self.label[index]\n",
    "        emglabel = emglabel.astype(np.int16)\n",
    "        emgData = self.transforms(emgData)\n",
    "        # 一维数据用下面的这个就行\n",
    "        # emgData = torch.Tensor(emgData)     \n",
    "        return emgData,emglabel\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    " \n",
    " \n",
    "# if __name__ == '__main__':\n",
    "dataarray = np.load('../../data/nina_db1/Fopgs2_multiset_20220611T161411.npy',allow_pickle=True)\n",
    "CNNdataset = dataarray.item()\n",
    "print(type(CNNdataset))\n",
    "traindata = CNNdataset['ktr_X']\n",
    "trainlabel = CNNdataset['ktr_Y']\n",
    "testdata = CNNdataset['kte_X']\n",
    "testlabel = CNNdataset['kte_Y']\n",
    "# 先用5类的\n",
    "valdata = CNNdataset['val_X_5c']\n",
    "vallabel = CNNdataset['val_Y_5c']\n",
    "\n",
    "trainlabel = trainlabel[:,0]\n",
    "testlabel = testlabel[:,0]\n",
    "vallabel = vallabel[:,0]\n",
    "# trainunknownc_label = trainunknownc_label[:,0]\n",
    "# print(type(trainlabel))\n",
    "train_set = EMGDataset(traindata, trainlabel)\n",
    "test_set = EMGDataset(testdata, testlabel)\n",
    "val_set = EMGDataset(valdata, vallabel)\n",
    "# train_unknown = EMGDataset(trainunknown_data,trainunknownc_label)\n",
    "# train_loader = torch.utils.data.DataLoader(train_set, batch_size=1, shuffle=True, pin_memory=True,\n",
    "#                                             num_workers=3)\n",
    "\n",
    "# 试运行数据集，构建迭代器\n",
    "sample = iter(test_set)\n",
    "print(sample)\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=2, shuffle=False)\n",
    "batch1 = iter(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: torch.Size([2, 1, 20, 10]) tensor([[[[0.2930, 0.3223, 0.0562, 0.0903, 0.0024, 0.0024, 0.0024, 0.1147,\n",
      "           0.0464, 0.0049],\n",
      "          [0.3101, 0.3027, 0.0854, 0.0757, 0.0024, 0.0024, 0.0024, 0.1270,\n",
      "           0.0830, 0.0024],\n",
      "          [0.3540, 0.3296, 0.1270, 0.0684, 0.0024, 0.0024, 0.0024, 0.1367,\n",
      "           0.1294, 0.0024],\n",
      "          [0.3760, 0.3809, 0.1416, 0.0659, 0.0024, 0.0024, 0.0024, 0.1416,\n",
      "           0.1660, 0.0024],\n",
      "          [0.3882, 0.4443, 0.1489, 0.0610, 0.0024, 0.0024, 0.0024, 0.1416,\n",
      "           0.1807, 0.0024],\n",
      "          [0.3882, 0.4932, 0.2368, 0.0830, 0.0024, 0.0024, 0.0024, 0.1758,\n",
      "           0.1953, 0.0049],\n",
      "          [0.3809, 0.5029, 0.4102, 0.1050, 0.0049, 0.0049, 0.0073, 0.2246,\n",
      "           0.2173, 0.0024],\n",
      "          [0.3662, 0.4761, 0.4834, 0.1099, 0.0024, 0.0024, 0.0098, 0.2612,\n",
      "           0.2222, 0.0049],\n",
      "          [0.3418, 0.4297, 0.4810, 0.1343, 0.0049, 0.0024, 0.0073, 0.2661,\n",
      "           0.2148, 0.0073],\n",
      "          [0.3271, 0.4272, 0.4517, 0.1563, 0.0122, 0.0073, 0.0098, 0.2515,\n",
      "           0.2002, 0.0146],\n",
      "          [0.3491, 0.4272, 0.4053, 0.1587, 0.0122, 0.0073, 0.0073, 0.2368,\n",
      "           0.1782, 0.0244],\n",
      "          [0.3784, 0.4175, 0.3589, 0.1563, 0.0122, 0.0049, 0.0024, 0.2563,\n",
      "           0.1611, 0.0342],\n",
      "          [0.4272, 0.4688, 0.3198, 0.1440, 0.0098, 0.0049, 0.0049, 0.3271,\n",
      "           0.1416, 0.0366],\n",
      "          [0.4321, 0.4785, 0.2832, 0.1367, 0.0073, 0.0024, 0.0049, 0.4028,\n",
      "           0.1221, 0.0366],\n",
      "          [0.4102, 0.4517, 0.2466, 0.1318, 0.0049, 0.0024, 0.0073, 0.4126,\n",
      "           0.1074, 0.0342],\n",
      "          [0.3809, 0.4639, 0.2661, 0.1514, 0.0024, 0.0024, 0.0098, 0.4028,\n",
      "           0.1343, 0.0342],\n",
      "          [0.3784, 0.5273, 0.2881, 0.1660, 0.0024, 0.0049, 0.0073, 0.4077,\n",
      "           0.1636, 0.0366],\n",
      "          [0.4053, 0.5273, 0.3003, 0.1709, 0.0024, 0.0049, 0.0073, 0.4028,\n",
      "           0.1709, 0.0366],\n",
      "          [0.4150, 0.4932, 0.3467, 0.1733, 0.0073, 0.0049, 0.0049, 0.3857,\n",
      "           0.1807, 0.0342],\n",
      "          [0.4150, 0.4468, 0.3687, 0.1880, 0.0122, 0.0049, 0.0024, 0.3564,\n",
      "           0.1758, 0.0342]]],\n",
      "\n",
      "\n",
      "        [[[0.3491, 0.4272, 0.4053, 0.1587, 0.0122, 0.0073, 0.0073, 0.2368,\n",
      "           0.1782, 0.0244],\n",
      "          [0.3784, 0.4175, 0.3589, 0.1563, 0.0122, 0.0049, 0.0024, 0.2563,\n",
      "           0.1611, 0.0342],\n",
      "          [0.4272, 0.4688, 0.3198, 0.1440, 0.0098, 0.0049, 0.0049, 0.3271,\n",
      "           0.1416, 0.0366],\n",
      "          [0.4321, 0.4785, 0.2832, 0.1367, 0.0073, 0.0024, 0.0049, 0.4028,\n",
      "           0.1221, 0.0366],\n",
      "          [0.4102, 0.4517, 0.2466, 0.1318, 0.0049, 0.0024, 0.0073, 0.4126,\n",
      "           0.1074, 0.0342],\n",
      "          [0.3809, 0.4639, 0.2661, 0.1514, 0.0024, 0.0024, 0.0098, 0.4028,\n",
      "           0.1343, 0.0342],\n",
      "          [0.3784, 0.5273, 0.2881, 0.1660, 0.0024, 0.0049, 0.0073, 0.4077,\n",
      "           0.1636, 0.0366],\n",
      "          [0.4053, 0.5273, 0.3003, 0.1709, 0.0024, 0.0049, 0.0073, 0.4028,\n",
      "           0.1709, 0.0366],\n",
      "          [0.4150, 0.4932, 0.3467, 0.1733, 0.0073, 0.0049, 0.0049, 0.3857,\n",
      "           0.1807, 0.0342],\n",
      "          [0.4150, 0.4468, 0.3687, 0.1880, 0.0122, 0.0049, 0.0024, 0.3564,\n",
      "           0.1758, 0.0342],\n",
      "          [0.4053, 0.4541, 0.3931, 0.1929, 0.0146, 0.0073, 0.0049, 0.3589,\n",
      "           0.1807, 0.0464],\n",
      "          [0.3833, 0.4688, 0.4492, 0.2100, 0.0195, 0.0073, 0.0098, 0.3906,\n",
      "           0.2002, 0.0610],\n",
      "          [0.3760, 0.4443, 0.4517, 0.2075, 0.0195, 0.0073, 0.0098, 0.4175,\n",
      "           0.2466, 0.0659],\n",
      "          [0.3809, 0.4053, 0.4248, 0.2075, 0.0171, 0.0049, 0.0098, 0.4321,\n",
      "           0.2905, 0.0659],\n",
      "          [0.3833, 0.3833, 0.3931, 0.2026, 0.0122, 0.0049, 0.0098, 0.4419,\n",
      "           0.2979, 0.0757],\n",
      "          [0.3662, 0.3442, 0.3589, 0.2051, 0.0073, 0.0024, 0.0073, 0.4224,\n",
      "           0.2856, 0.0732],\n",
      "          [0.3540, 0.3027, 0.3418, 0.2124, 0.0073, 0.0024, 0.0024, 0.3931,\n",
      "           0.3027, 0.0708],\n",
      "          [0.3613, 0.3101, 0.3564, 0.2441, 0.0146, 0.0049, 0.0049, 0.4028,\n",
      "           0.3101, 0.0635],\n",
      "          [0.3979, 0.4053, 0.3760, 0.2734, 0.0220, 0.0098, 0.0122, 0.3931,\n",
      "           0.2954, 0.0610],\n",
      "          [0.4077, 0.4590, 0.4248, 0.2881, 0.0244, 0.0146, 0.0146, 0.3760,\n",
      "           0.2856, 0.0586]]]], dtype=torch.float64) \n",
      " y: torch.Size([2]) tensor([0, 0], dtype=torch.int16)\n"
     ]
    }
   ],
   "source": [
    "batch2 = next(batch1)\n",
    "d_x,d_y = batch2\n",
    "print(\n",
    "    \"x:\",\n",
    "    d_x.shape,\n",
    "    d_x,\n",
    "    \"\\n\",\n",
    "    \"y:\",\n",
    "    d_y.shape,\n",
    "    d_y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n",
      "Setting up a new session...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "visdom has started\n",
      "epoch 1 total_train_acc: 0.25286380992787444 loss: 40.65820324420929 test_loss: 1.77101469039917 test_acc: 0.4227373068432671\n",
      "epoch 2 total_train_acc: 0.48939329656342806 loss: 32.19743287563324 test_loss: 1.480151653289795 test_acc: 0.5750551876379691\n",
      "epoch 3 total_train_acc: 0.5903691132795927 loss: 24.275026202201843 test_loss: 0.9560816884040833 test_acc: 0.6512141280353201\n",
      "epoch 4 total_train_acc: 0.6574034789987272 loss: 19.52819150686264 test_loss: 0.9936180114746094 test_acc: 0.6981236203090507\n",
      "epoch 5 total_train_acc: 0.6943148069579974 loss: 16.80268633365631 test_loss: 0.5179039835929871 test_acc: 0.7240618101545254\n",
      "epoch 6 total_train_acc: 0.7223165040305473 loss: 15.133083045482635 test_loss: 0.46932855248451233 test_acc: 0.7334437086092715\n",
      "epoch 7 total_train_acc: 0.7422571064913025 loss: 14.02824604511261 test_loss: 0.33878859877586365 test_acc: 0.7522075055187638\n",
      "epoch 8 total_train_acc: 0.7448027153160798 loss: 13.189512014389038 test_loss: 0.5464238524436951 test_acc: 0.7676600441501104\n",
      "epoch 9 total_train_acc: 0.7630462452269835 loss: 12.489289939403534 test_loss: 0.6063508987426758 test_acc: 0.7731788079470199\n",
      "epoch 10 total_train_acc: 0.7783198981756471 loss: 11.813001215457916 test_loss: 0.5224540829658508 test_acc: 0.7820088300220751\n",
      "epoch 11 total_train_acc: 0.7781077641069156 loss: 11.521953016519547 test_loss: 0.46664151549339294 test_acc: 0.7886313465783664\n",
      "epoch 12 total_train_acc: 0.7931692829868476 loss: 11.009263396263123 test_loss: 0.7584853172302246 test_acc: 0.7886313465783664\n",
      "epoch 13 total_train_acc: 0.7982605006364022 loss: 10.648701906204224 test_loss: 0.6722199320793152 test_acc: 0.7974613686534217\n",
      "epoch 14 total_train_acc: 0.8001697072549852 loss: 10.425140082836151 test_loss: 0.7100024819374084 test_acc: 0.793046357615894\n",
      "epoch 15 total_train_acc: 0.8031395842172253 loss: 10.19945678114891 test_loss: 0.38619229197502136 test_acc: 0.7991169977924945\n",
      "epoch 16 total_train_acc: 0.8086550700042426 loss: 10.03262808918953 test_loss: 0.7706419825553894 test_acc: 0.8096026490066225\n",
      "epoch 17 total_train_acc: 0.8169282986847688 loss: 9.797703921794891 test_loss: 0.48409315943717957 test_acc: 0.8145695364238411\n",
      "epoch 18 total_train_acc: 0.8186253712346203 loss: 9.633453279733658 test_loss: 0.46644172072410583 test_acc: 0.815121412803532\n",
      "epoch 19 total_train_acc: 0.8237165888841748 loss: 9.44859716296196 test_loss: 0.5292177796363831 test_acc: 0.8206401766004415\n",
      "epoch 20 total_train_acc: 0.8292320746711922 loss: 9.074728399515152 test_loss: 0.5837648510932922 test_acc: 0.8300220750551877\n",
      "epoch 21 total_train_acc: 0.8296563428086551 loss: 9.013340324163437 test_loss: 0.7472818493843079 test_acc: 0.8211920529801324\n",
      "epoch 22 total_train_acc: 0.8381417055579126 loss: 8.744281888008118 test_loss: 0.12155937403440475 test_acc: 0.8283664459161147\n",
      "epoch 23 total_train_acc: 0.8449299957573186 loss: 8.547044783830643 test_loss: 0.2630540132522583 test_acc: 0.8416114790286976\n",
      "epoch 24 total_train_acc: 0.8400509121764955 loss: 8.457868605852127 test_loss: 0.24193473160266876 test_acc: 0.8294701986754967\n",
      "epoch 25 total_train_acc: 0.8466270683071702 loss: 8.413409382104874 test_loss: 0.16483508050441742 test_acc: 0.8410596026490066\n",
      "epoch 26 total_train_acc: 0.8440814594823929 loss: 8.092853784561157 test_loss: 0.4275899827480316 test_acc: 0.8311258278145696\n",
      "epoch 27 total_train_acc: 0.8525668222316504 loss: 8.117306798696518 test_loss: 0.22581441700458527 test_acc: 0.8454746136865342\n",
      "epoch 28 total_train_acc: 0.8527789563003818 loss: 7.964159429073334 test_loss: 0.19177000224590302 test_acc: 0.8454746136865342\n",
      "epoch 29 total_train_acc: 0.855748833262622 loss: 7.821879714727402 test_loss: 0.44401052594184875 test_acc: 0.8504415011037527\n",
      "epoch 30 total_train_acc: 0.8589308442935936 loss: 7.547274321317673 test_loss: 0.3458296060562134 test_acc: 0.8471302428256071\n",
      "epoch 31 total_train_acc: 0.861052184980908 loss: 7.474699378013611 test_loss: 0.4795362651348114 test_acc: 0.8559602649006622\n",
      "epoch 32 total_train_acc: 0.8661434026304624 loss: 7.46495857834816 test_loss: 0.35527491569519043 test_acc: 0.8509933774834437\n",
      "epoch 33 total_train_acc: 0.8737802291047942 loss: 7.155553728342056 test_loss: 0.32708248496055603 test_acc: 0.8526490066225165\n",
      "epoch 34 total_train_acc: 0.8642341960118796 loss: 7.300444722175598 test_loss: 0.35936975479125977 test_acc: 0.8487858719646799\n",
      "epoch 35 total_train_acc: 0.8633856597369538 loss: 7.174942314624786 test_loss: 0.37859511375427246 test_acc: 0.8647902869757175\n",
      "epoch 36 total_train_acc: 0.8686890114552397 loss: 7.148462295532227 test_loss: 0.16341499984264374 test_acc: 0.8570640176600441\n",
      "epoch 37 total_train_acc: 0.8680526092490454 loss: 7.103453695774078 test_loss: 0.2816648483276367 test_acc: 0.858167770419426\n",
      "epoch 38 total_train_acc: 0.8669919389053882 loss: 6.937714338302612 test_loss: 0.35670578479766846 test_acc: 0.8647902869757175\n",
      "epoch 39 total_train_acc: 0.8725074246924056 loss: 6.781306147575378 test_loss: 0.27599817514419556 test_acc: 0.8647902869757175\n",
      "epoch 40 total_train_acc: 0.8718710224862113 loss: 6.852437645196915 test_loss: 0.5283868908882141 test_acc: 0.8664459161147903\n",
      "epoch 41 total_train_acc: 0.8737802291047942 loss: 6.674419611692429 test_loss: 0.4184763431549072 test_acc: 0.8642384105960265\n",
      "epoch 42 total_train_acc: 0.8809927874416631 loss: 6.462347224354744 test_loss: 0.4769168794155121 test_acc: 0.8730684326710817\n",
      "epoch 43 total_train_acc: 0.8812049215103945 loss: 6.470955669879913 test_loss: 0.1703275591135025 test_acc: 0.8708609271523179\n",
      "epoch 44 total_train_acc: 0.8775986423419602 loss: 6.422371178865433 test_loss: 0.06484953314065933 test_acc: 0.8719646799116998\n",
      "epoch 45 total_train_acc: 0.8831141281289775 loss: 6.360255181789398 test_loss: 0.3307488262653351 test_acc: 0.8725165562913907\n",
      "epoch 46 total_train_acc: 0.879507848960543 loss: 6.347144857048988 test_loss: 0.44966205954551697 test_acc: 0.8686534216335541\n",
      "epoch 47 total_train_acc: 0.8831141281289775 loss: 6.259577691555023 test_loss: 0.1160915270447731 test_acc: 0.8708609271523179\n",
      "epoch 48 total_train_acc: 0.8812049215103945 loss: 6.198182597756386 test_loss: 0.17608018219470978 test_acc: 0.8725165562913907\n",
      "epoch 49 total_train_acc: 0.8845990666100976 loss: 6.00632606446743 test_loss: 0.716002881526947 test_acc: 0.8664459161147903\n",
      "epoch 50 total_train_acc: 0.8892660161221893 loss: 5.871782034635544 test_loss: 0.6501800417900085 test_acc: 0.8725165562913907\n",
      "epoch 51 total_train_acc: 0.8877810776410692 loss: 5.990639373660088 test_loss: 0.6494848132133484 test_acc: 0.8763796909492274\n",
      "epoch 52 total_train_acc: 0.8871446754348749 loss: 6.104833766818047 test_loss: 0.6876930594444275 test_acc: 0.8714128035320088\n",
      "epoch 53 total_train_acc: 0.8837505303351718 loss: 5.947445958852768 test_loss: 0.10646834224462509 test_acc: 0.8747240618101545\n",
      "epoch 54 total_train_acc: 0.8903266864658465 loss: 5.883289203047752 test_loss: 0.28654220700263977 test_acc: 0.8769315673289183\n",
      "epoch 55 total_train_acc: 0.8875689435723377 loss: 5.887168318033218 test_loss: 0.59002685546875 test_acc: 0.869757174392936\n",
      "epoch 56 total_train_acc: 0.8896902842596521 loss: 5.7821004539728165 test_loss: 0.20510123670101166 test_acc: 0.8692052980132451\n",
      "epoch 57 total_train_acc: 0.8935086974968179 loss: 5.608120262622833 test_loss: 0.3270665109157562 test_acc: 0.8824503311258278\n",
      "epoch 58 total_train_acc: 0.8988120492151039 loss: 5.510358422994614 test_loss: 0.2569618821144104 test_acc: 0.8835540838852097\n",
      "epoch 59 total_train_acc: 0.8924480271531608 loss: 5.5390374809503555 test_loss: 0.2547720670700073 test_acc: 0.8813465783664459\n",
      "epoch 60 total_train_acc: 0.894993635977938 loss: 5.483210057020187 test_loss: 0.1601295918226242 test_acc: 0.8813465783664459\n",
      "epoch 61 total_train_acc: 0.8985999151463725 loss: 5.534411802887917 test_loss: 0.18585868179798126 test_acc: 0.8752759381898455\n",
      "epoch 62 total_train_acc: 0.8983877810776411 loss: 5.398642465472221 test_loss: 0.17723415791988373 test_acc: 0.8813465783664459\n",
      "epoch 63 total_train_acc: 0.8958421722528638 loss: 5.4232122749090195 test_loss: 0.38469198346138 test_acc: 0.8841059602649006\n",
      "epoch 64 total_train_acc: 0.9019940602460755 loss: 5.469757080078125 test_loss: 0.39948567748069763 test_acc: 0.8885209713024282\n",
      "epoch 65 total_train_acc: 0.8973271107339839 loss: 5.278687804937363 test_loss: 0.43344974517822266 test_acc: 0.8879690949227373\n",
      "epoch 66 total_train_acc: 0.9024183283835384 loss: 5.162440970540047 test_loss: 0.3648204505443573 test_acc: 0.8824503311258278\n",
      "epoch 67 total_train_acc: 0.9005091217649555 loss: 5.299196362495422 test_loss: 0.40983060002326965 test_acc: 0.8885209713024282\n",
      "epoch 68 total_train_acc: 0.9045396690708528 loss: 5.014233082532883 test_loss: 0.08839548379182816 test_acc: 0.8818984547461368\n",
      "epoch 69 total_train_acc: 0.8977513788714467 loss: 5.210703745484352 test_loss: 0.1899992972612381 test_acc: 0.8796909492273731\n",
      "epoch 70 total_train_acc: 0.9043275350021214 loss: 5.087662443518639 test_loss: 0.06941816955804825 test_acc: 0.8852097130242825\n",
      "epoch 71 total_train_acc: 0.9015697921086127 loss: 5.092959031462669 test_loss: 0.5010515451431274 test_acc: 0.8879690949227373\n",
      "epoch 72 total_train_acc: 0.9011455239711498 loss: 5.188257783651352 test_loss: 0.344409704208374 test_acc: 0.8879690949227373\n",
      "epoch 73 total_train_acc: 0.90560033941451 loss: 5.018562138080597 test_loss: 0.37364378571510315 test_acc: 0.891832229580574\n",
      "epoch 74 total_train_acc: 0.9062367416207043 loss: 4.880227670073509 test_loss: 0.44899287819862366 test_acc: 0.8835540838852097\n",
      "epoch 75 total_train_acc: 0.9077216801018243 loss: 4.893403321504593 test_loss: 0.43316707015037537 test_acc: 0.8885209713024282\n",
      "epoch 76 total_train_acc: 0.9068731438268987 loss: 4.959416434168816 test_loss: 0.21644629538059235 test_acc: 0.8912803532008831\n",
      "epoch 77 total_train_acc: 0.9058124734832415 loss: 4.822550073266029 test_loss: 0.1727622002363205 test_acc: 0.8956953642384106\n",
      "epoch 78 total_train_acc: 0.9096308867204073 loss: 4.732175186276436 test_loss: 0.1672237515449524 test_acc: 0.8956953642384106\n",
      "epoch 79 total_train_acc: 0.9013576580398812 loss: 4.778825417160988 test_loss: 0.46465519070625305 test_acc: 0.8967991169977925\n",
      "epoch 80 total_train_acc: 0.9070852778956301 loss: 4.8271766901016235 test_loss: 0.5981175303459167 test_acc: 0.8951434878587197\n",
      "epoch 81 total_train_acc: 0.9106915570640645 loss: 4.798254802823067 test_loss: 0.6350056529045105 test_acc: 0.8896247240618101\n",
      "epoch 82 total_train_acc: 0.9132371658888417 loss: 4.613914757966995 test_loss: 0.16608618199825287 test_acc: 0.8879690949227373\n",
      "epoch 83 total_train_acc: 0.9100551548578701 loss: 4.575888961553574 test_loss: 0.3275909721851349 test_acc: 0.8945916114790287\n",
      "epoch 84 total_train_acc: 0.9119643614764531 loss: 4.469536408782005 test_loss: 0.12275862693786621 test_acc: 0.8923841059602649\n",
      "epoch 85 total_train_acc: 0.9083580823080186 loss: 4.6225795447826385 test_loss: 0.5164766907691956 test_acc: 0.8951434878587197\n",
      "epoch 86 total_train_acc: 0.9166313109885448 loss: 4.470454305410385 test_loss: 0.44599902629852295 test_acc: 0.8951434878587197\n",
      "epoch 87 total_train_acc: 0.9138735680950361 loss: 4.564507901668549 test_loss: 0.8343076705932617 test_acc: 0.9001103752759382\n",
      "epoch 88 total_train_acc: 0.9142978362324989 loss: 4.459430277347565 test_loss: 0.47265270352363586 test_acc: 0.8934878587196468\n",
      "epoch 89 total_train_acc: 0.917691981332202 loss: 4.299645811319351 test_loss: 0.2272806316614151 test_acc: 0.8945916114790287\n",
      "epoch 90 total_train_acc: 0.912388629613916 loss: 4.414196535944939 test_loss: 0.12142913788557053 test_acc: 0.8984547461368654\n",
      "epoch 91 total_train_acc: 0.9181162494696649 loss: 4.230092212557793 test_loss: 0.24546194076538086 test_acc: 0.9006622516556292\n",
      "epoch 92 total_train_acc: 0.9202375901569793 loss: 4.123424276709557 test_loss: 0.15043577551841736 test_acc: 0.8956953642384106\n",
      "epoch 93 total_train_acc: 0.9170555791260077 loss: 4.287678569555283 test_loss: 0.056045759469270706 test_acc: 0.9012141280353201\n",
      "epoch 94 total_train_acc: 0.9208739923631736 loss: 4.267186641693115 test_loss: 0.18519185483455658 test_acc: 0.8995584988962473\n",
      "epoch 95 total_train_acc: 0.9191769198133221 loss: 4.247377797961235 test_loss: 0.09783289581537247 test_acc: 0.8874172185430463\n",
      "epoch 96 total_train_acc: 0.9253288078065337 loss: 4.0121685564517975 test_loss: 0.06531619280576706 test_acc: 0.8912803532008831\n",
      "epoch 97 total_train_acc: 0.9179041154009334 loss: 4.199402764439583 test_loss: 0.2552017867565155 test_acc: 0.9089403973509934\n",
      "epoch 98 total_train_acc: 0.9223589308442935 loss: 4.009915888309479 test_loss: 0.30310890078544617 test_acc: 0.8967991169977925\n",
      "epoch 99 total_train_acc: 0.9138735680950361 loss: 4.200713813304901 test_loss: 0.394014835357666 test_acc: 0.8979028697571744\n",
      "epoch 100 total_train_acc: 0.9174798472634705 loss: 4.0666233748197556 test_loss: 0.25125911831855774 test_acc: 0.8934878587196468\n",
      "epoch 101 total_train_acc: 0.9253288078065337 loss: 3.9434477984905243 test_loss: 0.16331914067268372 test_acc: 0.9034216335540839\n",
      "epoch 102 total_train_acc: 0.9274501484938481 loss: 3.96226903796196 test_loss: 0.2758868932723999 test_acc: 0.9039735099337748\n",
      "epoch 103 total_train_acc: 0.9274501484938481 loss: 3.664809674024582 test_loss: 0.1552620232105255 test_acc: 0.9028697571743929\n",
      "epoch 104 total_train_acc: 0.9291472210436996 loss: 3.761337637901306 test_loss: 0.21908940374851227 test_acc: 0.8995584988962473\n",
      "epoch 105 total_train_acc: 0.9287229529062367 loss: 3.7241664677858353 test_loss: 0.343886137008667 test_acc: 0.8995584988962473\n",
      "epoch 106 total_train_acc: 0.9325413661434027 loss: 3.6773401871323586 test_loss: 0.3877633512020111 test_acc: 0.9001103752759382\n",
      "epoch 107 total_train_acc: 0.9302078913873568 loss: 3.7271531224250793 test_loss: 0.15641574561595917 test_acc: 0.9061810154525386\n",
      "epoch 108 total_train_acc: 0.9238438693254136 loss: 3.701697513461113 test_loss: 0.33777275681495667 test_acc: 0.9017660044150111\n",
      "epoch 109 total_train_acc: 0.9293593551124311 loss: 3.617385044693947 test_loss: 0.03846825659275055 test_acc: 0.8984547461368654\n",
      "epoch 110 total_train_acc: 0.9282986847687739 loss: 3.6515777111053467 test_loss: 0.29753193259239197 test_acc: 0.9039735099337748\n",
      "epoch 111 total_train_acc: 0.9285108188375053 loss: 3.6824239045381546 test_loss: 0.16367213428020477 test_acc: 0.9061810154525386\n",
      "epoch 112 total_train_acc: 0.9295714891811625 loss: 3.668506905436516 test_loss: 0.2643025815486908 test_acc: 0.9001103752759382\n",
      "epoch 113 total_train_acc: 0.927874416631311 loss: 3.6351655274629593 test_loss: 0.3048170208930969 test_acc: 0.9039735099337748\n",
      "epoch 114 total_train_acc: 0.9280865507000424 loss: 3.6905410140752792 test_loss: 0.02418442629277706 test_acc: 0.9061810154525386\n",
      "epoch 115 total_train_acc: 0.9299957573186254 loss: 3.6681892722845078 test_loss: 0.12373194098472595 test_acc: 0.9056291390728477\n",
      "epoch 116 total_train_acc: 0.933177768349597 loss: 3.524928867816925 test_loss: 0.5164535641670227 test_acc: 0.9001103752759382\n",
      "epoch 117 total_train_acc: 0.9340263046245227 loss: 3.6380302757024765 test_loss: 0.33769723773002625 test_acc: 0.9034216335540839\n",
      "epoch 118 total_train_acc: 0.9270258803563852 loss: 3.6542575359344482 test_loss: 0.23168540000915527 test_acc: 0.9006622516556292\n",
      "epoch 119 total_train_acc: 0.9367840475180313 loss: 3.4660862535238266 test_loss: 0.07900706678628922 test_acc: 0.9050772626931567\n",
      "epoch 120 total_train_acc: 0.9308442935935511 loss: 3.5861116498708725 test_loss: 0.1489577442407608 test_acc: 0.8956953642384106\n",
      "epoch 121 total_train_acc: 0.9321170980059398 loss: 3.547443687915802 test_loss: 0.06386633962392807 test_acc: 0.9050772626931567\n",
      "epoch 122 total_train_acc: 0.9342384386932542 loss: 3.5801280438899994 test_loss: 0.329118937253952 test_acc: 0.902317880794702\n",
      "epoch 123 total_train_acc: 0.9314806957997455 loss: 3.5645680353045464 test_loss: 0.40913116931915283 test_acc: 0.9017660044150111\n",
      "epoch 124 total_train_acc: 0.9314806957997455 loss: 3.3981166929006577 test_loss: 0.03592419624328613 test_acc: 0.9061810154525386\n",
      "epoch 125 total_train_acc: 0.9291472210436996 loss: 3.56343574821949 test_loss: 0.4000297784805298 test_acc: 0.8995584988962473\n",
      "epoch 126 total_train_acc: 0.9276622825625795 loss: 3.5986818969249725 test_loss: 0.12776152789592743 test_acc: 0.9006622516556292\n",
      "epoch 127 total_train_acc: 0.9327535002121341 loss: 3.4806812554597855 test_loss: 0.6369358897209167 test_acc: 0.9039735099337748\n",
      "epoch 128 total_train_acc: 0.9374204497242257 loss: 3.3274471759796143 test_loss: 0.16093198955059052 test_acc: 0.9122516556291391\n",
      "epoch 129 total_train_acc: 0.9344505727619856 loss: 3.4682483226060867 test_loss: 0.6420721411705017 test_acc: 0.9017660044150111\n",
      "epoch 130 total_train_acc: 0.9336020364870599 loss: 3.397024780511856 test_loss: 0.23136712610721588 test_acc: 0.9072847682119205\n",
      "epoch 131 total_train_acc: 0.9365719134492999 loss: 3.291246250271797 test_loss: 0.2794763445854187 test_acc: 0.9078366445916115\n",
      "epoch 132 total_train_acc: 0.9355112431056428 loss: 3.4288221895694733 test_loss: 0.185576930642128 test_acc: 0.902317880794702\n",
      "epoch 133 total_train_acc: 0.9338141705557913 loss: 3.462395414710045 test_loss: 0.45864999294281006 test_acc: 0.9072847682119205\n",
      "epoch 134 total_train_acc: 0.9393296563428086 loss: 3.3421528935432434 test_loss: 0.11240237206220627 test_acc: 0.9094922737306843\n",
      "epoch 135 total_train_acc: 0.9340263046245227 loss: 3.372881606221199 test_loss: 0.10660069435834885 test_acc: 0.9050772626931567\n",
      "epoch 136 total_train_acc: 0.9376325837929571 loss: 3.205370679497719 test_loss: 0.4482857882976532 test_acc: 0.9056291390728477\n",
      "epoch 137 total_train_acc: 0.936147645311837 loss: 3.3223575353622437 test_loss: 0.1524292379617691 test_acc: 0.9067328918322296\n",
      "epoch 138 total_train_acc: 0.939966058549003 loss: 3.30865915119648 test_loss: 0.15685343742370605 test_acc: 0.9017660044150111\n",
      "epoch 139 total_train_acc: 0.9406024607551973 loss: 3.118898279964924 test_loss: 0.17007939517498016 test_acc: 0.9078366445916115\n",
      "epoch 140 total_train_acc: 0.9355112431056428 loss: 3.349643237888813 test_loss: 0.22169800102710724 test_acc: 0.9089403973509934\n",
      "epoch 141 total_train_acc: 0.931268561731014 loss: 3.2829791605472565 test_loss: 0.0865255668759346 test_acc: 0.9061810154525386\n",
      "epoch 142 total_train_acc: 0.9382689859991514 loss: 3.2105607762932777 test_loss: 0.20098263025283813 test_acc: 0.9056291390728477\n",
      "epoch 143 total_train_acc: 0.9372083156554942 loss: 3.2743139192461967 test_loss: 0.2505437135696411 test_acc: 0.9083885209713024\n",
      "epoch 144 total_train_acc: 0.9391175222740772 loss: 3.0915322974324226 test_loss: 0.3438452482223511 test_acc: 0.9078366445916115\n",
      "epoch 145 total_train_acc: 0.9357233771743741 loss: 3.3062507808208466 test_loss: 0.23671603202819824 test_acc: 0.9072847682119205\n",
      "epoch 146 total_train_acc: 0.941450997030123 loss: 3.1329138725996017 test_loss: 0.144917294383049 test_acc: 0.9072847682119205\n",
      "epoch 147 total_train_acc: 0.9395417904115401 loss: 3.1138706132769585 test_loss: 0.15127725899219513 test_acc: 0.9078366445916115\n",
      "epoch 148 total_train_acc: 0.9410267288926601 loss: 3.1444335654377937 test_loss: 0.5103012323379517 test_acc: 0.9116997792494481\n",
      "epoch 149 total_train_acc: 0.9408145948239287 loss: 3.089892841875553 test_loss: 0.21164768934249878 test_acc: 0.9083885209713024\n",
      "epoch 150 total_train_acc: 0.9384811200678829 loss: 3.2318617552518845 test_loss: 0.018135609105229378 test_acc: 0.9067328918322296\n",
      "epoch 151 total_train_acc: 0.9429359355112431 loss: 3.132600151002407 test_loss: 0.4425070285797119 test_acc: 0.9045253863134658\n",
      "epoch 152 total_train_acc: 0.943360203648706 loss: 3.0503306463360786 test_loss: 0.24445945024490356 test_acc: 0.9100441501103753\n",
      "epoch 153 total_train_acc: 0.9391175222740772 loss: 3.0981781631708145 test_loss: 0.403810977935791 test_acc: 0.9001103752759382\n",
      "epoch 154 total_train_acc: 0.9420873992363173 loss: 3.046501003205776 test_loss: 0.2437446117401123 test_acc: 0.91280353200883\n",
      "epoch 155 total_train_acc: 0.9431480695799745 loss: 3.0636462047696114 test_loss: 0.39533165097236633 test_acc: 0.9144591611479028\n",
      "epoch 156 total_train_acc: 0.943360203648706 loss: 2.969335824251175 test_loss: 0.1166832447052002 test_acc: 0.9072847682119205\n",
      "epoch 157 total_train_acc: 0.9395417904115401 loss: 3.187133140861988 test_loss: 0.6071242690086365 test_acc: 0.9078366445916115\n",
      "epoch 158 total_train_acc: 0.9397539244802715 loss: 3.1134409308433533 test_loss: 0.09418461471796036 test_acc: 0.9094922737306843\n",
      "epoch 159 total_train_acc: 0.9422995333050488 loss: 3.0439817905426025 test_loss: 0.2986460030078888 test_acc: 0.9094922737306843\n",
      "epoch 160 total_train_acc: 0.9435723377174374 loss: 2.9644665643572807 test_loss: 0.044660523533821106 test_acc: 0.9067328918322296\n",
      "epoch 161 total_train_acc: 0.9435723377174374 loss: 2.971653677523136 test_loss: 0.4941041171550751 test_acc: 0.9133554083885209\n",
      "epoch 162 total_train_acc: 0.9382689859991514 loss: 3.0819664895534515 test_loss: 0.23487280309200287 test_acc: 0.9072847682119205\n",
      "epoch 163 total_train_acc: 0.9450572761985575 loss: 3.0398899391293526 test_loss: 0.5759000182151794 test_acc: 0.9116997792494481\n",
      "epoch 164 total_train_acc: 0.9431480695799745 loss: 3.0352800637483597 test_loss: 0.3946444094181061 test_acc: 0.9111479028697572\n",
      "epoch 165 total_train_acc: 0.9420873992363173 loss: 3.027992859482765 test_loss: 0.29937493801116943 test_acc: 0.9078366445916115\n",
      "epoch 166 total_train_acc: 0.9386932541366143 loss: 2.9915023297071457 test_loss: 0.06840571016073227 test_acc: 0.9050772626931567\n",
      "epoch 167 total_train_acc: 0.948663555366992 loss: 2.843128055334091 test_loss: 0.3043211102485657 test_acc: 0.9100441501103753\n",
      "epoch 168 total_train_acc: 0.9439966058549003 loss: 2.9467236325144768 test_loss: 0.24895180761814117 test_acc: 0.91280353200883\n",
      "epoch 169 total_train_acc: 0.9480271531607977 loss: 2.841838113963604 test_loss: 0.23561008274555206 test_acc: 0.9078366445916115\n",
      "epoch 170 total_train_acc: 0.9429359355112431 loss: 2.885755844414234 test_loss: 0.060480326414108276 test_acc: 0.9094922737306843\n",
      "epoch 171 total_train_acc: 0.9456936784047518 loss: 2.8904122039675713 test_loss: 0.31416967511177063 test_acc: 0.9089403973509934\n",
      "epoch 172 total_train_acc: 0.9465422146796776 loss: 2.8011669144034386 test_loss: 0.13620398938655853 test_acc: 0.9105960264900662\n",
      "epoch 173 total_train_acc: 0.9465422146796776 loss: 2.949882708489895 test_loss: 0.047981228679418564 test_acc: 0.9078366445916115\n",
      "epoch 174 total_train_acc: 0.9484514212982605 loss: 2.7912721261382103 test_loss: 0.14391091465950012 test_acc: 0.9083885209713024\n",
      "epoch 175 total_train_acc: 0.9444208739923632 loss: 2.9328649267554283 test_loss: 0.24480198323726654 test_acc: 0.9116997792494481\n",
      "epoch 176 total_train_acc: 0.9437844717861689 loss: 2.9285998344421387 test_loss: 0.07627382129430771 test_acc: 0.9105960264900662\n",
      "epoch 177 total_train_acc: 0.9420873992363173 loss: 2.903788037598133 test_loss: 0.3252156674861908 test_acc: 0.9133554083885209\n",
      "epoch 178 total_train_acc: 0.9416631310988545 loss: 3.05241971462965 test_loss: 0.2621157467365265 test_acc: 0.9067328918322296\n",
      "epoch 179 total_train_acc: 0.9480271531607977 loss: 2.7313440665602684 test_loss: 0.13946940004825592 test_acc: 0.9100441501103753\n",
      "epoch 180 total_train_acc: 0.9444208739923632 loss: 2.718263953924179 test_loss: 0.7405166625976562 test_acc: 0.9116997792494481\n",
      "epoch 181 total_train_acc: 0.9490878235044549 loss: 2.686385653913021 test_loss: 0.03364134952425957 test_acc: 0.9083885209713024\n",
      "epoch 182 total_train_acc: 0.9471786168858719 loss: 2.8426795601844788 test_loss: 0.1081593856215477 test_acc: 0.91280353200883\n",
      "epoch 183 total_train_acc: 0.9512091641917692 loss: 2.7009529545903206 test_loss: 0.17933332920074463 test_acc: 0.9100441501103753\n",
      "epoch 184 total_train_acc: 0.9505727619855748 loss: 2.640941970050335 test_loss: 0.26524439454078674 test_acc: 0.9100441501103753\n",
      "epoch 185 total_train_acc: 0.9450572761985575 loss: 2.8005392402410507 test_loss: 0.19433335959911346 test_acc: 0.9061810154525386\n",
      "epoch 186 total_train_acc: 0.9482392872295291 loss: 2.708105482161045 test_loss: 0.4164728820323944 test_acc: 0.9105960264900662\n",
      "epoch 187 total_train_acc: 0.9439966058549003 loss: 2.8622224777936935 test_loss: 0.2641623616218567 test_acc: 0.9050772626931567\n",
      "epoch 188 total_train_acc: 0.9482392872295291 loss: 2.7713184356689453 test_loss: 0.009615659713745117 test_acc: 0.9083885209713024\n",
      "epoch 189 total_train_acc: 0.9524819686041578 loss: 2.663630336523056 test_loss: 0.06870725005865097 test_acc: 0.9105960264900662\n",
      "epoch 190 total_train_acc: 0.9490878235044549 loss: 2.6780295073986053 test_loss: 0.14001326262950897 test_acc: 0.9045253863134658\n",
      "epoch 191 total_train_acc: 0.9480271531607977 loss: 2.7003664076328278 test_loss: 0.23998665809631348 test_acc: 0.9133554083885209\n",
      "epoch 192 total_train_acc: 0.9518455663979635 loss: 2.7356544211506844 test_loss: 0.034962091594934464 test_acc: 0.9100441501103753\n",
      "epoch 193 total_train_acc: 0.9518455663979635 loss: 2.6853004470467567 test_loss: 0.08401235938072205 test_acc: 0.9111479028697572\n",
      "epoch 194 total_train_acc: 0.9484514212982605 loss: 2.685091659426689 test_loss: 0.16539163887500763 test_acc: 0.9078366445916115\n",
      "epoch 195 total_train_acc: 0.9531183708103521 loss: 2.530301570892334 test_loss: 0.2123321294784546 test_acc: 0.9116997792494481\n",
      "epoch 196 total_train_acc: 0.9492999575731863 loss: 2.653482511639595 test_loss: 0.09944180399179459 test_acc: 0.9111479028697572\n",
      "epoch 197 total_train_acc: 0.951633432329232 loss: 2.593707412481308 test_loss: 0.0771273598074913 test_acc: 0.9061810154525386\n",
      "epoch 198 total_train_acc: 0.9482392872295291 loss: 2.4995504170656204 test_loss: 0.2648296654224396 test_acc: 0.9111479028697572\n",
      "epoch 199 total_train_acc: 0.9579974543911752 loss: 2.428955413401127 test_loss: 0.0339973159134388 test_acc: 0.9122516556291391\n",
      "epoch 200 total_train_acc: 0.9482392872295291 loss: 2.6497636809945107 test_loss: 0.038337744772434235 test_acc: 0.9100441501103753\n",
      "epoch 201 total_train_acc: 0.9537547730165464 loss: 2.5205403864383698 test_loss: 0.23367799818515778 test_acc: 0.9172185430463576\n",
      "epoch 202 total_train_acc: 0.9524819686041578 loss: 2.4715876430273056 test_loss: 0.30779439210891724 test_acc: 0.9111479028697572\n",
      "epoch 203 total_train_acc: 0.9541790411540093 loss: 2.556512013077736 test_loss: 0.01050635427236557 test_acc: 0.9139072847682119\n",
      "epoch 204 total_train_acc: 0.9550275774289351 loss: 2.4471423998475075 test_loss: 0.16640546917915344 test_acc: 0.9139072847682119\n",
      "epoch 205 total_train_acc: 0.9548154433602036 loss: 2.4846533313393593 test_loss: 0.47881796956062317 test_acc: 0.9116997792494481\n",
      "epoch 206 total_train_acc: 0.9539669070852779 loss: 2.4630336612462997 test_loss: 0.12369873374700546 test_acc: 0.91280353200883\n",
      "epoch 207 total_train_acc: 0.9548154433602036 loss: 2.3831024020910263 test_loss: 0.033861514180898666 test_acc: 0.9111479028697572\n",
      "epoch 208 total_train_acc: 0.9514212982605006 loss: 2.4553059116005898 test_loss: 0.18452055752277374 test_acc: 0.9144591611479028\n",
      "epoch 209 total_train_acc: 0.951633432329232 loss: 2.4250569492578506 test_loss: 0.20681530237197876 test_acc: 0.9116997792494481\n",
      "epoch 210 total_train_acc: 0.9522698345354264 loss: 2.5122931376099586 test_loss: 0.04244644567370415 test_acc: 0.9161147902869757\n",
      "epoch 211 total_train_acc: 0.953542638947815 loss: 2.446266733109951 test_loss: 0.053698450326919556 test_acc: 0.9122516556291391\n",
      "epoch 212 total_train_acc: 0.955451845566398 loss: 2.394710034132004 test_loss: 0.18437419831752777 test_acc: 0.9122516556291391\n",
      "epoch 213 total_train_acc: 0.951633432329232 loss: 2.4497060999274254 test_loss: 0.17982183396816254 test_acc: 0.9105960264900662\n",
      "epoch 214 total_train_acc: 0.9599066610097582 loss: 2.3556410521268845 test_loss: 0.2283528596162796 test_acc: 0.9161147902869757\n",
      "epoch 215 total_train_acc: 0.9529062367416207 loss: 2.4441656172275543 test_loss: 0.018155537545681 test_acc: 0.9139072847682119\n",
      "epoch 216 total_train_acc: 0.9546033092914722 loss: 2.445506177842617 test_loss: 0.2774973213672638 test_acc: 0.9089403973509934\n",
      "epoch 217 total_train_acc: 0.9579974543911752 loss: 2.4057215452194214 test_loss: 0.09970729798078537 test_acc: 0.9144591611479028\n",
      "epoch 218 total_train_acc: 0.9541790411540093 loss: 2.310343436896801 test_loss: 0.2541745603084564 test_acc: 0.9111479028697572\n",
      "epoch 219 total_train_acc: 0.953542638947815 loss: 2.456870459020138 test_loss: 0.03965630382299423 test_acc: 0.9166666666666666\n",
      "epoch 220 total_train_acc: 0.9529062367416207 loss: 2.430281423032284 test_loss: 0.08797178417444229 test_acc: 0.9155629139072847\n",
      "epoch 221 total_train_acc: 0.9558761137038608 loss: 2.392330527305603 test_loss: 0.08919456601142883 test_acc: 0.9155629139072847\n",
      "epoch 222 total_train_acc: 0.9579974543911752 loss: 2.3299998342990875 test_loss: 0.02319582737982273 test_acc: 0.9144591611479028\n",
      "epoch 223 total_train_acc: 0.9567246499787866 loss: 2.2957410141825676 test_loss: 0.3928581476211548 test_acc: 0.9116997792494481\n",
      "epoch 224 total_train_acc: 0.950148493848112 loss: 2.443689815700054 test_loss: 0.06616418808698654 test_acc: 0.9133554083885209\n",
      "epoch 225 total_train_acc: 0.9571489181162495 loss: 2.3088395595550537 test_loss: 0.4638511836528778 test_acc: 0.9111479028697572\n",
      "epoch 226 total_train_acc: 0.9579974543911752 loss: 2.3242122679948807 test_loss: 0.2032136768102646 test_acc: 0.9177704194260485\n",
      "epoch 227 total_train_acc: 0.9533305048790836 loss: 2.33153036236763 test_loss: 0.11485276371240616 test_acc: 0.9105960264900662\n",
      "epoch 228 total_train_acc: 0.9529062367416207 loss: 2.40792652964592 test_loss: 0.03079460747539997 test_acc: 0.91280353200883\n",
      "epoch 229 total_train_acc: 0.9565125159100551 loss: 2.360228382050991 test_loss: 0.15224142372608185 test_acc: 0.9150110375275938\n",
      "epoch 230 total_train_acc: 0.9543911752227408 loss: 2.3703818693757057 test_loss: 0.12615960836410522 test_acc: 0.9133554083885209\n",
      "epoch 231 total_train_acc: 0.9560882477725923 loss: 2.377592973411083 test_loss: 0.03884052857756615 test_acc: 0.9161147902869757\n",
      "epoch 232 total_train_acc: 0.9596945269410267 loss: 2.203838288784027 test_loss: 0.12972486019134521 test_acc: 0.9177704194260485\n",
      "epoch 233 total_train_acc: 0.9567246499787866 loss: 2.307404413819313 test_loss: 0.32849231362342834 test_acc: 0.9105960264900662\n",
      "epoch 234 total_train_acc: 0.9592702588035639 loss: 2.264176979660988 test_loss: 0.4013361930847168 test_acc: 0.9150110375275938\n",
      "epoch 235 total_train_acc: 0.9599066610097582 loss: 2.2851593792438507 test_loss: 0.1915157288312912 test_acc: 0.9139072847682119\n",
      "epoch 236 total_train_acc: 0.9586338565973695 loss: 2.279772236943245 test_loss: 0.059239644557237625 test_acc: 0.9105960264900662\n",
      "epoch 237 total_train_acc: 0.9575731862537123 loss: 2.300270453095436 test_loss: 0.052330534905195236 test_acc: 0.9133554083885209\n",
      "epoch 238 total_train_acc: 0.9594823928722953 loss: 2.1994720324873924 test_loss: 0.3159463703632355 test_acc: 0.9172185430463576\n",
      "epoch 239 total_train_acc: 0.9590581247348324 loss: 2.320958487689495 test_loss: 0.017276830971240997 test_acc: 0.9094922737306843\n",
      "epoch 240 total_train_acc: 0.9579974543911752 loss: 2.282746158540249 test_loss: 0.25083982944488525 test_acc: 0.9139072847682119\n",
      "epoch 241 total_train_acc: 0.9599066610097582 loss: 2.27878288179636 test_loss: 0.0047582280822098255 test_acc: 0.9122516556291391\n",
      "epoch 242 total_train_acc: 0.9567246499787866 loss: 2.2149483039975166 test_loss: 0.03400835394859314 test_acc: 0.91280353200883\n",
      "epoch 243 total_train_acc: 0.9577853203224438 loss: 2.2608968168497086 test_loss: 0.08331546187400818 test_acc: 0.9155629139072847\n",
      "epoch 244 total_train_acc: 0.9573610521849809 loss: 2.18938310444355 test_loss: 0.011465686373412609 test_acc: 0.9133554083885209\n",
      "epoch 245 total_train_acc: 0.9577853203224438 loss: 2.2750610932707787 test_loss: 0.09486424922943115 test_acc: 0.9172185430463576\n",
      "epoch 246 total_train_acc: 0.9605430632159525 loss: 2.255271762609482 test_loss: 0.042902883142232895 test_acc: 0.9172185430463576\n",
      "epoch 247 total_train_acc: 0.9584217225286381 loss: 2.2704673558473587 test_loss: 0.06137393042445183 test_acc: 0.9155629139072847\n",
      "epoch 248 total_train_acc: 0.9586338565973695 loss: 2.160615809261799 test_loss: 0.21521109342575073 test_acc: 0.91280353200883\n",
      "epoch 249 total_train_acc: 0.9594823928722953 loss: 2.2121748998761177 test_loss: 0.18700195848941803 test_acc: 0.9116997792494481\n",
      "epoch 250 total_train_acc: 0.9558761137038608 loss: 2.2970632761716843 test_loss: 0.3924063742160797 test_acc: 0.9177704194260485\n",
      "epoch 251 total_train_acc: 0.958845990666101 loss: 2.3022286370396614 test_loss: 0.026546290144324303 test_acc: 0.9139072847682119\n",
      "epoch 252 total_train_acc: 0.9560882477725923 loss: 2.33564256131649 test_loss: 0.16953562200069427 test_acc: 0.9150110375275938\n",
      "epoch 253 total_train_acc: 0.9633008061094612 loss: 2.1823651045560837 test_loss: 0.10184723883867264 test_acc: 0.9139072847682119\n",
      "epoch 254 total_train_acc: 0.9575731862537123 loss: 2.385775975883007 test_loss: 0.42819884419441223 test_acc: 0.9166666666666666\n",
      "epoch 255 total_train_acc: 0.9573610521849809 loss: 2.2083337008953094 test_loss: 0.06831584125757217 test_acc: 0.9166666666666666\n",
      "epoch 256 total_train_acc: 0.9639372083156555 loss: 2.0861852914094925 test_loss: 0.3750976324081421 test_acc: 0.9166666666666666\n",
      "epoch 257 total_train_acc: 0.9620280016970726 loss: 2.2136072739958763 test_loss: 0.17463135719299316 test_acc: 0.91280353200883\n",
      "epoch 258 total_train_acc: 0.9563003818413237 loss: 2.2778010591864586 test_loss: 0.049994755536317825 test_acc: 0.9139072847682119\n",
      "epoch 259 total_train_acc: 0.9603309291472211 loss: 2.1510614827275276 test_loss: 0.11762654036283493 test_acc: 0.9155629139072847\n",
      "epoch 260 total_train_acc: 0.958845990666101 loss: 2.332023151218891 test_loss: 0.10200972110033035 test_acc: 0.9133554083885209\n",
      "epoch 261 total_train_acc: 0.9596945269410267 loss: 2.2315279692411423 test_loss: 0.07595639675855637 test_acc: 0.9133554083885209\n",
      "epoch 262 total_train_acc: 0.9582095884599067 loss: 2.273009281605482 test_loss: 0.2293512374162674 test_acc: 0.9111479028697572\n",
      "epoch 263 total_train_acc: 0.958845990666101 loss: 2.071685239672661 test_loss: 0.46493789553642273 test_acc: 0.9122516556291391\n",
      "epoch 264 total_train_acc: 0.9601187950784896 loss: 2.0920020639896393 test_loss: 0.12156669050455093 test_acc: 0.91280353200883\n",
      "epoch 265 total_train_acc: 0.9630886720407298 loss: 2.1230086013674736 test_loss: 0.2949986755847931 test_acc: 0.9155629139072847\n",
      "epoch 266 total_train_acc: 0.9567246499787866 loss: 2.2018879130482674 test_loss: 0.401674747467041 test_acc: 0.9139072847682119\n",
      "epoch 267 total_train_acc: 0.9575731862537123 loss: 2.132997915148735 test_loss: 0.19440315663814545 test_acc: 0.9150110375275938\n",
      "epoch 268 total_train_acc: 0.9596945269410267 loss: 2.221254788339138 test_loss: 0.08452543616294861 test_acc: 0.91280353200883\n",
      "epoch 269 total_train_acc: 0.9594823928722953 loss: 2.179910533130169 test_loss: 0.06661690026521683 test_acc: 0.9155629139072847\n",
      "epoch 270 total_train_acc: 0.9550275774289351 loss: 2.281516507267952 test_loss: 0.14999330043792725 test_acc: 0.9161147902869757\n",
      "epoch 271 total_train_acc: 0.9596945269410267 loss: 2.2048352658748627 test_loss: 0.01778220571577549 test_acc: 0.9111479028697572\n",
      "epoch 272 total_train_acc: 0.9596945269410267 loss: 2.177728220820427 test_loss: 0.02826179377734661 test_acc: 0.9166666666666666\n",
      "epoch 273 total_train_acc: 0.9586338565973695 loss: 2.142238676548004 test_loss: 0.3876707851886749 test_acc: 0.9139072847682119\n",
      "epoch 274 total_train_acc: 0.9609673313534154 loss: 2.090970315039158 test_loss: 0.18611516058444977 test_acc: 0.9161147902869757\n",
      "epoch 275 total_train_acc: 0.960755197284684 loss: 2.142836943268776 test_loss: 0.07115443795919418 test_acc: 0.9122516556291391\n",
      "epoch 276 total_train_acc: 0.962240135765804 loss: 2.0940604731440544 test_loss: 0.0214957594871521 test_acc: 0.9089403973509934\n",
      "epoch 277 total_train_acc: 0.9616037335596097 loss: 2.188746377825737 test_loss: 0.24502259492874146 test_acc: 0.9150110375275938\n",
      "epoch 278 total_train_acc: 0.9573610521849809 loss: 2.1569726690649986 test_loss: 0.09222853928804398 test_acc: 0.9155629139072847\n",
      "epoch 279 total_train_acc: 0.9613915994908783 loss: 2.117422230541706 test_loss: 0.035827841609716415 test_acc: 0.9144591611479028\n",
      "epoch 280 total_train_acc: 0.965634280865507 loss: 2.064580351114273 test_loss: 0.1155676618218422 test_acc: 0.9166666666666666\n",
      "epoch 281 total_train_acc: 0.9618158676283411 loss: 2.0099653378129005 test_loss: 0.20898252725601196 test_acc: 0.9183222958057395\n",
      "epoch 282 total_train_acc: 0.9609673313534154 loss: 2.1340415254235268 test_loss: 0.21399842202663422 test_acc: 0.9105960264900662\n",
      "epoch 283 total_train_acc: 0.9590581247348324 loss: 2.122119925916195 test_loss: 0.06434150785207748 test_acc: 0.9166666666666666\n",
      "epoch 284 total_train_acc: 0.9628765379719983 loss: 2.080413267016411 test_loss: 0.5374100804328918 test_acc: 0.9155629139072847\n",
      "epoch 285 total_train_acc: 0.9613915994908783 loss: 2.117935597896576 test_loss: 0.27517881989479065 test_acc: 0.91280353200883\n",
      "epoch 286 total_train_acc: 0.9594823928722953 loss: 2.0739696100354195 test_loss: 0.26202839612960815 test_acc: 0.9105960264900662\n",
      "epoch 287 total_train_acc: 0.9639372083156555 loss: 1.9957722090184689 test_loss: 0.2804020047187805 test_acc: 0.9172185430463576\n",
      "epoch 288 total_train_acc: 0.960755197284684 loss: 2.063548691570759 test_loss: 0.2734444737434387 test_acc: 0.9150110375275938\n",
      "epoch 289 total_train_acc: 0.9624522698345355 loss: 2.074456848204136 test_loss: 0.29989728331565857 test_acc: 0.9105960264900662\n",
      "epoch 290 total_train_acc: 0.9637250742469241 loss: 1.9854561015963554 test_loss: 0.2451990842819214 test_acc: 0.9150110375275938\n",
      "epoch 291 total_train_acc: 0.9666949512091642 loss: 1.97404745221138 test_loss: 0.7204616069793701 test_acc: 0.9144591611479028\n",
      "epoch 292 total_train_acc: 0.9611794654221468 loss: 2.0338454991579056 test_loss: 0.1882903128862381 test_acc: 0.9150110375275938\n",
      "epoch 293 total_train_acc: 0.9628765379719983 loss: 2.036748915910721 test_loss: 0.5146409869194031 test_acc: 0.9155629139072847\n",
      "epoch 294 total_train_acc: 0.9645736105218498 loss: 1.9883606135845184 test_loss: 0.04506943002343178 test_acc: 0.9172185430463576\n",
      "epoch 295 total_train_acc: 0.958845990666101 loss: 2.157683528959751 test_loss: 0.2629562020301819 test_acc: 0.9094922737306843\n",
      "epoch 296 total_train_acc: 0.964149342384387 loss: 2.0599603801965714 test_loss: 0.23608291149139404 test_acc: 0.9172185430463576\n",
      "epoch 297 total_train_acc: 0.9630886720407298 loss: 2.0622957944869995 test_loss: 0.48081931471824646 test_acc: 0.9122516556291391\n",
      "epoch 298 total_train_acc: 0.9630886720407298 loss: 2.0827459916472435 test_loss: 0.5771278738975525 test_acc: 0.9144591611479028\n",
      "epoch 299 total_train_acc: 0.9633008061094612 loss: 2.009027361869812 test_loss: 0.039371129125356674 test_acc: 0.9122516556291391\n",
      "epoch 300 total_train_acc: 0.9645736105218498 loss: 2.028285637497902 test_loss: 0.09890997409820557 test_acc: 0.9166666666666666\n",
      "epoch 301 total_train_acc: 0.9630886720407298 loss: 2.0619052350521088 test_loss: 0.31243690848350525 test_acc: 0.9150110375275938\n",
      "epoch 302 total_train_acc: 0.9633008061094612 loss: 2.0096691995859146 test_loss: 0.15852807462215424 test_acc: 0.9139072847682119\n",
      "epoch 303 total_train_acc: 0.9649978786593126 loss: 1.9595239087939262 test_loss: 0.3404920995235443 test_acc: 0.9194260485651214\n",
      "epoch 304 total_train_acc: 0.9596945269410267 loss: 2.0157776698470116 test_loss: 0.2166922688484192 test_acc: 0.9183222958057395\n",
      "epoch 305 total_train_acc: 0.967119219346627 loss: 1.9176358357071877 test_loss: 0.31905895471572876 test_acc: 0.9116997792494481\n",
      "epoch 306 total_train_acc: 0.964149342384387 loss: 2.020532503724098 test_loss: 0.02965456247329712 test_acc: 0.9144591611479028\n",
      "epoch 307 total_train_acc: 0.9630886720407298 loss: 2.022263213992119 test_loss: 0.32824721932411194 test_acc: 0.9144591611479028\n",
      "epoch 308 total_train_acc: 0.9647857445905812 loss: 1.9854403510689735 test_loss: 0.30246207118034363 test_acc: 0.9172185430463576\n",
      "epoch 309 total_train_acc: 0.9618158676283411 loss: 1.9897321686148643 test_loss: 0.2813011407852173 test_acc: 0.9166666666666666\n",
      "epoch 310 total_train_acc: 0.9635129401781927 loss: 2.0432659089565277 test_loss: 0.34001970291137695 test_acc: 0.9150110375275938\n",
      "epoch 311 total_train_acc: 0.9666949512091642 loss: 1.9626823961734772 test_loss: 0.2487933188676834 test_acc: 0.9155629139072847\n",
      "epoch 312 total_train_acc: 0.9643614764531183 loss: 2.0183857902884483 test_loss: 0.4409691393375397 test_acc: 0.9166666666666666\n",
      "epoch 313 total_train_acc: 0.9643614764531183 loss: 1.9382195584475994 test_loss: 0.09048078209161758 test_acc: 0.91280353200883\n",
      "epoch 314 total_train_acc: 0.9599066610097582 loss: 1.9685811921954155 test_loss: 0.8650334477424622 test_acc: 0.9150110375275938\n",
      "epoch 315 total_train_acc: 0.9647857445905812 loss: 1.9066498391330242 test_loss: 0.10223957151174545 test_acc: 0.9155629139072847\n",
      "epoch 316 total_train_acc: 0.9630886720407298 loss: 2.0532292425632477 test_loss: 0.1720184087753296 test_acc: 0.9144591611479028\n",
      "epoch 317 total_train_acc: 0.9664828171404327 loss: 1.905528862029314 test_loss: 0.06502769887447357 test_acc: 0.9150110375275938\n",
      "epoch 318 total_train_acc: 0.9675434874840899 loss: 1.9772493615746498 test_loss: 0.14159062504768372 test_acc: 0.9139072847682119\n",
      "epoch 319 total_train_acc: 0.9628765379719983 loss: 1.96968524903059 test_loss: 0.3164561092853546 test_acc: 0.9139072847682119\n",
      "epoch 320 total_train_acc: 0.9679677556215528 loss: 1.9596564657986164 test_loss: 0.3889777362346649 test_acc: 0.9183222958057395\n",
      "epoch 321 total_train_acc: 0.9613915994908783 loss: 1.9956543371081352 test_loss: 0.15026195347309113 test_acc: 0.9150110375275938\n",
      "epoch 322 total_train_acc: 0.9635129401781927 loss: 1.9288341030478477 test_loss: 0.2179354429244995 test_acc: 0.9144591611479028\n",
      "epoch 323 total_train_acc: 0.9637250742469241 loss: 1.974581517279148 test_loss: 0.11178931593894958 test_acc: 0.9155629139072847\n",
      "epoch 324 total_train_acc: 0.9673313534153585 loss: 1.9723193049430847 test_loss: 0.2547859847545624 test_acc: 0.9161147902869757\n",
      "epoch 325 total_train_acc: 0.9662706830717013 loss: 1.9559548161923885 test_loss: 0.11303297430276871 test_acc: 0.9166666666666666\n",
      "epoch 326 total_train_acc: 0.9635129401781927 loss: 1.9659027718007565 test_loss: 0.19066303968429565 test_acc: 0.9161147902869757\n",
      "epoch 327 total_train_acc: 0.9647857445905812 loss: 1.9676413722336292 test_loss: 0.37427738308906555 test_acc: 0.9133554083885209\n",
      "epoch 328 total_train_acc: 0.9666949512091642 loss: 1.87163046002388 test_loss: 0.10341861844062805 test_acc: 0.9133554083885209\n",
      "epoch 329 total_train_acc: 0.9649978786593126 loss: 1.9634428843855858 test_loss: 0.18400655686855316 test_acc: 0.9177704194260485\n",
      "epoch 330 total_train_acc: 0.9645736105218498 loss: 1.924962617456913 test_loss: 0.004647162277251482 test_acc: 0.9155629139072847\n",
      "epoch 331 total_train_acc: 0.9626644039032669 loss: 1.9717543125152588 test_loss: 0.0587761290371418 test_acc: 0.9177704194260485\n",
      "epoch 332 total_train_acc: 0.9692405600339414 loss: 1.8240089602768421 test_loss: 0.29500263929367065 test_acc: 0.9177704194260485\n",
      "epoch 333 total_train_acc: 0.9686041578277471 loss: 1.8776432946324348 test_loss: 0.0756077989935875 test_acc: 0.9177704194260485\n",
      "epoch 334 total_train_acc: 0.964149342384387 loss: 1.968228667974472 test_loss: 0.0149242477491498 test_acc: 0.9166666666666666\n",
      "epoch 335 total_train_acc: 0.9662706830717013 loss: 1.9457081034779549 test_loss: 0.20505058765411377 test_acc: 0.9166666666666666\n",
      "epoch 336 total_train_acc: 0.9630886720407298 loss: 2.0163825526833534 test_loss: 0.454938143491745 test_acc: 0.9150110375275938\n",
      "epoch 337 total_train_acc: 0.9643614764531183 loss: 1.889788817614317 test_loss: 0.2102879285812378 test_acc: 0.9133554083885209\n",
      "epoch 338 total_train_acc: 0.9658464149342384 loss: 1.8771602734923363 test_loss: 0.18173176050186157 test_acc: 0.9166666666666666\n",
      "epoch 339 total_train_acc: 0.9675434874840899 loss: 1.8489690124988556 test_loss: 0.16318640112876892 test_acc: 0.9122516556291391\n",
      "epoch 340 total_train_acc: 0.967119219346627 loss: 1.8366183899343014 test_loss: 0.19573324918746948 test_acc: 0.9155629139072847\n",
      "epoch 341 total_train_acc: 0.9679677556215528 loss: 1.9456288665533066 test_loss: 0.2217872142791748 test_acc: 0.9139072847682119\n",
      "epoch 342 total_train_acc: 0.964149342384387 loss: 1.8945696912705898 test_loss: 0.41451215744018555 test_acc: 0.9161147902869757\n",
      "epoch 343 total_train_acc: 0.9647857445905812 loss: 1.8880486637353897 test_loss: 0.25051435828208923 test_acc: 0.9177704194260485\n",
      "epoch 344 total_train_acc: 0.9686041578277471 loss: 1.8168813847005367 test_loss: 0.6582790017127991 test_acc: 0.9161147902869757\n",
      "epoch 345 total_train_acc: 0.9654221467967755 loss: 1.8907191529870033 test_loss: 0.2312847226858139 test_acc: 0.9144591611479028\n",
      "epoch 346 total_train_acc: 0.9660585490029698 loss: 1.8086767196655273 test_loss: 0.16715121269226074 test_acc: 0.9133554083885209\n",
      "epoch 347 total_train_acc: 0.9639372083156555 loss: 1.9366368874907494 test_loss: 0.038939815014600754 test_acc: 0.9139072847682119\n",
      "epoch 348 total_train_acc: 0.9662706830717013 loss: 1.8994987420737743 test_loss: 0.0905420109629631 test_acc: 0.9155629139072847\n",
      "epoch 349 total_train_acc: 0.9698769622401358 loss: 1.7701872736215591 test_loss: 0.006726061459630728 test_acc: 0.9144591611479028\n",
      "epoch 350 total_train_acc: 0.9649978786593126 loss: 1.8117039501667023 test_loss: 0.2785801887512207 test_acc: 0.91280353200883\n",
      "epoch 351 total_train_acc: 0.9649978786593126 loss: 1.8931331187486649 test_loss: 0.7453126907348633 test_acc: 0.9139072847682119\n",
      "epoch 352 total_train_acc: 0.9660585490029698 loss: 1.8190738074481487 test_loss: 0.17108626663684845 test_acc: 0.9144591611479028\n",
      "epoch 353 total_train_acc: 0.9681798896902842 loss: 1.8518254607915878 test_loss: 0.09300773590803146 test_acc: 0.9161147902869757\n",
      "epoch 354 total_train_acc: 0.9639372083156555 loss: 1.8813013657927513 test_loss: 0.31074488162994385 test_acc: 0.9150110375275938\n",
      "epoch 355 total_train_acc: 0.9654221467967755 loss: 1.9309149459004402 test_loss: 0.2340279072523117 test_acc: 0.9150110375275938\n",
      "epoch 356 total_train_acc: 0.9683920237590157 loss: 1.8508725315332413 test_loss: 0.08941110968589783 test_acc: 0.9166666666666666\n",
      "epoch 357 total_train_acc: 0.9652100127280441 loss: 1.9376071766018867 test_loss: 0.16324502229690552 test_acc: 0.9133554083885209\n",
      "epoch 358 total_train_acc: 0.9635129401781927 loss: 2.0073765888810158 test_loss: 0.2888847887516022 test_acc: 0.9150110375275938\n",
      "epoch 359 total_train_acc: 0.9677556215528214 loss: 1.8290471956133842 test_loss: 1.1691670417785645 test_acc: 0.9150110375275938\n",
      "epoch 360 total_train_acc: 0.9673313534153585 loss: 1.8316084742546082 test_loss: 0.08156851679086685 test_acc: 0.9144591611479028\n",
      "epoch 361 total_train_acc: 0.9692405600339414 loss: 1.8324629068374634 test_loss: 0.22824423015117645 test_acc: 0.9150110375275938\n",
      "epoch 362 total_train_acc: 0.96902842596521 loss: 1.8127127848565578 test_loss: 0.3734501600265503 test_acc: 0.9155629139072847\n",
      "epoch 363 total_train_acc: 0.9649978786593126 loss: 1.8543623238801956 test_loss: 0.0903920903801918 test_acc: 0.9172185430463576\n",
      "epoch 364 total_train_acc: 0.9664828171404327 loss: 1.8363198563456535 test_loss: 0.1785445362329483 test_acc: 0.9172185430463576\n",
      "epoch 365 total_train_acc: 0.9688162918964786 loss: 1.8000398948788643 test_loss: 0.2122579962015152 test_acc: 0.9172185430463576\n",
      "epoch 366 total_train_acc: 0.9639372083156555 loss: 1.9063032418489456 test_loss: 0.09997732192277908 test_acc: 0.91280353200883\n",
      "epoch 367 total_train_acc: 0.9717861688587187 loss: 1.6529957354068756 test_loss: 0.054840922355651855 test_acc: 0.9155629139072847\n",
      "epoch 368 total_train_acc: 0.9654221467967755 loss: 1.8273944146931171 test_loss: 0.27743101119995117 test_acc: 0.9177704194260485\n",
      "epoch 369 total_train_acc: 0.9696648281714043 loss: 1.7778053879737854 test_loss: 0.258052259683609 test_acc: 0.9144591611479028\n",
      "epoch 370 total_train_acc: 0.9654221467967755 loss: 1.9278069287538528 test_loss: 0.38671228289604187 test_acc: 0.9139072847682119\n",
      "epoch 371 total_train_acc: 0.9637250742469241 loss: 1.9874829985201359 test_loss: 0.10159045457839966 test_acc: 0.9144591611479028\n",
      "epoch 372 total_train_acc: 0.9717861688587187 loss: 1.790348082780838 test_loss: 0.09782439470291138 test_acc: 0.9122516556291391\n",
      "epoch 373 total_train_acc: 0.9673313534153585 loss: 1.7317992821335793 test_loss: 0.04279381409287453 test_acc: 0.9144591611479028\n",
      "epoch 374 total_train_acc: 0.9677556215528214 loss: 1.7994154952466488 test_loss: 0.2682521939277649 test_acc: 0.9172185430463576\n",
      "epoch 375 total_train_acc: 0.9639372083156555 loss: 1.8655961342155933 test_loss: 0.1657087355852127 test_acc: 0.9183222958057395\n",
      "epoch 376 total_train_acc: 0.9639372083156555 loss: 1.9407635927200317 test_loss: 0.008943143300712109 test_acc: 0.9144591611479028\n",
      "epoch 377 total_train_acc: 0.9698769622401358 loss: 1.760478813201189 test_loss: 0.283882737159729 test_acc: 0.9155629139072847\n",
      "epoch 378 total_train_acc: 0.9696648281714043 loss: 1.7299165055155754 test_loss: 0.3404681384563446 test_acc: 0.9183222958057395\n",
      "epoch 379 total_train_acc: 0.9633008061094612 loss: 1.8387935012578964 test_loss: 0.12427658587694168 test_acc: 0.9161147902869757\n",
      "epoch 380 total_train_acc: 0.9696648281714043 loss: 1.6416113749146461 test_loss: 0.013296320103108883 test_acc: 0.9150110375275938\n",
      "epoch 381 total_train_acc: 0.9681798896902842 loss: 1.8202357962727547 test_loss: 0.08263114839792252 test_acc: 0.9166666666666666\n",
      "epoch 382 total_train_acc: 0.9677556215528214 loss: 1.7901900112628937 test_loss: 0.025530874729156494 test_acc: 0.9150110375275938\n",
      "epoch 383 total_train_acc: 0.9696648281714043 loss: 1.7489118203520775 test_loss: 0.6130533814430237 test_acc: 0.9144591611479028\n",
      "epoch 384 total_train_acc: 0.9677556215528214 loss: 1.8864538446068764 test_loss: 0.04013281688094139 test_acc: 0.9155629139072847\n",
      "epoch 385 total_train_acc: 0.9692405600339414 loss: 1.7450752891600132 test_loss: 0.010977942496538162 test_acc: 0.9150110375275938\n",
      "epoch 386 total_train_acc: 0.9669070852778956 loss: 1.8356684930622578 test_loss: 0.13121028244495392 test_acc: 0.9144591611479028\n",
      "epoch 387 total_train_acc: 0.9686041578277471 loss: 1.8169058859348297 test_loss: 0.11557165533304214 test_acc: 0.9172185430463576\n",
      "epoch 388 total_train_acc: 0.9675434874840899 loss: 1.7815270125865936 test_loss: 0.00038086375570856035 test_acc: 0.9133554083885209\n",
      "epoch 389 total_train_acc: 0.9705133644463301 loss: 1.672553077340126 test_loss: 0.19903814792633057 test_acc: 0.9150110375275938\n",
      "epoch 390 total_train_acc: 0.9681798896902842 loss: 1.7254820838570595 test_loss: 0.04810886085033417 test_acc: 0.9205298013245033\n",
      "epoch 391 total_train_acc: 0.9675434874840899 loss: 1.8546834290027618 test_loss: 0.006334943696856499 test_acc: 0.9199779249448123\n",
      "epoch 392 total_train_acc: 0.9703012303775986 loss: 1.8852738440036774 test_loss: 0.040984585881233215 test_acc: 0.9161147902869757\n",
      "epoch 393 total_train_acc: 0.9666949512091642 loss: 1.7793729826807976 test_loss: 0.1749473661184311 test_acc: 0.9166666666666666\n",
      "epoch 394 total_train_acc: 0.967119219346627 loss: 1.8208940401673317 test_loss: 0.06448616832494736 test_acc: 0.9161147902869757\n",
      "epoch 395 total_train_acc: 0.9633008061094612 loss: 1.9440038204193115 test_loss: 0.4173758029937744 test_acc: 0.9133554083885209\n",
      "epoch 396 total_train_acc: 0.9707254985150615 loss: 1.7962263450026512 test_loss: 0.12689585983753204 test_acc: 0.91280353200883\n",
      "epoch 397 total_train_acc: 0.970937632583793 loss: 1.7694229893386364 test_loss: 0.5346865057945251 test_acc: 0.9161147902869757\n",
      "epoch 398 total_train_acc: 0.96902842596521 loss: 1.7583155818283558 test_loss: 0.14682841300964355 test_acc: 0.9172185430463576\n",
      "epoch 399 total_train_acc: 0.9711497666525244 loss: 1.6896237693727016 test_loss: 0.17456428706645966 test_acc: 0.9161147902869757\n",
      "epoch 400 total_train_acc: 0.9703012303775986 loss: 1.7164174765348434 test_loss: 0.30098405480384827 test_acc: 0.9172185430463576\n",
      "epoch 401 total_train_acc: 0.9669070852778956 loss: 1.8466832377016544 test_loss: 0.6502193808555603 test_acc: 0.9155629139072847\n",
      "epoch 402 total_train_acc: 0.9696648281714043 loss: 1.7573054321110249 test_loss: 0.5602036118507385 test_acc: 0.9161147902869757\n",
      "epoch 403 total_train_acc: 0.9673313534153585 loss: 1.8237711228430271 test_loss: 0.0003657591005321592 test_acc: 0.9150110375275938\n",
      "epoch 404 total_train_acc: 0.9696648281714043 loss: 1.7575832158327103 test_loss: 0.12591807544231415 test_acc: 0.9155629139072847\n",
      "epoch 405 total_train_acc: 0.9675434874840899 loss: 1.749698981642723 test_loss: 0.2953055202960968 test_acc: 0.9144591611479028\n",
      "epoch 406 total_train_acc: 0.967119219346627 loss: 1.7260621264576912 test_loss: 0.044001538306474686 test_acc: 0.9150110375275938\n",
      "epoch 407 total_train_acc: 0.9683920237590157 loss: 1.7576599642634392 test_loss: 0.09788450598716736 test_acc: 0.9155629139072847\n",
      "epoch 408 total_train_acc: 0.9694526941026729 loss: 1.695482112467289 test_loss: 0.11563581228256226 test_acc: 0.9172185430463576\n",
      "epoch 409 total_train_acc: 0.9649978786593126 loss: 1.8568427488207817 test_loss: 0.128448948264122 test_acc: 0.9161147902869757\n",
      "epoch 410 total_train_acc: 0.9652100127280441 loss: 1.8161559402942657 test_loss: 0.5579020977020264 test_acc: 0.9161147902869757\n",
      "epoch 411 total_train_acc: 0.9679677556215528 loss: 1.768799889832735 test_loss: 0.6149684190750122 test_acc: 0.9161147902869757\n",
      "epoch 412 total_train_acc: 0.9692405600339414 loss: 1.7133192978799343 test_loss: 0.13936498761177063 test_acc: 0.9172185430463576\n",
      "epoch 413 total_train_acc: 0.9703012303775986 loss: 1.8235931172966957 test_loss: 0.43254029750823975 test_acc: 0.9166666666666666\n",
      "epoch 414 total_train_acc: 0.9673313534153585 loss: 1.778680257499218 test_loss: 0.08747097849845886 test_acc: 0.9172185430463576\n",
      "epoch 415 total_train_acc: 0.9673313534153585 loss: 1.7960580736398697 test_loss: 0.05213546380400658 test_acc: 0.9177704194260485\n",
      "epoch 416 total_train_acc: 0.9675434874840899 loss: 1.6970438994467258 test_loss: 0.0535067617893219 test_acc: 0.9172185430463576\n",
      "epoch 417 total_train_acc: 0.9705133644463301 loss: 1.70719575881958 test_loss: 0.030655255541205406 test_acc: 0.9161147902869757\n",
      "epoch 418 total_train_acc: 0.9715740347899873 loss: 1.6950573027133942 test_loss: 0.2985658347606659 test_acc: 0.9116997792494481\n",
      "epoch 419 total_train_acc: 0.9662706830717013 loss: 1.7949990183115005 test_loss: 0.028958650305867195 test_acc: 0.9144591611479028\n",
      "epoch 420 total_train_acc: 0.9681798896902842 loss: 1.812347423285246 test_loss: 0.2223314642906189 test_acc: 0.9144591611479028\n",
      "epoch 421 total_train_acc: 0.9707254985150615 loss: 1.703295048326254 test_loss: 0.327781617641449 test_acc: 0.9155629139072847\n",
      "epoch 422 total_train_acc: 0.9658464149342384 loss: 1.8742413073778152 test_loss: 0.47399958968162537 test_acc: 0.9155629139072847\n",
      "epoch 423 total_train_acc: 0.9696648281714043 loss: 1.6803730838000774 test_loss: 0.037855420261621475 test_acc: 0.9172185430463576\n",
      "epoch 424 total_train_acc: 0.970937632583793 loss: 1.676961250603199 test_loss: 0.0008228273945860565 test_acc: 0.9172185430463576\n",
      "epoch 425 total_train_acc: 0.9719983029274502 loss: 1.6520842052996159 test_loss: 0.13861821591854095 test_acc: 0.9177704194260485\n",
      "epoch 426 total_train_acc: 0.9662706830717013 loss: 1.7019163705408573 test_loss: 0.30980977416038513 test_acc: 0.9172185430463576\n",
      "epoch 427 total_train_acc: 0.9692405600339414 loss: 1.8401205576956272 test_loss: 0.4050399363040924 test_acc: 0.9172185430463576\n",
      "epoch 428 total_train_acc: 0.9705133644463301 loss: 1.7574882619082928 test_loss: 0.7376989722251892 test_acc: 0.9172185430463576\n",
      "epoch 429 total_train_acc: 0.9717861688587187 loss: 1.6803493350744247 test_loss: 0.005752303171902895 test_acc: 0.9166666666666666\n",
      "epoch 430 total_train_acc: 0.9647857445905812 loss: 1.8198453485965729 test_loss: 0.19994060695171356 test_acc: 0.9177704194260485\n",
      "epoch 431 total_train_acc: 0.9669070852778956 loss: 1.75070870667696 test_loss: 0.08933470398187637 test_acc: 0.9183222958057395\n",
      "epoch 432 total_train_acc: 0.9726347051336445 loss: 1.6696778014302254 test_loss: 0.06626545637845993 test_acc: 0.9194260485651214\n",
      "epoch 433 total_train_acc: 0.9683920237590157 loss: 1.7876717410981655 test_loss: 0.33408844470977783 test_acc: 0.9155629139072847\n",
      "epoch 434 total_train_acc: 0.96902842596521 loss: 1.6583656817674637 test_loss: 0.31931135058403015 test_acc: 0.9150110375275938\n",
      "epoch 435 total_train_acc: 0.9669070852778956 loss: 1.7318655624985695 test_loss: 0.08387338370084763 test_acc: 0.9161147902869757\n",
      "epoch 436 total_train_acc: 0.970937632583793 loss: 1.6675111539661884 test_loss: 0.06961416453123093 test_acc: 0.9150110375275938\n",
      "epoch 437 total_train_acc: 0.9700890963088672 loss: 1.5864073932170868 test_loss: 0.0037146033719182014 test_acc: 0.9172185430463576\n",
      "epoch 438 total_train_acc: 0.9686041578277471 loss: 1.770991187542677 test_loss: 0.2376570701599121 test_acc: 0.9172185430463576\n",
      "epoch 439 total_train_acc: 0.9664828171404327 loss: 1.744962178170681 test_loss: 0.12859927117824554 test_acc: 0.9161147902869757\n",
      "epoch 440 total_train_acc: 0.9654221467967755 loss: 1.8079880326986313 test_loss: 0.33399972319602966 test_acc: 0.9172185430463576\n",
      "epoch 441 total_train_acc: 0.9662706830717013 loss: 1.6686296872794628 test_loss: 0.5629860758781433 test_acc: 0.9144591611479028\n",
      "epoch 442 total_train_acc: 0.9681798896902842 loss: 1.7446470893919468 test_loss: 0.06585273891687393 test_acc: 0.9150110375275938\n",
      "epoch 443 total_train_acc: 0.970937632583793 loss: 1.7254525050520897 test_loss: 0.25890693068504333 test_acc: 0.9161147902869757\n",
      "epoch 444 total_train_acc: 0.9707254985150615 loss: 1.7184151485562325 test_loss: 0.2364727109670639 test_acc: 0.9172185430463576\n",
      "epoch 445 total_train_acc: 0.9700890963088672 loss: 1.634834460914135 test_loss: 0.08360185474157333 test_acc: 0.9150110375275938\n",
      "epoch 446 total_train_acc: 0.9736953754773017 loss: 1.558196172118187 test_loss: 0.23060397803783417 test_acc: 0.9155629139072847\n",
      "epoch 447 total_train_acc: 0.9722104369961816 loss: 1.6977662928402424 test_loss: 0.2608295977115631 test_acc: 0.9144591611479028\n",
      "epoch 448 total_train_acc: 0.9683920237590157 loss: 1.753042209893465 test_loss: 0.1138714924454689 test_acc: 0.9166666666666666\n",
      "epoch 449 total_train_acc: 0.9675434874840899 loss: 1.7528364397585392 test_loss: 0.10288352519273758 test_acc: 0.9166666666666666\n",
      "epoch 450 total_train_acc: 0.9692405600339414 loss: 1.7325764074921608 test_loss: 0.23987950384616852 test_acc: 0.9166666666666666\n",
      "epoch 451 total_train_acc: 0.96902842596521 loss: 1.6741597168147564 test_loss: 0.06968662887811661 test_acc: 0.9172185430463576\n",
      "epoch 452 total_train_acc: 0.9722104369961816 loss: 1.634699258953333 test_loss: 0.3981296122074127 test_acc: 0.9172185430463576\n",
      "epoch 453 total_train_acc: 0.96902842596521 loss: 1.716693066060543 test_loss: 0.2946983873844147 test_acc: 0.9155629139072847\n",
      "epoch 454 total_train_acc: 0.9707254985150615 loss: 1.7330626286566257 test_loss: 0.08111352473497391 test_acc: 0.9161147902869757\n",
      "epoch 455 total_train_acc: 0.9681798896902842 loss: 1.6600074283778667 test_loss: 0.01050566229969263 test_acc: 0.9172185430463576\n",
      "epoch 456 total_train_acc: 0.9692405600339414 loss: 1.7910985052585602 test_loss: 0.030733147636055946 test_acc: 0.9166666666666666\n",
      "epoch 457 total_train_acc: 0.9717861688587187 loss: 1.6479616425931454 test_loss: 0.8195876479148865 test_acc: 0.9150110375275938\n",
      "epoch 458 total_train_acc: 0.9683920237590157 loss: 1.74693151563406 test_loss: 0.3638066053390503 test_acc: 0.9161147902869757\n",
      "epoch 459 total_train_acc: 0.9705133644463301 loss: 1.6620373874902725 test_loss: 0.027106814086437225 test_acc: 0.9150110375275938\n",
      "epoch 460 total_train_acc: 0.9711497666525244 loss: 1.669395487755537 test_loss: 0.00959034450352192 test_acc: 0.9150110375275938\n",
      "epoch 461 total_train_acc: 0.9669070852778956 loss: 1.7773653529584408 test_loss: 0.21561847627162933 test_acc: 0.9155629139072847\n",
      "epoch 462 total_train_acc: 0.972422571064913 loss: 1.6793612986803055 test_loss: 0.5467022061347961 test_acc: 0.9166666666666666\n",
      "epoch 463 total_train_acc: 0.976240984302079 loss: 1.5838917084038258 test_loss: 0.13270291686058044 test_acc: 0.9166666666666666\n",
      "epoch 464 total_train_acc: 0.9722104369961816 loss: 1.6440396942198277 test_loss: 0.027737364172935486 test_acc: 0.9166666666666666\n",
      "epoch 465 total_train_acc: 0.9715740347899873 loss: 1.6998678371310234 test_loss: 0.09048562496900558 test_acc: 0.9139072847682119\n",
      "epoch 466 total_train_acc: 0.9719983029274502 loss: 1.7000900618731976 test_loss: 0.1738448292016983 test_acc: 0.9166666666666666\n",
      "epoch 467 total_train_acc: 0.9719983029274502 loss: 1.644583273679018 test_loss: 0.07876039296388626 test_acc: 0.9155629139072847\n",
      "epoch 468 total_train_acc: 0.9719983029274502 loss: 1.6151109412312508 test_loss: 0.21076983213424683 test_acc: 0.9155629139072847\n",
      "epoch 469 total_train_acc: 0.9675434874840899 loss: 1.7499173171818256 test_loss: 0.08891087770462036 test_acc: 0.9172185430463576\n",
      "epoch 470 total_train_acc: 0.9681798896902842 loss: 1.6956513561308384 test_loss: 0.48423337936401367 test_acc: 0.9155629139072847\n",
      "epoch 471 total_train_acc: 0.972422571064913 loss: 1.6350838243961334 test_loss: 0.253834068775177 test_acc: 0.9155629139072847\n",
      "epoch 472 total_train_acc: 0.9726347051336445 loss: 1.6104924380779266 test_loss: 0.16530250012874603 test_acc: 0.9172185430463576\n",
      "epoch 473 total_train_acc: 0.9713619007212558 loss: 1.6069591715931892 test_loss: 0.03387380391359329 test_acc: 0.9161147902869757\n",
      "epoch 474 total_train_acc: 0.9662706830717013 loss: 1.7087171450257301 test_loss: 0.02773992158472538 test_acc: 0.9150110375275938\n",
      "epoch 475 total_train_acc: 0.9688162918964786 loss: 1.6786283776164055 test_loss: 0.9526803493499756 test_acc: 0.9188741721854304\n",
      "epoch 476 total_train_acc: 0.9703012303775986 loss: 1.6648832447826862 test_loss: 0.10258234292268753 test_acc: 0.9166666666666666\n",
      "epoch 477 total_train_acc: 0.9719983029274502 loss: 1.7256853878498077 test_loss: 0.00046610378194600344 test_acc: 0.9172185430463576\n",
      "epoch 478 total_train_acc: 0.9741196436147646 loss: 1.591820389032364 test_loss: 0.22765684127807617 test_acc: 0.9150110375275938\n",
      "epoch 479 total_train_acc: 0.9700890963088672 loss: 1.7452725023031235 test_loss: 0.2859439551830292 test_acc: 0.9155629139072847\n",
      "epoch 480 total_train_acc: 0.9703012303775986 loss: 1.667065855115652 test_loss: 0.0329669751226902 test_acc: 0.9155629139072847\n",
      "epoch 481 total_train_acc: 0.9662706830717013 loss: 1.7463251575827599 test_loss: 0.0016854522982612252 test_acc: 0.9155629139072847\n",
      "epoch 482 total_train_acc: 0.9747560458209589 loss: 1.5529080405831337 test_loss: 0.060619864612817764 test_acc: 0.9183222958057395\n",
      "epoch 483 total_train_acc: 0.9705133644463301 loss: 1.656354233622551 test_loss: 0.0794738158583641 test_acc: 0.9150110375275938\n",
      "epoch 484 total_train_acc: 0.9688162918964786 loss: 1.6839591525495052 test_loss: 0.08882232755422592 test_acc: 0.9155629139072847\n",
      "epoch 485 total_train_acc: 0.9736953754773017 loss: 1.6234513260424137 test_loss: 0.3188185691833496 test_acc: 0.9166666666666666\n",
      "epoch 486 total_train_acc: 0.9719983029274502 loss: 1.650748997926712 test_loss: 0.6896662712097168 test_acc: 0.9155629139072847\n",
      "epoch 487 total_train_acc: 0.970937632583793 loss: 1.7259036228060722 test_loss: 0.16701780259609222 test_acc: 0.9161147902869757\n",
      "epoch 488 total_train_acc: 0.9700890963088672 loss: 1.686114214360714 test_loss: 0.128770649433136 test_acc: 0.9161147902869757\n",
      "epoch 489 total_train_acc: 0.9675434874840899 loss: 1.740568295121193 test_loss: 0.12052130699157715 test_acc: 0.9161147902869757\n",
      "epoch 490 total_train_acc: 0.9707254985150615 loss: 1.612595185637474 test_loss: 0.004124941769987345 test_acc: 0.9144591611479028\n",
      "epoch 491 total_train_acc: 0.9696648281714043 loss: 1.673567783087492 test_loss: 0.3386158049106598 test_acc: 0.9139072847682119\n",
      "epoch 492 total_train_acc: 0.9707254985150615 loss: 1.537849884480238 test_loss: 0.17126141488552094 test_acc: 0.9155629139072847\n",
      "epoch 493 total_train_acc: 0.9736953754773017 loss: 1.5678701885044575 test_loss: 0.6209455728530884 test_acc: 0.9150110375275938\n",
      "epoch 494 total_train_acc: 0.9683920237590157 loss: 1.7521376833319664 test_loss: 0.36829736828804016 test_acc: 0.9133554083885209\n",
      "epoch 495 total_train_acc: 0.9694526941026729 loss: 1.617177825421095 test_loss: 0.3456888198852539 test_acc: 0.9166666666666666\n",
      "epoch 496 total_train_acc: 0.9717861688587187 loss: 1.6130568347871304 test_loss: 0.1190476045012474 test_acc: 0.9172185430463576\n",
      "epoch 497 total_train_acc: 0.9696648281714043 loss: 1.7688531801104546 test_loss: 0.01574995554983616 test_acc: 0.9155629139072847\n",
      "epoch 498 total_train_acc: 0.970937632583793 loss: 1.6482532545924187 test_loss: 0.18349678814411163 test_acc: 0.9172185430463576\n",
      "epoch 499 total_train_acc: 0.9683920237590157 loss: 1.6947511583566666 test_loss: 0.18086028099060059 test_acc: 0.9177704194260485\n",
      "epoch 500 total_train_acc: 0.9711497666525244 loss: 1.6567468494176865 test_loss: 0.3083801567554474 test_acc: 0.9172185430463576\n",
      "epoch 501 total_train_acc: 0.9692405600339414 loss: 1.6983167305588722 test_loss: 0.0067362538538873196 test_acc: 0.9177704194260485\n",
      "epoch 502 total_train_acc: 0.9681798896902842 loss: 1.6967403665184975 test_loss: 0.20609478652477264 test_acc: 0.9161147902869757\n",
      "epoch 503 total_train_acc: 0.9698769622401358 loss: 1.631342001259327 test_loss: 0.16998295485973358 test_acc: 0.9161147902869757\n",
      "epoch 504 total_train_acc: 0.972422571064913 loss: 1.6065480895340443 test_loss: 0.12764199078083038 test_acc: 0.9161147902869757\n",
      "epoch 505 total_train_acc: 0.9688162918964786 loss: 1.6330266706645489 test_loss: 0.069643035531044 test_acc: 0.9172185430463576\n",
      "epoch 506 total_train_acc: 0.9722104369961816 loss: 1.6281523406505585 test_loss: 0.054867908358573914 test_acc: 0.9139072847682119\n",
      "epoch 507 total_train_acc: 0.9662706830717013 loss: 1.7397288233041763 test_loss: 0.37236395478248596 test_acc: 0.9155629139072847\n",
      "epoch 508 total_train_acc: 0.9707254985150615 loss: 1.5733642578125 test_loss: 0.034175582230091095 test_acc: 0.9150110375275938\n",
      "epoch 509 total_train_acc: 0.9696648281714043 loss: 1.6762383170425892 test_loss: 0.06436318904161453 test_acc: 0.9150110375275938\n",
      "epoch 510 total_train_acc: 0.9700890963088672 loss: 1.637845505028963 test_loss: 0.28121113777160645 test_acc: 0.9144591611479028\n",
      "epoch 511 total_train_acc: 0.9707254985150615 loss: 1.6544478870928288 test_loss: 0.08020172268152237 test_acc: 0.9166666666666666\n",
      "epoch 512 total_train_acc: 0.9705133644463301 loss: 1.7114675641059875 test_loss: 0.09832245856523514 test_acc: 0.9161147902869757\n",
      "epoch 513 total_train_acc: 0.9700890963088672 loss: 1.6384006962180138 test_loss: 0.25710123777389526 test_acc: 0.9172185430463576\n",
      "epoch 514 total_train_acc: 0.9705133644463301 loss: 1.6317675150930882 test_loss: 0.11874004453420639 test_acc: 0.9172185430463576\n",
      "epoch 515 total_train_acc: 0.967119219346627 loss: 1.7045120038092136 test_loss: 0.39408090710639954 test_acc: 0.9166666666666666\n",
      "epoch 516 total_train_acc: 0.9703012303775986 loss: 1.6060064509510994 test_loss: 0.03824305161833763 test_acc: 0.9155629139072847\n",
      "epoch 517 total_train_acc: 0.9688162918964786 loss: 1.741434283554554 test_loss: 0.011128620244562626 test_acc: 0.9144591611479028\n",
      "epoch 518 total_train_acc: 0.9728468392023759 loss: 1.6255887560546398 test_loss: 0.07275842875242233 test_acc: 0.9144591611479028\n",
      "epoch 519 total_train_acc: 0.970937632583793 loss: 1.6267198324203491 test_loss: 0.13301941752433777 test_acc: 0.9155629139072847\n",
      "epoch 520 total_train_acc: 0.9688162918964786 loss: 1.628637932240963 test_loss: 0.31138136982917786 test_acc: 0.9150110375275938\n",
      "epoch 521 total_train_acc: 0.9756045820958846 loss: 1.5317698940634727 test_loss: 0.1159103736281395 test_acc: 0.9150110375275938\n",
      "epoch 522 total_train_acc: 0.9711497666525244 loss: 1.5932041741907597 test_loss: 0.14939774572849274 test_acc: 0.9150110375275938\n",
      "epoch 523 total_train_acc: 0.9677556215528214 loss: 1.6835200637578964 test_loss: 0.0018358835950493813 test_acc: 0.9155629139072847\n",
      "epoch 524 total_train_acc: 0.96902842596521 loss: 1.56257988139987 test_loss: 0.14266791939735413 test_acc: 0.9172185430463576\n",
      "epoch 525 total_train_acc: 0.9700890963088672 loss: 1.6535602733492851 test_loss: 0.2286878079175949 test_acc: 0.9172185430463576\n",
      "epoch 526 total_train_acc: 0.9711497666525244 loss: 1.6769536398351192 test_loss: 0.07141318172216415 test_acc: 0.9161147902869757\n",
      "epoch 527 total_train_acc: 0.9726347051336445 loss: 1.614364668726921 test_loss: 0.026413097977638245 test_acc: 0.9144591611479028\n",
      "epoch 528 total_train_acc: 0.9692405600339414 loss: 1.6524452678859234 test_loss: 0.00148776697460562 test_acc: 0.9161147902869757\n",
      "epoch 529 total_train_acc: 0.9728468392023759 loss: 1.6098173037171364 test_loss: 0.14532369375228882 test_acc: 0.9161147902869757\n",
      "epoch 530 total_train_acc: 0.970937632583793 loss: 1.6162464283406734 test_loss: 0.05876212939620018 test_acc: 0.9161147902869757\n",
      "epoch 531 total_train_acc: 0.9717861688587187 loss: 1.6483063735067844 test_loss: 0.04704020917415619 test_acc: 0.9166666666666666\n",
      "epoch 532 total_train_acc: 0.9675434874840899 loss: 1.7182591147720814 test_loss: 0.1089855208992958 test_acc: 0.9155629139072847\n",
      "epoch 533 total_train_acc: 0.9694526941026729 loss: 1.660516880452633 test_loss: 0.3123449385166168 test_acc: 0.9155629139072847\n",
      "epoch 534 total_train_acc: 0.9717861688587187 loss: 1.6433539055287838 test_loss: 0.012287899851799011 test_acc: 0.9150110375275938\n",
      "epoch 535 total_train_acc: 0.9745439117522274 loss: 1.5468560084700584 test_loss: 0.2231731414794922 test_acc: 0.9155629139072847\n",
      "epoch 536 total_train_acc: 0.9700890963088672 loss: 1.6093849837779999 test_loss: 0.20078320801258087 test_acc: 0.9155629139072847\n",
      "epoch 537 total_train_acc: 0.9736953754773017 loss: 1.5606307908892632 test_loss: 0.14560218155384064 test_acc: 0.9166666666666666\n",
      "epoch 538 total_train_acc: 0.9675434874840899 loss: 1.78553307056427 test_loss: 0.5211684703826904 test_acc: 0.9166666666666666\n",
      "epoch 539 total_train_acc: 0.972422571064913 loss: 1.5419286526739597 test_loss: 0.4955410659313202 test_acc: 0.9172185430463576\n",
      "epoch 540 total_train_acc: 0.9726347051336445 loss: 1.6413482874631882 test_loss: 0.3364996612071991 test_acc: 0.9166666666666666\n",
      "epoch 541 total_train_acc: 0.9713619007212558 loss: 1.606290329247713 test_loss: 0.007817082107067108 test_acc: 0.9155629139072847\n",
      "epoch 542 total_train_acc: 0.9686041578277471 loss: 1.679774846881628 test_loss: 0.12307065725326538 test_acc: 0.9166666666666666\n",
      "epoch 543 total_train_acc: 0.9675434874840899 loss: 1.7789427787065506 test_loss: 0.15959174931049347 test_acc: 0.9150110375275938\n",
      "epoch 544 total_train_acc: 0.9696648281714043 loss: 1.7308939918875694 test_loss: 0.2424510270357132 test_acc: 0.9161147902869757\n",
      "epoch 545 total_train_acc: 0.9705133644463301 loss: 1.7122937552630901 test_loss: 0.09520483762025833 test_acc: 0.9161147902869757\n",
      "epoch 546 total_train_acc: 0.9692405600339414 loss: 1.6617245897650719 test_loss: 0.5558509826660156 test_acc: 0.9161147902869757\n",
      "epoch 547 total_train_acc: 0.9722104369961816 loss: 1.6450576521456242 test_loss: 0.07286961376667023 test_acc: 0.9155629139072847\n",
      "epoch 548 total_train_acc: 0.9705133644463301 loss: 1.6539455838501453 test_loss: 0.1371363401412964 test_acc: 0.9155629139072847\n",
      "epoch 549 total_train_acc: 0.9741196436147646 loss: 1.5902160629630089 test_loss: 0.07564464211463928 test_acc: 0.9150110375275938\n",
      "epoch 550 total_train_acc: 0.9707254985150615 loss: 1.6918106749653816 test_loss: 0.011589373461902142 test_acc: 0.9155629139072847\n",
      "epoch 551 total_train_acc: 0.9707254985150615 loss: 1.6319593116641045 test_loss: 0.02119511365890503 test_acc: 0.9161147902869757\n",
      "epoch 552 total_train_acc: 0.972422571064913 loss: 1.5999905206263065 test_loss: 0.13966259360313416 test_acc: 0.9155629139072847\n",
      "epoch 553 total_train_acc: 0.9681798896902842 loss: 1.6918426230549812 test_loss: 0.4988870620727539 test_acc: 0.9150110375275938\n",
      "epoch 554 total_train_acc: 0.9715740347899873 loss: 1.6127759851515293 test_loss: 0.335236519575119 test_acc: 0.9155629139072847\n",
      "epoch 555 total_train_acc: 0.970937632583793 loss: 1.6242310255765915 test_loss: 0.18958216905593872 test_acc: 0.9155629139072847\n",
      "epoch 556 total_train_acc: 0.9732711073398388 loss: 1.5168650299310684 test_loss: 0.003436199389398098 test_acc: 0.9144591611479028\n",
      "epoch 557 total_train_acc: 0.9722104369961816 loss: 1.6366068832576275 test_loss: 0.2440415471792221 test_acc: 0.9150110375275938\n",
      "epoch 558 total_train_acc: 0.9715740347899873 loss: 1.5956214033067226 test_loss: 0.024199962615966797 test_acc: 0.9150110375275938\n",
      "epoch 559 total_train_acc: 0.9713619007212558 loss: 1.6373231671750546 test_loss: 0.0012904361356049776 test_acc: 0.9150110375275938\n",
      "epoch 560 total_train_acc: 0.9692405600339414 loss: 1.6353410966694355 test_loss: 0.21409986913204193 test_acc: 0.9161147902869757\n",
      "epoch 561 total_train_acc: 0.9758167161646161 loss: 1.5799650251865387 test_loss: 0.08177969604730606 test_acc: 0.9161147902869757\n",
      "epoch 562 total_train_acc: 0.9715740347899873 loss: 1.6607678048312664 test_loss: 0.007068282458931208 test_acc: 0.9155629139072847\n",
      "epoch 563 total_train_acc: 0.9696648281714043 loss: 1.6228350065648556 test_loss: 0.09776613861322403 test_acc: 0.9161147902869757\n",
      "epoch 564 total_train_acc: 0.9728468392023759 loss: 1.5370182283222675 test_loss: 0.04072022810578346 test_acc: 0.9166666666666666\n",
      "epoch 565 total_train_acc: 0.9726347051336445 loss: 1.7365306094288826 test_loss: 0.13195885717868805 test_acc: 0.9161147902869757\n",
      "epoch 566 total_train_acc: 0.9700890963088672 loss: 1.6692690998315811 test_loss: 0.036726102232933044 test_acc: 0.9161147902869757\n",
      "epoch 567 total_train_acc: 0.9692405600339414 loss: 1.6520405188202858 test_loss: 0.043165307492017746 test_acc: 0.9161147902869757\n",
      "epoch 568 total_train_acc: 0.9736953754773017 loss: 1.5945082157850266 test_loss: 0.07631201297044754 test_acc: 0.9177704194260485\n",
      "epoch 569 total_train_acc: 0.9705133644463301 loss: 1.6648738160729408 test_loss: 0.02106405980885029 test_acc: 0.9172185430463576\n",
      "epoch 570 total_train_acc: 0.9728468392023759 loss: 1.676569603383541 test_loss: 0.18719454109668732 test_acc: 0.9161147902869757\n",
      "epoch 571 total_train_acc: 0.9688162918964786 loss: 1.5846266150474548 test_loss: 0.017663361504673958 test_acc: 0.9161147902869757\n",
      "epoch 572 total_train_acc: 0.9719983029274502 loss: 1.69667087495327 test_loss: 0.5793754458427429 test_acc: 0.9183222958057395\n",
      "epoch 573 total_train_acc: 0.9705133644463301 loss: 1.6508514881134033 test_loss: 0.005159867461770773 test_acc: 0.9177704194260485\n",
      "epoch 574 total_train_acc: 0.9713619007212558 loss: 1.6332867294549942 test_loss: 0.5429344177246094 test_acc: 0.9172185430463576\n",
      "epoch 575 total_train_acc: 0.972422571064913 loss: 1.57976596057415 test_loss: 0.044682037085294724 test_acc: 0.9183222958057395\n",
      "epoch 576 total_train_acc: 0.9694526941026729 loss: 1.677794586867094 test_loss: 0.2925894558429718 test_acc: 0.9166666666666666\n",
      "epoch 577 total_train_acc: 0.9800593975392448 loss: 1.425105158239603 test_loss: 0.09371498972177505 test_acc: 0.9155629139072847\n",
      "epoch 578 total_train_acc: 0.9719983029274502 loss: 1.6110065504908562 test_loss: 0.12882405519485474 test_acc: 0.9150110375275938\n",
      "epoch 579 total_train_acc: 0.9703012303775986 loss: 1.62748671323061 test_loss: 0.2258649468421936 test_acc: 0.9161147902869757\n",
      "epoch 580 total_train_acc: 0.9705133644463301 loss: 1.6063486263155937 test_loss: 0.2330266237258911 test_acc: 0.9161147902869757\n",
      "epoch 581 total_train_acc: 0.9696648281714043 loss: 1.6216649040579796 test_loss: 0.09676382690668106 test_acc: 0.9155629139072847\n",
      "epoch 582 total_train_acc: 0.9730589732711074 loss: 1.5814207419753075 test_loss: 0.32265469431877136 test_acc: 0.9155629139072847\n",
      "epoch 583 total_train_acc: 0.972422571064913 loss: 1.6457404717803001 test_loss: 0.1194174587726593 test_acc: 0.9144591611479028\n",
      "epoch 584 total_train_acc: 0.974331777683496 loss: 1.5565352067351341 test_loss: 0.04507638141512871 test_acc: 0.9166666666666666\n",
      "epoch 585 total_train_acc: 0.9707254985150615 loss: 1.6487732417881489 test_loss: 0.10557141155004501 test_acc: 0.9150110375275938\n",
      "epoch 586 total_train_acc: 0.9715740347899873 loss: 1.6201098524034023 test_loss: 0.3194645345211029 test_acc: 0.9155629139072847\n",
      "epoch 587 total_train_acc: 0.9686041578277471 loss: 1.6993585526943207 test_loss: 0.00021808540623169392 test_acc: 0.9155629139072847\n",
      "epoch 588 total_train_acc: 0.9711497666525244 loss: 1.5681465603411198 test_loss: 0.13676734268665314 test_acc: 0.9144591611479028\n",
      "epoch 589 total_train_acc: 0.9730589732711074 loss: 1.5833844058215618 test_loss: 0.498122900724411 test_acc: 0.9155629139072847\n",
      "epoch 590 total_train_acc: 0.9705133644463301 loss: 1.6495508924126625 test_loss: 0.2452635020017624 test_acc: 0.9155629139072847\n",
      "epoch 591 total_train_acc: 0.9719983029274502 loss: 1.672977328300476 test_loss: 0.0704762265086174 test_acc: 0.9155629139072847\n",
      "epoch 592 total_train_acc: 0.970937632583793 loss: 1.5733904354274273 test_loss: 0.33354952931404114 test_acc: 0.9155629139072847\n",
      "epoch 593 total_train_acc: 0.9734832414085702 loss: 1.6138349436223507 test_loss: 0.08170830458402634 test_acc: 0.9161147902869757\n",
      "epoch 594 total_train_acc: 0.9734832414085702 loss: 1.5793102160096169 test_loss: 0.3289443254470825 test_acc: 0.9172185430463576\n",
      "epoch 595 total_train_acc: 0.9747560458209589 loss: 1.573914259672165 test_loss: 0.4200003147125244 test_acc: 0.9161147902869757\n",
      "epoch 596 total_train_acc: 0.9700890963088672 loss: 1.6480047516524792 test_loss: 0.06947901099920273 test_acc: 0.9150110375275938\n",
      "epoch 597 total_train_acc: 0.9658464149342384 loss: 1.8012740239501 test_loss: 0.17404060065746307 test_acc: 0.9144591611479028\n",
      "epoch 598 total_train_acc: 0.9745439117522274 loss: 1.5503648109734058 test_loss: 0.09276115149259567 test_acc: 0.9144591611479028\n",
      "epoch 599 total_train_acc: 0.9717861688587187 loss: 1.6604118309915066 test_loss: 0.32187482714653015 test_acc: 0.9150110375275938\n",
      "epoch 600 total_train_acc: 0.9719983029274502 loss: 1.5948954299092293 test_loss: 0.244828999042511 test_acc: 0.9155629139072847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WinError 10054] An existing connection was forcibly closed by the remote host\n",
      "[WinError 10054] An existing connection was forcibly closed by the remote host\n",
      "[WinError 10054] An existing connection was forcibly closed by the remote host\n",
      "on_close() takes 1 positional argument but 3 were given\n",
      "on_close() takes 1 positional argument but 3 were given\n",
      "on_close() takes 1 positional argument but 3 were given\n"
     ]
    }
   ],
   "source": [
    "# CNN 训练\n",
    "# 损失\n",
    "# criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "# 加载数据，设置优化器\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=256, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=100, shuffle=True)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.0002)\n",
    "lr_schedule = torch.optim.lr_scheduler.StepLR(optimizer, 100, gamma=0.5, last_epoch=-1)\n",
    "# 初始化 visdom\n",
    "viz.close()\n",
    "viz = viz_init()\n",
    "if not os.path.exists(vislogDir):\n",
    "    os.makedirs(vislogDir)\n",
    "viz = Visdom(\n",
    "    env=viz_acnt.evns, log_to_filename=vislogDir + \"vislog_\" + get_current_time()\n",
    ")\n",
    "vizx = 0\n",
    "viz.text(\n",
    "    \"MONITOR: Show train process~~\",\n",
    "    win=\"Monitor\",\n",
    "    opts={\n",
    "        \"title\": \"ProcessMonitor\",\n",
    "    },\n",
    ")\n",
    "\n",
    "total_test_acc = 0\n",
    "total_test_correct = 0\n",
    "totaltest = 0\n",
    "# 训练过程\n",
    "epoch_num = 600\n",
    "net.to(device)\n",
    "for epoch in range(epoch_num):\n",
    "    viz.text(\n",
    "        \"ep\" + str(epoch + 1) + \" start\",\n",
    "        win=\"Monitor\",\n",
    "        opts={\n",
    "            \"title\": \"ProcessMonitor\",\n",
    "        },\n",
    "    )\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    curr_total_correct = 0\n",
    "    total_traintnum = 0\n",
    "    for batch in train_loader:\n",
    "        # 载入本批次数据\n",
    "        batch = batch\n",
    "        images, labels = batch\n",
    "        images = images.to(torch.float32).to(device)\n",
    "        labels = labels.long().to(device)\n",
    "        net.train()\n",
    "        preds = net(images)\n",
    "        trainloss = F.cross_entropy(preds, labels)  # 真实数据的鉴别器损失\n",
    "        optimizer.zero_grad()\n",
    "        trainloss.backward()  # Calculate Gradients\n",
    "        optimizer.step()  # Update Weight\n",
    "        total_loss += trainloss.item()\n",
    "        curr_total_correct = get_num_correct(preds, labels)\n",
    "        total_correct += curr_total_correct\n",
    "        total_traintnum += labels.size(0)\n",
    "    total_train_acc = total_correct / total_traintnum\n",
    "    total_correct = 0\n",
    "    # 测试\n",
    "    net.eval()\n",
    "    total_testnum = 0\n",
    "    for testemgdatas, testemglabels in test_loader:  # Get Batch\n",
    "        testemgdatas = testemgdatas.to(torch.float32).to(device)\n",
    "        testemglabels = testemglabels.long().to(device)\n",
    "        predstest = net(testemgdatas)\n",
    "        testloss = F.cross_entropy(\n",
    "            predstest, testemglabels\n",
    "        )  # Calculate Loss\n",
    "        curr_test_correct = get_num_correct(\n",
    "            predstest, testemglabels\n",
    "        )\n",
    "        total_testnum += testemglabels.size(0)\n",
    "        total_test_correct += curr_test_correct\n",
    "        # totaltest += testemglabels.size(0)\n",
    "    # total_test_acc = total_test_correct/(trainlabel.size)\n",
    "    total_test_acc = total_test_correct / total_testnum\n",
    "    print(\n",
    "        \"epoch\",\n",
    "        epoch+1,\n",
    "        \"total_train_acc:\",\n",
    "        total_train_acc,\n",
    "        \"loss:\",\n",
    "        total_loss,\n",
    "        \"test_loss:\",\n",
    "        float(testloss),\n",
    "        \"test_acc:\",\n",
    "        total_test_acc,\n",
    "    )\n",
    "    total_test_correct = 0\n",
    "    # 可视化，每 epoch 更新\n",
    "    viz.line(\n",
    "        [float(trainloss)],\n",
    "        [epoch],\n",
    "        win=\"loss_perEpoch\",\n",
    "        name=\"train_loss\",\n",
    "        update=\"append\",\n",
    "        opts=dict(title=\"loss_perEpoch\", xlabel=\"epoch\", ylabel=\"loss\"),\n",
    "    )\n",
    "    viz.line(\n",
    "        [float(testloss)], [epoch], win=\"loss_perEpoch\", name=\"D_loss\", update=\"append\"\n",
    "    )\n",
    "    viz.line(\n",
    "        [float(total_train_acc)],\n",
    "        [epoch],\n",
    "        win=\"acc_perEpoch\",\n",
    "        name=\"train_acc\",\n",
    "        update=\"append\",\n",
    "        opts=dict(title=\"acc_perEpoch\", xlabel=\"epoch\", ylabel=\"acc\"),\n",
    "    )\n",
    "    viz.line(\n",
    "        [float(total_test_acc)],\n",
    "        [epoch],\n",
    "        win=\"acc_perEpoch\",\n",
    "        name=\"test_acc\",\n",
    "        update=\"append\",\n",
    "    )\n",
    "\n",
    "    viz.line(\n",
    "        [float(optimizer.state_dict()[\"param_groups\"][0][\"lr\"])],\n",
    "        [epoch],\n",
    "        win=\"lr_perEpoch\",\n",
    "        name=\"lr\",\n",
    "        update=\"append\",\n",
    "        opts=dict(title=\"lr_perEpoch\", xlabel=\"epoch\", ylabel=\"lr\"),\n",
    "    )\n",
    "    # 更新学习率\n",
    "    lr_schedule.step()\n",
    "    # viz.text('updating weights',win='Monitor',append=True,opts = {'title':'ProcessMonitor',},)\n",
    "    # 定期保存\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        timeForSave = datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "        checkpointPath = (\n",
    "            ckpDir\n",
    "            + \"c_ep_\"\n",
    "            + str(epoch + 1)\n",
    "            + \"_acc\"\n",
    "            + str(int(total_test_acc * 10000))\n",
    "            + \"_\"\n",
    "            + timeForSave\n",
    "            + \".pth\"\n",
    "        )\n",
    "        c_state = {\n",
    "            \"model\": net.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "            \"epoch\": epoch,\n",
    "        }\n",
    "        torch.save(c_state, checkpointPath)\n",
    "        viz.text(\n",
    "            \"epoch \" + str(epoch + 1) + \" model saved\",\n",
    "            win=\"Monitor\",\n",
    "            append=True,\n",
    "            opts={\n",
    "                \"title\": \"ProcessMonitor\",\n",
    "            },\n",
    "        )\n",
    "\n",
    "\n",
    "checkpointPath_model = (\n",
    "    model_Dir\n",
    "    + \"c_final_\"\n",
    "    + \"acc\"\n",
    "    + str(int(total_test_acc * 10000))\n",
    "    + \"_\"\n",
    "    + timeForSave\n",
    "    + \".pth\"\n",
    ")\n",
    "torch.save(net.state_dict(), checkpointPath_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "           Flatten-1                     [1, 4]               0\n",
      "            Linear-2                    [1, 64]             320\n",
      "       BatchNorm1d-3                    [1, 64]             128\n",
      "              ReLU-4                    [1, 64]               0\n",
      "         Dropout2d-5                    [1, 64]               0\n",
      "            Linear-6                   [1, 128]           8,320\n",
      "       BatchNorm1d-7                   [1, 128]             256\n",
      "              ReLU-8                   [1, 128]               0\n",
      "         Dropout2d-9                   [1, 128]               0\n",
      "           Linear-10                    [1, 10]           1,290\n",
      "================================================================\n",
      "Total params: 10,314\n",
      "Trainable params: 10,314\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.01\n",
      "Params size (MB): 0.04\n",
      "Estimated Total Size (MB): 0.05\n",
      "----------------------------------------------------------------\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "           Flatten-1                    [1, 10]               0\n",
      "            Linear-2                   [1, 128]           1,408\n",
      "       BatchNorm1d-3                   [1, 128]             256\n",
      "         LeakyReLU-4                   [1, 128]               0\n",
      "         Dropout2d-5                   [1, 128]               0\n",
      "            Linear-6                    [1, 64]           8,256\n",
      "       BatchNorm1d-7                    [1, 64]             128\n",
      "         LeakyReLU-8                    [1, 64]               0\n",
      "         Dropout2d-9                    [1, 64]               0\n",
      "           Linear-10                     [1, 1]              65\n",
      "================================================================\n",
      "Total params: 10,113\n",
      "Trainable params: 10,113\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.01\n",
      "Params size (MB): 0.04\n",
      "Estimated Total Size (MB): 0.04\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Flatten(start_dim=1, end_dim=-1)\n",
       "  (1): Linear(in_features=10, out_features=128, bias=True)\n",
       "  (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (3): LeakyReLU(negative_slope=0.2)\n",
       "  (4): Dropout2d(p=0.2, inplace=False)\n",
       "  (5): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (6): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (7): LeakyReLU(negative_slope=0.2)\n",
       "  (8): Dropout2d(p=0.2, inplace=False)\n",
       "  (9): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# EMG需要转化成6维特征，用到刚刚训练的模型c_ep_200_acc9763_2022_02_26_23_18_44,\n",
    "# 并重组为新数据集OpenganDatafea_smr_10cl_220227.npy，步骤见 emgDataprocess.ipynb\n",
    "\n",
    "# opGAN部分开始\n",
    "# GAN 模型搭建\n",
    "# 潜在张量大小,32，4是因为 cnn feature 只有6个，不好比它大\n",
    "latent_size = 4\n",
    "# 输出通道数\n",
    "n_channel = 10\n",
    "# 生成网络隐藏层大小\n",
    "n_g_feature = 64\n",
    "\n",
    "gnet = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(latent_size, n_g_feature * 1),  # 用线性变换将输入映射到64维\n",
    "    nn.BatchNorm1d(n_g_feature * 1),\n",
    "    nn.ReLU(True),  # relu激活\n",
    "    nn.Dropout2d(0.2),\n",
    "    nn.Linear(n_g_feature * 1, n_g_feature * 2),  # 线性变换\n",
    "    nn.BatchNorm1d(n_g_feature * 2),\n",
    "    nn.ReLU(True),  # relu激活\n",
    "    nn.Dropout2d(0.2),\n",
    "    nn.Linear(n_g_feature * 2, n_channel),  # 线性变换\n",
    ").to(device)\n",
    "# print(gnet)\n",
    "summary(gnet, (1, 1, 4), batch_size=1, device=\"cuda\")\n",
    "# 鉴别网络隐藏层大小\n",
    "# 32,\n",
    "n_d_feature = 64\n",
    "dnet = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(n_channel, n_d_feature * 2),  # 输入特征数为784，输出为256\n",
    "    nn.BatchNorm1d(n_d_feature * 2),\n",
    "    nn.LeakyReLU(0.2),  # 进行非线性映射\n",
    "    nn.Dropout2d(0.2),\n",
    "    nn.Linear(n_d_feature * 2, n_d_feature),  # 进行一个线性映射\n",
    "    nn.BatchNorm1d(n_d_feature),\n",
    "    nn.LeakyReLU(0.2),\n",
    "    nn.Dropout2d(0.2),\n",
    "    nn.Linear(n_d_feature, 1),\n",
    "    # nn.Sigmoid()  # 也是一个激活函数，二分类问题中，\n",
    "    # sigmoid可以班实数映射到【0,1】，作为概率值，\n",
    "    # 多分类用softmax函数\n",
    ").to(device)\n",
    "# print(dnet)\n",
    "summary(dnet, (1, 1, 10), batch_size=1, device=\"cuda\")\n",
    "# 初始化权重\n",
    "def weights_init(m):\n",
    "    if type(m) in [nn.ConvTranspose2d, nn.Conv2d]:\n",
    "        init.xavier_normal_(m.weight)\n",
    "    elif type(m) == nn.BatchNorm2d:\n",
    "        init.normal_(m.weight, 1.0, 0.02)\n",
    "        init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "gnet.apply(weights_init)\n",
    "dnet.apply(weights_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAN 数据加载，构建数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "(tensor([ 24.3551, -16.3104,   0.2861,  10.1533, -11.1485,  -1.0131,   0.6599,\n",
      "        -21.4095, -38.7771, -20.4391]), 1)\n"
     ]
    }
   ],
   "source": [
    "# 加载 openGAN 所需数据，这里只有训练、验证两部分，没有测试集，因为本次数据采集划分时只划出两部分，不影响最终结果\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),  # 将图片转换为Tensor,归一化至[0,1]\n",
    "    ]\n",
    ")\n",
    "GAN_dataarray = np.load(\"../../data/nina_db1/Fopg_s2_multiset_20220611_vec.npy\", allow_pickle=True)\n",
    "GANdataset = GAN_dataarray.item()\n",
    "print(type(GANdataset))\n",
    "traindata = GANdataset[\"ktr_vec_X\"]\n",
    "trainlabel = GANdataset[\"ktr_Y\"]\n",
    "# 本方法中的 test 实际上是个大号的完整版openset,所以标签是01\n",
    "testdata = GANdataset['dte_vec_X_30c']\n",
    "testlabel = GANdataset['dte_vec_Y_30c']\n",
    "valdata = GANdataset[\"val_vec_X_30c\"]\n",
    "vallabel = GANdataset[\"val_vec_Y_30c\"]\n",
    "vallabel_for_auc = GANdataset[\"val_vec_Y_30c\"]\n",
    "vallabel_for_auc = vallabel_for_auc.ravel()\n",
    "# 注意这里 ktr，kte 的标签需要降维，其他的在数据制作阶段已经降过了\n",
    "trainlabel = trainlabel[:, 0]\n",
    "# testlabel = testlabel[:,0]\n",
    "# vallabel = vallabel[:, 0]\n",
    "# trainunknownc_label = trainunknownc_label[:,0]\n",
    "# print(type(trainlabel))\n",
    "train_set = EMGDataset_1D(traindata, trainlabel)\n",
    "test_set = EMGDataset_1D(testdata, testlabel)\n",
    "val_set = EMGDataset_1D(valdata, vallabel)\n",
    "# train_unknown = EMGDataset(trainunknown_data,trainunknownc_label)\n",
    "# train_loader = torch.utils.data.DataLoader(train_set, batch_size=1, shuffle=True, pin_memory=True,\n",
    "#                                             num_workers=3)\n",
    "\n",
    "sample = next(iter(val_set))\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练 GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n",
      "Setting up a new session...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "visdom has started\n",
      "[1/2000]D loss:1.41995 G loss:0.902771真判真:0.495409 假判真:0.485809/0.570376\n",
      "[1/2000]D loss:2.84641 G loss:7.90166真判真:0.638448 假判真:0.586823/0.197729\n",
      "[1/2000]D loss:8.92267 G loss:0.924056真判真:0.226833 假判真:0.204401/0.700598\n",
      "[1/2000]D loss:3.31932 G loss:1.33657真判真:0.657051 假判真:0.645898/0.419815\n",
      "[1/2000]D loss:2.45784 G loss:0.668708真判真:0.450624 假判真:0.4435/0.527007\n",
      "[1/2000]D loss:1.4151 G loss:0.666167真判真:0.534685 假判真:0.514262/0.52723\n",
      "[1/2000]D loss:1.44522 G loss:0.793077真判真:0.53036 假判真:0.527123/0.458778\n",
      "[1/2000]D loss:1.40056 G loss:0.626198真判真:0.457295 假判真:0.449341/0.538521\n",
      "[1/2000]D loss:1.42945 G loss:0.746564真判真:0.541103 假判真:0.549374/0.477142\n",
      "[1/2000]D loss:1.37835 G loss:0.680968真判真:0.476847 假判真:0.465002/0.508934\n",
      "[1/2000]D loss:1.39647 G loss:0.737683真判真:0.509062 假判真:0.508046/0.484575\n",
      "[1/2000]D loss:1.3853 G loss:0.64182真判真:0.492404 假判真:0.484072/0.529172\n",
      "[1/2000]D loss:1.38839 G loss:0.83351真判真:0.535395 假判真:0.52954/0.437455\n",
      "[1/2000]D loss:1.4004 G loss:0.525545真判真:0.443414 假判真:0.438984/0.592873\n",
      "[1/2000]D loss:1.41112 G loss:1.02454真判真:0.604428 假判真:0.593155/0.361515\n",
      "[1/2000]D loss:1.4387 G loss:0.432116真判真:0.380859 假判真:0.36851/0.65172\n",
      "[1/2000]D loss:1.49062 G loss:1.09065真判真:0.662718 假判真:0.652925/0.340277\n",
      "[1/2000]D loss:1.46444 G loss:0.494512真判真:0.352865 假判真:0.336945/0.613629\n",
      "[1/2000]D loss:1.45359 G loss:0.797722真判真:0.625012 假判真:0.617708/0.453628\n",
      "[1/2000]D loss:1.38767 G loss:0.734352真判真:0.463939 假判真:0.455573/0.481801\n",
      "[1/2000]D loss:1.40301 G loss:0.645754真判真:0.486139 假判真:0.488252/0.52567\n",
      "[1/2000]D loss:1.37904 G loss:0.720098真判真:0.531991 假判真:0.523411/0.488213\n",
      "[1/2000]D loss:1.37851 G loss:0.70583真判真:0.495369 假判真:0.487712/0.495698\n",
      "[1/2000]D loss:1.38465 G loss:0.668194真判真:0.496323 假判真:0.492382/0.515451\n",
      "[1/2000]D loss:1.37034 G loss:0.710366真判真:0.520391 假判真:0.507952/0.494595\n",
      "[1/2000]D loss:1.38528 G loss:0.728417真判真:0.504465 假判真:0.498126/0.486912\n",
      "[1/2000]D loss:1.38282 G loss:0.694567真判真:0.500345 假判真:0.490261/0.503466\n",
      "[1/2000]D loss:1.37645 G loss:0.703022真判真:0.516085 假判真:0.499417/0.499356\n",
      "[1/2000]D loss:1.39908 G loss:0.732159真判真:0.502715 假判真:0.501907/0.483695\n",
      "[1/2000]D loss:1.39614 G loss:0.635389真判真:0.487303 假判真:0.486721/0.531377\n",
      "[1/2000]D loss:1.39336 G loss:0.741184真判真:0.526952 假判真:0.52471/0.479\n",
      "[1/2000]D loss:1.39317 G loss:0.713797真判真:0.487285 假判真:0.486802/0.491448\n",
      "[1/2000]D loss:1.38499 G loss:0.682607真判真:0.494909 假判真:0.491379/0.506829\n",
      "[1/2000]D loss:1.38061 G loss:0.71508真判真:0.514374 假判真:0.507817/0.492301\n",
      "[1/2000]D loss:1.38629 G loss:0.749441真判真:0.49861 假判真:0.494613/0.475701\n",
      "[1/2000]D loss:1.382 G loss:0.634002真判真:0.4864 假判真:0.475519/0.532717\n",
      "[1/2000]D loss:1.37183 G loss:0.808611真判真:0.54682 假判真:0.52915/0.448817\n"
     ]
    },
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-516e3989a330>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[0mdnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m     \u001b[0mdnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cpu\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 202\u001b[1;33m     \u001b[0memg_vec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0memgdata_to_net_preds_sigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_set\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnet_vector\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdnet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    203\u001b[0m     \u001b[0memg_vec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0memg_vec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m     \u001b[0mauc_current\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mroc_auc_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvallabel_for_auc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0memg_vec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"micro\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\cwdbo\\OneDrive\\桌面\\theTEMP\\技术\\torchlearn\\src\\code_opg\\..\\utils\\reuse.py\u001b[0m in \u001b[0;36memgdata_to_net_preds_sigmoid\u001b[1;34m(data_set, net_vector)\u001b[0m\n\u001b[0;32m     41\u001b[0m     ]\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0memg_vec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m         \u001b[0msample_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_label\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatchl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m         \u001b[0msample_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msample_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[0msample_label\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_label\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# GAN 训练,120\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=128)\n",
    "# 损失\n",
    "criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "# criterion = F.cross_entropy()\n",
    "\n",
    "# 优化器\n",
    "goptimizer = torch.optim.Adam(gnet.parameters(), lr=0.0008 * 1.5, betas=(0.5, 0.999))\n",
    "doptimizer = torch.optim.Adam(dnet.parameters(), lr=0.4 * 1, betas=(0.5, 0.999))\n",
    "g_lr_schedule = torch.optim.lr_scheduler.StepLR(\n",
    "    goptimizer, 1000, gamma=0.95, last_epoch=-1\n",
    ")\n",
    "d_lr_schedule = torch.optim.lr_scheduler.StepLR(\n",
    "    doptimizer, 1000, gamma=0.95, last_epoch=-1\n",
    ")\n",
    "# 用于测试的固定噪声,用来查看相同的潜在张量在训练过程中生成图片的变换\n",
    "# batch_size = 512\n",
    "# fixed_noises = torch.randn(batch_size, latent_size, 1, 1).to(device)\n",
    "# 初始化 visdom\n",
    "viz.close()\n",
    "viz = viz_init()\n",
    "viz = Visdom(\n",
    "    env=viz_acnt.evns, log_to_filename=vislogDir + \"vislog_\" + get_current_time()\n",
    ")\n",
    "vizx = 0\n",
    "viz.text(\n",
    "    \"MONITOR: Show train process~~\",\n",
    "    win=\"Monitor\",\n",
    "    opts={\n",
    "        \"title\": \"ProcessMonitor\",\n",
    "    },\n",
    ")\n",
    "# AUC值\n",
    "auc_current = 0\n",
    "auc_max = 0\n",
    "# 训练过程\n",
    "epoch_num = 2000\n",
    "for epoch in range(epoch_num):\n",
    "    viz.text(\n",
    "        \"ep\" + str(epoch + 1) + \" start\",\n",
    "        win=\"Monitor\",\n",
    "        opts={\n",
    "            \"title\": \"ProcessMonitor\",\n",
    "        },\n",
    "    )\n",
    "    dnet.to(device)\n",
    "    for batch in train_loader:\n",
    "        # 载入本批次数据\n",
    "        # real_images, _ = data\n",
    "        # viz.text('load data',win='Monitor',append=True,opts = {'title':'ProcessMonitor',},)\n",
    "        rimages, rlabels = batch\n",
    "        rimages = rimages.to(device)\n",
    "        rlabels = rlabels.to(device)\n",
    "        rimages = rimages.to(torch.float32)\n",
    "        rlabels = rlabels.long()\n",
    "        batch_size = rimages.size(0)\n",
    "        # batch_size = 1344\n",
    "        # viz.text('training',win='Monitor',append=True,opts = {'title':'ProcessMonitor',},)\n",
    "        # 训练鉴别网络\n",
    "        dnet.train()\n",
    "        gnet.train()\n",
    "        labels = torch.ones(batch_size)  # 真实数据对应标签为1\n",
    "        labels = labels.to(device)\n",
    "        preds = dnet(rimages)  # 对真实数据进行判别\n",
    "        preds = preds.reshape(-1)\n",
    "\n",
    "        dloss_real = criterion(preds, labels)  # 真实数据的鉴别器损失\n",
    "        dmean_real = preds.sigmoid().mean()\n",
    "        # 计算鉴别器将多少比例的真数据判定为真,仅用于输出显示\n",
    "\n",
    "        noises = torch.randn(batch_size, latent_size, 1, 1)  # 潜在噪声\n",
    "        noises = noises.to(device)\n",
    "        fake_images = gnet(noises)  # 生成假数据\n",
    "        labels = torch.zeros(batch_size)  # 假数据对应标签为0\n",
    "        labels = labels.to(device)\n",
    "        fake = fake_images.detach()\n",
    "        # 使得梯度的计算不回溯到生成网络,可用于加快训练速度.删去此步结果不变\n",
    "        preds = dnet(fake)  # 对假数据进行鉴别\n",
    "        preds = preds.view(-1)\n",
    "        dloss_fake = criterion(preds, labels)  # 假数据的鉴别器损失\n",
    "        dmean_fake = preds.sigmoid().mean()\n",
    "        # 计算鉴别器将多少比例的假数据判定为真,仅用于输出显示\n",
    "\n",
    "        dloss = dloss_real + dloss_fake  # 总的鉴别器损失\n",
    "        dnet.zero_grad()\n",
    "        dloss.backward()\n",
    "        doptimizer.step()\n",
    "        # now_dloss += dloss.item()\n",
    "        # 训练生成网络\n",
    "        # viz.text(' Generator training',win='Monitor',append=True,opts = {'title':'ProcessMonitor',},)\n",
    "        labels = torch.ones(batch_size)\n",
    "        labels = labels.to(device)\n",
    "        # 生成网络希望所有生成的数据都被认为是真数据\n",
    "        preds = dnet(fake_images)  # 把假数据通过鉴别网络\n",
    "        preds = preds.view(-1)\n",
    "        gloss = criterion(preds, labels)  # 真数据看到的损失\n",
    "        gmean_fake = preds.sigmoid().mean()\n",
    "        # 计算鉴别器将多少比例的假数据判定为真,仅用于输出显示\n",
    "        gnet.zero_grad()\n",
    "        gloss.backward()\n",
    "        goptimizer.step()\n",
    "\n",
    "        # viz.text('train Generator',win='Monitor',append=True,opts = {'title':'ProcessMonitor',},)\n",
    "        print(\n",
    "            \"[{}/{}]\".format(epoch + 1, epoch_num)\n",
    "            + \"D loss:{:g} G loss:{:g}\".format(dloss, gloss)\n",
    "            + \"真判真:{:g} 假判真:{:g}/{:g}\".format(dmean_real, dmean_fake, gmean_fake)\n",
    "        )\n",
    "\n",
    "    # 可视化，每 epoch 更新\n",
    "    viz.line(\n",
    "        [float(gloss)],\n",
    "        [epoch],\n",
    "        win=\"loss_perEpoch\",\n",
    "        name=\"G_loss\",\n",
    "        update=\"append\",\n",
    "        opts=dict(title=\"loss_perEpoch\", xlabel=\"epoch\", ylabel=\"loss\"),\n",
    "    )\n",
    "    viz.line(\n",
    "        [float(dloss)], [epoch], win=\"loss_perEpoch\", name=\"D_loss\", update=\"append\"\n",
    "    )\n",
    "    viz.line(\n",
    "        [float(goptimizer.state_dict()[\"param_groups\"][0][\"lr\"])],\n",
    "        [epoch],\n",
    "        win=\"lr_perEpoch\",\n",
    "        name=\"G_loss\",\n",
    "        update=\"append\",\n",
    "        opts=dict(title=\"lr_perEpoch\", xlabel=\"epoch\", ylabel=\"lr\"),\n",
    "    )\n",
    "    viz.line(\n",
    "        [float(doptimizer.state_dict()[\"param_groups\"][0][\"lr\"])],\n",
    "        [epoch],\n",
    "        win=\"lr_perEpoch\",\n",
    "        name=\"D_loss\",\n",
    "        update=\"append\",\n",
    "    )\n",
    "    viz.line(\n",
    "        [float(dmean_real)],\n",
    "        [epoch],\n",
    "        win=\"D real_perEpoch\",\n",
    "        update=\"append\",\n",
    "        opts=dict(title=\"D real真判真_perEpoch\", xlabel=\"epoch\"),\n",
    "    )\n",
    "    viz.line(\n",
    "        [float(gmean_fake)],\n",
    "        [epoch],\n",
    "        win=\"D fake_perEpoch\",\n",
    "        update=\"append\",\n",
    "        opts=dict(title=\"D fake假判真_perEpoch\", xlabel=\"epoch\"),\n",
    "    )\n",
    "    # 更新学习率\n",
    "    g_lr_schedule.step()\n",
    "    d_lr_schedule.step()\n",
    "    viz.text(\n",
    "        \"updating weights\",\n",
    "        win=\"Monitor\",\n",
    "        append=True,\n",
    "        opts={\n",
    "            \"title\": \"ProcessMonitor\",\n",
    "        },\n",
    "    )\n",
    "    # 定期保存\n",
    "    if (epoch + 1) % 500 == 0:\n",
    "        timeForSave = datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "        checkpointPath_g = (\n",
    "            ckpDir + \"g_ep_\" + str(epoch + 1) + \"_\" + timeForSave + \".pth\"\n",
    "        )\n",
    "        checkpointPath_d = (\n",
    "            ckpDir + \"d_ep_\" + str(epoch + 1) + \"_\" + timeForSave + \".pth\"\n",
    "        )\n",
    "        d_state = {\n",
    "            \"model\": dnet.state_dict(),\n",
    "            \"optimizer\": doptimizer.state_dict(),\n",
    "            \"epoch\": epoch,\n",
    "        }\n",
    "        torch.save(d_state, checkpointPath_d)\n",
    "        g_state = {\n",
    "            \"model\": gnet.state_dict(),\n",
    "            \"optimizer\": goptimizer.state_dict(),\n",
    "            \"epoch\": epoch,\n",
    "        }\n",
    "        torch.save(g_state, checkpointPath_g)\n",
    "        viz.text(\n",
    "            \"epoch \" + str(epoch + 1) + \" model saved\",\n",
    "            win=\"Monitor\",\n",
    "            append=True,\n",
    "            opts={\n",
    "                \"title\": \"ProcessMonitor\",\n",
    "            },\n",
    "        )\n",
    "\n",
    "    #     计算验证集上的 AUC 值\n",
    "    viz.text(\n",
    "        \"calculate auc start\",\n",
    "        win=\"Monitor\",\n",
    "        opts={\n",
    "            \"title\": \"ProcessMonitor\",\n",
    "        },\n",
    "    )\n",
    "    dnet.eval()\n",
    "    dnet.to(\"cpu\")\n",
    "    emg_vec = emgdata_to_net_preds_sigmoid(data_set=val_set, net_vector=dnet)\n",
    "    emg_vec = emg_vec.ravel()\n",
    "    auc_current = metrics.roc_auc_score(vallabel_for_auc, emg_vec, average=\"micro\")\n",
    "    viz.line(\n",
    "        [auc_current],\n",
    "        [epoch],\n",
    "        win=\"val_AUC\",\n",
    "        update=\"append\",\n",
    "        opts=dict(title=\"VAL_AUC\", xlabel=\"epoch\", ylabel=\"AUC\"),\n",
    "    )\n",
    "    #     保存策略：整个训练过程中最大的AUC及所有AUC值超过0.8的对应模型，由于最大值源于实时比较，刚开始会有一部分低AUC模型被保存，\n",
    "    #     无需在意，过后删去即可\n",
    "    if auc_current > auc_max:\n",
    "        auc_max = auc_current\n",
    "        viz.text(\n",
    "            \"max auc: \" + str(auc_max) + \", in ep \" + str(epoch + 1),\n",
    "            win=\"message\",\n",
    "            append=False,\n",
    "            opts={\n",
    "                \"title\": \"Max AUC\",\n",
    "            },\n",
    "        )\n",
    "        max_auc_state = {\n",
    "            \"model\": dnet.state_dict(),\n",
    "            \"optimizer\": doptimizer.state_dict(),\n",
    "            \"epoch\": epoch,\n",
    "        }\n",
    "        checkpointPath_auc = (\n",
    "            ckpDir_auc\n",
    "            + \"auc\"\n",
    "            + str(int(auc_max * 1000))\n",
    "            + \"d_ep_\"\n",
    "            + str(epoch + 1)\n",
    "            + \"_\"\n",
    "            + timeForSave\n",
    "            + \".pth\"\n",
    "        )\n",
    "        torch.save(max_auc_state, checkpointPath_auc)\n",
    "        viz.text(\n",
    "            \"max auc: \"\n",
    "            + str(auc_max)\n",
    "            + \", in ep \"\n",
    "            + str(epoch + 1)\n",
    "            + \" saved at \"\n",
    "            + checkpointPath_auc,\n",
    "            win=\"message\",\n",
    "            append=True,\n",
    "            opts={\n",
    "                \"title\": \"Max AUC\",\n",
    "            },\n",
    "        )\n",
    "    elif auc_current >= 0.8:\n",
    "        checkpointPath_auc = (\n",
    "            ckpDir_auc\n",
    "            + \"auc\"\n",
    "            + str(int(auc_max * 1000))\n",
    "            + \"d_ep_\"\n",
    "            + str(epoch + 1)\n",
    "            + \"_\"\n",
    "            + timeForSave\n",
    "            + \".pth\"\n",
    "        )\n",
    "        torch.save(max_auc_state, checkpointPath_auc)\n",
    "        viz.text(\n",
    "            \"auc: \"\n",
    "            + str(auc_current)\n",
    "            + \", in ep \"\n",
    "            + str(epoch + 1)\n",
    "            + \" saved at \"\n",
    "            + checkpointPath_auc,\n",
    "            win=\"AUC_Monitor\",\n",
    "            append=True,\n",
    "            opts={\n",
    "                \"title\": \"Max AUC\",\n",
    "            },\n",
    "        )\n",
    "\n",
    "\n",
    "# 完整训练结束后，模型最终保存；意义不大，但以备不时之需\n",
    "checkpointPath_model = (\n",
    "    model_Dir\n",
    "    + \"g_final_\"\n",
    "    + \"acc\"\n",
    "    + str(int(total_test_acc * 10000))\n",
    "    + \"_\"\n",
    "    + timeForSave\n",
    "    + \".pth\"\n",
    ")\n",
    "torch.save(gnet.state_dict(), checkpointPath_model)\n",
    "checkpointPath_model = (\n",
    "    model_Dir\n",
    "    + \"d_final_\"\n",
    "    + \"acc\"\n",
    "    + str(int(total_test_acc * 10000))\n",
    "    + \"_\"\n",
    "    + timeForSave\n",
    "    + \".pth\"\n",
    ")\n",
    "torch.save(dnet.state_dict(), checkpointPath_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 验证结果\n",
    "绘制 ROC 曲线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 900x600 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 加载挑出的模型，计算 ROC 曲线、最佳阈值及 AUC 值\n",
    "checkpoint_net_fe = torch.load(\n",
    "    \"..//ckp//opg_0217a//auc//auc880d_ep_359_2022_02_28_09_16_29.pth\"\n",
    ")\n",
    "# net_fe = GAN_Dis_opengan_alpha(n_channel, n_d_feature)\n",
    "net_fe = dnet\n",
    "net_fe.to(\"cpu\")\n",
    "net_fe.load_state_dict(checkpoint_net_fe[\"model\"])\n",
    "net_fe.eval()\n",
    "# 计算模型对数据集的预测值，此函数还会通过sigmod函数以保持输出在0-1内\n",
    "emg_vec = emgdata_to_net_preds_sigmoid(data_set=val_set, net_vector=dnet)\n",
    "# 用来计算ROC,AUC 的数据需要同格式，直接全部展平至一维\n",
    "emg_vec = emg_vec.ravel()\n",
    "vallabel_for_auc = vallabel_for_auc.ravel()\n",
    "auc_test = metrics.roc_auc_score(vallabel_for_auc, emg_vec, average=\"micro\")\n",
    "# 计算roc曲线，以备后续绘制\n",
    "fpr, tpr, thresholds = metrics.roc_curve(vallabel_for_auc, emg_vec)\n",
    "auc_test_r = metrics.auc(fpr, tpr)\n",
    "# 按约登指数计算最佳阈值点\n",
    "optimals, points = Find_Optimal_Cutoff(tpr, fpr, thresholds)\n",
    "# 作ROC图\n",
    "plt.rcParams[\"figure.dpi\"] = 150\n",
    "plt.title(\"ROC(AUC:\" + str(\"%.5g\" % auc_test_r) + \")\")\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.plot(fpr, tpr, marker=\".\", label=\"ROC\")\n",
    "plt.plot(points[0], points[1], marker=\"*\", markersize=18, label=\"optimal cutoff\")\n",
    "# 在 ROC 图上标注最佳阈值点\n",
    "plt.annotate(\n",
    "    str(\"%.5g\" % points[0]) + \",\" + str(\"%.5g\" % points[1]),\n",
    "    xy=(points[0], points[1]),\n",
    "    xytext=(10, -20),\n",
    "    textcoords=\"offset points\",\n",
    "    bbox=dict(boxstyle=\"round,pad=0.5\", fc=\"yellow\", ec=\"k\", lw=1, alpha=0.5),\n",
    ")\n",
    "# 显示图例\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下一步应该让经过拒绝后的数据经过原始CNN分类器，再算一次准确率\n",
    "\n",
    "# 加载完整大数据集\n",
    "opg_data = np.load(\"../../data/OpgData_full_smr_10cl_220522_2.npy\", allow_pickle=True)\n",
    "opg_data = opg_data.item()\n",
    "Xtrain_known = opg_data[\"Xtrain_known\"]\n",
    "Xtest_known = opg_data[\"Xtest_known\"]\n",
    "Xval_opset = opg_data[\"Xval_opset\"]\n",
    "Ytrain_known = opg_data[\"Ytrain_known\"]\n",
    "Ytest_known = opg_data[\"Ytest_known\"]\n",
    "Yval_opset = opg_data[\"Yval_opset\"]\n",
    "X_train_Net_output = opg_data[\"X_train_Net_output\"]\n",
    "X_test_Net_output = opg_data[\"X_test_Net_output\"]\n",
    "X_val_Net_output = opg_data[\"X_val_Net_output\"]\n",
    "Yval_opset_mtv = opg_data[\"Yval_opset_mtv\"][:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 情形1，所有接受的样本中正确率变化\n",
    "# 加载CNN，过一遍验证集，中间加入 GAN 拒绝操作\n",
    "checkpoint_net_cnn = torch.load('..//ckp//opg_0217a//oth//c_ep_200_acc9763_2022_02_26_23_18_44.pth')\n",
    "net_cnn = Network_CNN_6ch_6cls_smr_v1()\n",
    "net_cnn.load_state_dict(checkpoint_net_cnn['model'])\n",
    "net_cnn.eval()\n",
    "# net_cnn.to(device)\n",
    "total_testnum = 0\n",
    "testdata = Xtest_known\n",
    "testlabel = Ytest_known[:,0]\n",
    "test_set = EMGDataset_2D(testdata, testlabel)\n",
    "test_val_on_cnn_set = EMGDataset_2D(Xval_opset, Yval_opset_mtv)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=2, shuffle=False)\n",
    "test_val_on_cnn_loader = torch.utils.data.DataLoader(test_val_on_cnn_set, batch_size=2, shuffle=False)\n",
    "batch1 = iter(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: torch.Size([2, 1, 200, 6]) tensor([[[[ 0.0195,  0.0015,  0.0049, -0.0190,  0.0638,  0.0233],\n",
      "          [ 0.0187, -0.0027,  0.0121,  0.0054,  0.0390,  0.0358],\n",
      "          [ 0.0098,  0.0011,  0.0121,  0.0240,  0.0028,  0.0296],\n",
      "          ...,\n",
      "          [ 0.0160,  0.0087,  0.0680, -0.0711,  0.0055,  0.0085],\n",
      "          [-0.0048,  0.0223,  0.0405, -0.0837,  0.0356,  0.0141],\n",
      "          [ 0.0116,  0.0266,  0.0162, -0.1236,  0.0647,  0.0280]]],\n",
      "\n",
      "\n",
      "        [[[-0.0173, -0.0149, -0.0623, -0.0209, -0.0621, -0.0923],\n",
      "          [-0.0231, -0.0156, -0.0641,  0.0186, -0.0390, -0.0488],\n",
      "          [-0.0206, -0.0148, -0.0535,  0.0628, -0.0144, -0.0224],\n",
      "          ...,\n",
      "          [-0.0247, -0.0351,  0.1067,  0.0336,  0.0373, -0.0140],\n",
      "          [-0.0192, -0.0440,  0.1082,  0.0433,  0.0340, -0.0670],\n",
      "          [-0.0185, -0.0384,  0.1060,  0.0619,  0.0261, -0.0861]]]],\n",
      "       dtype=torch.float64) \n",
      " y: torch.Size([2]) tensor([0, 0], dtype=torch.int16)\n"
     ]
    }
   ],
   "source": [
    "batch2 = next(batch1)\n",
    "d_x, d_y = batch2\n",
    "print(\n",
    "    \"x:\",\n",
    "    d_x.shape,\n",
    "    d_x,\n",
    "    \"\\n\",\n",
    "    \"y:\",\n",
    "    d_y.shape,\n",
    "    d_y,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 原始手势测试集，经过 CNN 后正确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin_acc: 0.976313640076232 \n",
      " opset acc: 0.5765273311897106\n"
     ]
    }
   ],
   "source": [
    "# 原始手势测试集，经过 CNN 后正确率\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=128, shuffle=False)\n",
    "test_val_on_cnn_loader = torch.utils.data.DataLoader(\n",
    "    test_val_on_cnn_set, batch_size=128, shuffle=False\n",
    ")\n",
    "# 是的，目前并没有必要使用 **不定参数，这只是一个练习，函数本身在reuse.py中\n",
    "kwargs = {\"device\": \"cuda:0\", \"data_set\": test_loader, \"network\": net_cnn}\n",
    "# 原始准确率\n",
    "origin_acc = test_network_accuracy_xy_only(**kwargs)\n",
    "kwargs = {\"device\": \"cuda:0\", \"data_set\": test_val_on_cnn_loader, \"network\": net_cnn}\n",
    "# 在包含未知类的验证集上的准确率\n",
    "opset_acc = test_network_accuracy_xy_only(**kwargs)\n",
    "print(\"origin_acc:\", origin_acc, \"\\n\", \"opset acc:\", opset_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 被接受的数据中，准确率是否生变"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Flatten(start_dim=1, end_dim=-1)\n",
       "  (1): Linear(in_features=6, out_features=128, bias=True)\n",
       "  (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (3): LeakyReLU(negative_slope=0.2)\n",
       "  (4): Dropout2d(p=0.2, inplace=False)\n",
       "  (5): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (6): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (7): LeakyReLU(negative_slope=0.2)\n",
       "  (8): Dropout2d(p=0.2, inplace=False)\n",
       "  (9): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 被接受的数据中，准确率是否生变\n",
    "# 加载判别器\n",
    "path_dnet = torch.load(\n",
    "    \"..//ckp//opg_0217a//auc//auc880d_ep_359_2022_02_28_09_16_29.pth\"\n",
    ")\n",
    "dnet_opset = GAN_Dis(n_channel = 6, n_d_feature = 64)\n",
    "dnet_opset.to(\"cpu\")\n",
    "dnet_opset.load_state_dict(path_dnet[\"model\"])\n",
    "dnet_opset.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算模型对数据集的预测值，此函数还会通过sigmod函数以保持输出在0-1内\n",
    "dnet_opset.to(\"cpu\")\n",
    "a_emg_vec = emgdata_to_net_preds_sigmoid(data_set=val_set, net_vector=dnet_opset)\n",
    "# 用来计算ROC,AUC 的数据需要同格式，直接全部展平至一维\n",
    "a_emg_vec = a_emg_vec.ravel()\n",
    "a_vallabel_for_auc = vallabel_for_auc.ravel()\n",
    "a_auc_test = metrics.roc_auc_score(vallabel_for_auc, emg_vec, average=\"micro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC before rejection: 0.5765273311897106 AER: 0.42347266881028944 \n",
      " ACC after rejection: 0.8107904642409034 AER: 0.1892095357590966\n"
     ]
    }
   ],
   "source": [
    "# 加载EMG原数据，让 CNN 判断，给出的特征值过一下判别器，通过的再计数\n",
    "# from collections import Counter\n",
    "test_val_on_cnn_loader = torch.utils.data.DataLoader(\n",
    "    test_val_on_cnn_set, batch_size=256, shuffle=False\n",
    ")\n",
    "optimal_cutoff = optimals\n",
    "total_test_correct_kn = 0\n",
    "total_testnum_kn = 0\n",
    "total_test_correct_un = 0\n",
    "total_testnum_un = 0\n",
    "a_vec_list = []\n",
    "a_vec_confidence_list = []\n",
    "net_cnn.to(device)\n",
    "dnet_opset.to(device)\n",
    "dnet_opset.eval()\n",
    "i = 0\n",
    "for testemgdatas, testemglabels in test_val_on_cnn_loader:\n",
    "    testemgdatas = testemgdatas.to(torch.float32)\n",
    "    testemgdatas = testemgdatas.to(device)\n",
    "    testemglabels = testemglabels.long()\n",
    "    testemglabels = testemglabels.to(device)\n",
    "    predstest = net_cnn(testemgdatas)\n",
    "    # 网络输出需要过一下 sigmoid ，我查了一天，果然是因为太久没看这个项目了\n",
    "    d_accept = dnet_opset(predstest).sigmoid()\n",
    "    # 下面代码，甚是妙极\n",
    "    aaa = d_accept >= optimal_cutoff\n",
    "    # d_reject = d_reject[aaa]\n",
    "    aaa_2 = aaa.squeeze()\n",
    "    predstest_2 = predstest[aaa_2, :]\n",
    "    preds_cpu = d_accept.to(\"cpu\")\n",
    "    a_vec_list.append((preds_cpu.detach().numpy().flatten()))\n",
    "    d_reject_label = testemglabels[aaa_2]\n",
    "    # 被接受的\n",
    "    curr_test_correct_kn = get_num_correct(predstest, testemglabels, dim=1)\n",
    "    total_testnum_kn += testemglabels.size(0)\n",
    "    total_test_correct_kn += curr_test_correct_kn\n",
    "    if aaa_2.sum() != 0:\n",
    "        curr_test_correct_un = get_num_correct(predstest_2, d_reject_label, dim=1)\n",
    "        total_testnum_un += d_reject_label.size(0)\n",
    "        total_test_correct_un += curr_test_correct_un\n",
    "# aaa_3 = Counter(a_vec_confidence_list)\n",
    "total_test_acc_kn = total_test_correct_kn / total_testnum_kn\n",
    "total_test_acc_un = total_test_correct_un / total_testnum_un\n",
    "print(\n",
    "    \"ACC before rejection:\",\n",
    "    total_test_acc_kn, \n",
    "    \"AER:\",1-total_test_acc_kn,\n",
    "    \"\\n\",\n",
    "    \"ACC after rejection:\",\n",
    "    total_test_acc_un,\n",
    "    \"AER:\",1-total_test_acc_un\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "696bc8a05a1eb808e62d18e9328544428937c8e11a4c703581cc2236b7d3d24e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 ('torchlearn')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
