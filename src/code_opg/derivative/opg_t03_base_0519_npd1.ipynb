{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## opg_t03_base_0519_npd1\n",
    "基于 opg_aftertraining0519.ipynb\n",
    "本代码主要完成ninapro db1 同 subject 下，不同的模型已知类数量对结果的影响验证工作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "import datetime\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io as scio\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn import metrics\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchsummary import summary\n",
    "from torchvision import transforms\n",
    "from torchviz import make_dot\n",
    "# import hiddenlayer as h\n",
    "from visdom import Visdom\n",
    "\n",
    "# from sklearn.preprocessing import label_binarize\n",
    "# from sklearn import preprocessing\n",
    "# from sklearn import tree \n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from utils.networks import *\n",
    "from utils.reuse import *\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预备操作\n",
    "设置检查点、visdom 日志文件存储等日志性文件存储位置；\n",
    "初始化 visdom,记得先在命令行输入 visdom 运行（python环境下）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022_06_29_18_42_37\n"
     ]
    }
   ],
   "source": [
    "# 以下是检查点路径\n",
    "# 请在当前环境下 CMD 输入python -m visdom.server 或 visdom 启动监视器\n",
    "# 数据处理现在已移至 emgDataprocess.ipynb\n",
    "# 现在model_Dir 作为所有文件的父目录，不再分设开导致文件难寻\n",
    "model_Dir = \"..//model//opg_testops_220628a//s1//\"\n",
    "if not os.path.exists(model_Dir):\n",
    "    os.makedirs(model_Dir)\n",
    "# 这是正常定期检查点存储位置\n",
    "ckpDir = model_Dir + \"ckp//\"\n",
    "if not os.path.exists(ckpDir):\n",
    "    os.makedirs(ckpDir)\n",
    "# 这里是特挑最佳AUC的位置\n",
    "ckpDir_auc = ckpDir + \"auc//\"\n",
    "if not os.path.exists(ckpDir_auc):\n",
    "    os.makedirs(ckpDir_auc)\n",
    "# 这是 visdom 日志文件存储位置，留待备用\n",
    "vislogDir = model_Dir + \"vislog//\"\n",
    "if not os.path.exists(vislogDir):\n",
    "    os.makedirs(vislogDir)\n",
    "\n",
    "def get_current_time():\n",
    "    return datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "\n",
    "\n",
    "print(get_current_time())\n",
    "\n",
    "timeForSave = datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "visdom has started\n"
     ]
    }
   ],
   "source": [
    "# 以下是 visdom 监视窗口初始化，实现每次启用时重新加载，这里只写了 NameError 以防其他错误不能被发现\n",
    "class visdom_account:\n",
    "    def __init__(self):\n",
    "        self.port = 8097\n",
    "        self.server = \"http://localhost\"\n",
    "        self.base_url = \"/\"\n",
    "        self.username = \"admin\"\n",
    "        self.passward = \"1234\"\n",
    "        self.evns = \"train_opg_t03_s2\"\n",
    "\n",
    "\n",
    "viz_acnt = visdom_account()\n",
    "\n",
    "\n",
    "def viz_init():\n",
    "    try:\n",
    "        viz\n",
    "    except NameError:\n",
    "        viz = Visdom(\n",
    "            env=viz_acnt.evns, log_to_filename=vislogDir + \"vislog_\" + timeForSave\n",
    "        )\n",
    "        print(\"visdom has started\")\n",
    "    else:\n",
    "        viz.close()\n",
    "        del viz\n",
    "        print(\"last visdom session closed\")\n",
    "        viz = Visdom(\n",
    "            env=viz_acnt.evns, log_to_filename=vislogDir + \"vislog_\" + timeForSave\n",
    "        )\n",
    "        print(\"visdom has restarted\")\n",
    "    return viz\n",
    "\n",
    "\n",
    "viz = viz_init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义CNN神经网络结构\n",
    "CNN 训练部分，为了获得一个已知类分类器\n",
    "本段由于需要频繁更改类别数，所以这里的默认最后一层个数不定，在4-20间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [1, 32, 20, 10]             320\n",
      "            Conv2d-2            [1, 32, 20, 10]           9,248\n",
      "            Linear-3                   [1, 128]         819,328\n",
      "         Dropout2d-4                   [1, 128]               0\n",
      "            Linear-5                    [1, 20]           2,580\n",
      "================================================================\n",
      "Total params: 831,476\n",
      "Trainable params: 831,476\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.10\n",
      "Params size (MB): 3.17\n",
      "Estimated Total Size (MB): 3.27\n",
      "----------------------------------------------------------------\n",
      "Outputshape: torch.Size([1, 20])\n"
     ]
    }
   ],
   "source": [
    "# 自定义神经网络,CNN\n",
    "def get_num_correct(preds, labels):\n",
    "    return preds.argmax(dim=1).eq(labels).sum().item()\n",
    "\n",
    "num_kn = 20\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self,num_kn = num_kn):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=32, out_channels=32, kernel_size=3, padding=1\n",
    "        )\n",
    "        self.conv3 = nn.Conv2d(\n",
    "            in_channels=32, out_channels=32, kernel_size=3, padding=0\n",
    "        )\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=32 * 10 * 20, out_features=128)\n",
    "        self.out = nn.Linear(in_features=128, out_features=num_kn)\n",
    "        self.dr1 = nn.Dropout2d(0.2)\n",
    "\n",
    "    def forward(self, t):\n",
    "        # (1) input layer\n",
    "        t = t\n",
    "\n",
    "        # (2) hidden conv layer\n",
    "        t = self.conv1(t)\n",
    "        t = F.relu(t)\n",
    "        # t = F.max_pool2d(t, kernel_size=2, stride=1)\n",
    "\n",
    "        # (3) hidden conv layer\n",
    "        t = self.conv2(t)\n",
    "        t = F.relu(t)\n",
    "        # t = self.dr1(t)\n",
    "        # t = F.max_pool2d(t, kernel_size=2, stride=1)\n",
    "\n",
    "        # (4) hidden linear layer\n",
    "        t = t.reshape(-1, 32 * 10 * 20)\n",
    "        t = self.fc1(t)\n",
    "        t = F.relu(t)\n",
    "        t = self.dr1(t)\n",
    "\n",
    "        # (5) output layer\n",
    "        t = self.out(t)\n",
    "\n",
    "        return t\n",
    "\n",
    "\n",
    "net = Network(num_kn  = num_kn)\n",
    "# 打印网络，检查输入输出 shape是否正确\n",
    "# print(net)\n",
    "summary(net, (1, 20, 10), batch_size=1, device=\"cpu\")\n",
    "# 可视化结构，torchviz\n",
    "sampleInput = torch.randn(1, 1, 20, 10).requires_grad_(True)\n",
    "sampleOutput = net(sampleInput)\n",
    "print(\"Outputshape:\", sampleOutput.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据集加载、构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "<iterator object at 0x0000016C80070A60>\n"
     ]
    }
   ],
   "source": [
    "# CNN训练数据加载\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # 将图片转换为Tensor,归一化至[0,1]\n",
    "])\n",
    "\n",
    "class EMGDataset(Dataset):\n",
    " \n",
    "    def __init__(self, data, label):\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "        self.transforms = transform\n",
    " \n",
    "    def __getitem__(self, index):\n",
    "        emgData = self.data[index,:,:,:]\n",
    "        emgData = np.squeeze(emgData)\n",
    "        emglabel = self.label[index]\n",
    "        emglabel = emglabel.astype(np.int16)\n",
    "        emgData = self.transforms(emgData)\n",
    "        # 一维数据用下面的这个就行\n",
    "        # emgData = torch.Tensor(emgData)     \n",
    "        return emgData,emglabel\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    " \n",
    "# if __name__ == '__main__':\n",
    "dataarray = np.load('../../data/nina_db1/t03/s5/Fopgs5_diffratio_20220627T192504.npy',allow_pickle=True)\n",
    "CNNdataset = dataarray.item()\n",
    "print(type(CNNdataset))\n",
    "tr_x_str = \"ktr_X_\"+str(num_kn)+\"c\"\n",
    "tr_y_str = \"ktr_Y_\"+str(num_kn)+\"c\"\n",
    "te_x_str = \"kte_X_\"+str(num_kn)+\"c\"\n",
    "te_y_str = \"kte_Y_\"+str(num_kn)+\"c\"\n",
    "traindata = CNNdataset[tr_x_str]\n",
    "trainlabel = CNNdataset[tr_y_str]\n",
    "testdata = CNNdataset[te_x_str]\n",
    "testlabel = CNNdataset[te_y_str]\n",
    "# # 先用4类的\n",
    "# valdata = CNNdataset['val_X_8c']\n",
    "# vallabel = CNNdataset['val_Y_8c']\n",
    "\n",
    "trainlabel = trainlabel[:,0]\n",
    "testlabel = testlabel[:,0]\n",
    "# vallabel = vallabel[:,0]\n",
    "\n",
    "train_set = EMGDataset(traindata, trainlabel)\n",
    "test_set = EMGDataset(testdata, testlabel)\n",
    "# val_set = EMGDataset(valdata, vallabel)\n",
    "# 试运行数据集，构建迭代器\n",
    "sample = iter(test_set)\n",
    "print(sample)\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=2, shuffle=False)\n",
    "batch1 = iter(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: torch.Size([2, 1, 20, 10]) tensor([[[[0.0269, 0.0293, 0.0024, 0.0024, 0.0024, 0.0024, 0.0732, 0.0488,\n",
      "           0.0024, 0.0073],\n",
      "          [0.0244, 0.0317, 0.0024, 0.0024, 0.0024, 0.0024, 0.1001, 0.0635,\n",
      "           0.0024, 0.0098],\n",
      "          [0.0220, 0.0244, 0.0024, 0.0024, 0.0024, 0.0024, 0.1074, 0.0757,\n",
      "           0.0024, 0.0195],\n",
      "          [0.0171, 0.0146, 0.0024, 0.0024, 0.0024, 0.0024, 0.1172, 0.0854,\n",
      "           0.0024, 0.0220],\n",
      "          [0.0195, 0.0049, 0.0024, 0.0024, 0.0024, 0.0024, 0.1147, 0.0854,\n",
      "           0.0024, 0.0220],\n",
      "          [0.0269, 0.0049, 0.0024, 0.0024, 0.0024, 0.0024, 0.1147, 0.0806,\n",
      "           0.0024, 0.0146],\n",
      "          [0.0342, 0.0171, 0.0024, 0.0024, 0.0024, 0.0024, 0.1221, 0.0781,\n",
      "           0.0024, 0.0098],\n",
      "          [0.0391, 0.0171, 0.0024, 0.0024, 0.0024, 0.0024, 0.1147, 0.0732,\n",
      "           0.0024, 0.0049],\n",
      "          [0.0488, 0.0146, 0.0024, 0.0024, 0.0024, 0.0024, 0.1001, 0.0659,\n",
      "           0.0024, 0.0024],\n",
      "          [0.0464, 0.0122, 0.0024, 0.0024, 0.0049, 0.0024, 0.0830, 0.0610,\n",
      "           0.0024, 0.0024],\n",
      "          [0.0464, 0.0073, 0.0024, 0.0024, 0.0024, 0.0024, 0.0757, 0.0562,\n",
      "           0.0024, 0.0024],\n",
      "          [0.0439, 0.0244, 0.0024, 0.0024, 0.0024, 0.0024, 0.0732, 0.0562,\n",
      "           0.0024, 0.0073],\n",
      "          [0.0488, 0.0415, 0.0024, 0.0024, 0.0024, 0.0024, 0.0732, 0.0537,\n",
      "           0.0024, 0.0098],\n",
      "          [0.0586, 0.0635, 0.0024, 0.0024, 0.0024, 0.0024, 0.0952, 0.0562,\n",
      "           0.0024, 0.0122],\n",
      "          [0.0537, 0.1196, 0.0049, 0.0024, 0.0024, 0.0024, 0.1025, 0.0537,\n",
      "           0.0098, 0.0098],\n",
      "          [0.0488, 0.1538, 0.0073, 0.0024, 0.0024, 0.0024, 0.0952, 0.0513,\n",
      "           0.0610, 0.0073],\n",
      "          [0.0391, 0.1538, 0.0049, 0.0024, 0.0024, 0.0024, 0.0806, 0.0464,\n",
      "           0.0806, 0.0098],\n",
      "          [0.0317, 0.1367, 0.0024, 0.0024, 0.0024, 0.0024, 0.0732, 0.0415,\n",
      "           0.0806, 0.0073],\n",
      "          [0.0317, 0.1123, 0.0024, 0.0024, 0.0024, 0.0024, 0.0659, 0.0439,\n",
      "           0.0732, 0.0049],\n",
      "          [0.0269, 0.0928, 0.0024, 0.0024, 0.0024, 0.0024, 0.0586, 0.0439,\n",
      "           0.0586, 0.0024]]],\n",
      "\n",
      "\n",
      "        [[[0.0464, 0.0073, 0.0024, 0.0024, 0.0024, 0.0024, 0.0757, 0.0562,\n",
      "           0.0024, 0.0024],\n",
      "          [0.0439, 0.0244, 0.0024, 0.0024, 0.0024, 0.0024, 0.0732, 0.0562,\n",
      "           0.0024, 0.0073],\n",
      "          [0.0488, 0.0415, 0.0024, 0.0024, 0.0024, 0.0024, 0.0732, 0.0537,\n",
      "           0.0024, 0.0098],\n",
      "          [0.0586, 0.0635, 0.0024, 0.0024, 0.0024, 0.0024, 0.0952, 0.0562,\n",
      "           0.0024, 0.0122],\n",
      "          [0.0537, 0.1196, 0.0049, 0.0024, 0.0024, 0.0024, 0.1025, 0.0537,\n",
      "           0.0098, 0.0098],\n",
      "          [0.0488, 0.1538, 0.0073, 0.0024, 0.0024, 0.0024, 0.0952, 0.0513,\n",
      "           0.0610, 0.0073],\n",
      "          [0.0391, 0.1538, 0.0049, 0.0024, 0.0024, 0.0024, 0.0806, 0.0464,\n",
      "           0.0806, 0.0098],\n",
      "          [0.0317, 0.1367, 0.0024, 0.0024, 0.0024, 0.0024, 0.0732, 0.0415,\n",
      "           0.0806, 0.0073],\n",
      "          [0.0317, 0.1123, 0.0024, 0.0024, 0.0024, 0.0024, 0.0659, 0.0439,\n",
      "           0.0732, 0.0049],\n",
      "          [0.0269, 0.0928, 0.0024, 0.0024, 0.0024, 0.0024, 0.0586, 0.0439,\n",
      "           0.0586, 0.0024],\n",
      "          [0.0244, 0.0708, 0.0024, 0.0024, 0.0024, 0.0024, 0.0757, 0.0439,\n",
      "           0.0488, 0.0024],\n",
      "          [0.0220, 0.0537, 0.0122, 0.0024, 0.0024, 0.0024, 0.1025, 0.0464,\n",
      "           0.0366, 0.0024],\n",
      "          [0.0195, 0.0391, 0.0220, 0.0024, 0.0024, 0.0024, 0.1294, 0.0439,\n",
      "           0.0293, 0.0024],\n",
      "          [0.0146, 0.0269, 0.0195, 0.0024, 0.0024, 0.0024, 0.1294, 0.0464,\n",
      "           0.0171, 0.0024],\n",
      "          [0.0122, 0.0146, 0.0146, 0.0024, 0.0024, 0.0024, 0.1196, 0.0488,\n",
      "           0.0098, 0.0024],\n",
      "          [0.0073, 0.0024, 0.0049, 0.0024, 0.0024, 0.0024, 0.1099, 0.0464,\n",
      "           0.0024, 0.0024],\n",
      "          [0.0122, 0.0024, 0.0024, 0.0024, 0.0024, 0.0024, 0.0952, 0.0464,\n",
      "           0.0024, 0.0024],\n",
      "          [0.0146, 0.0024, 0.0024, 0.0024, 0.0024, 0.0024, 0.0903, 0.0439,\n",
      "           0.0024, 0.0024],\n",
      "          [0.0293, 0.0220, 0.0024, 0.0024, 0.0024, 0.0024, 0.0879, 0.0415,\n",
      "           0.0269, 0.0024],\n",
      "          [0.0366, 0.0684, 0.0049, 0.0024, 0.0024, 0.0024, 0.1270, 0.0439,\n",
      "           0.0952, 0.0024]]]], dtype=torch.float64) \n",
      " y: torch.Size([2]) tensor([0, 0], dtype=torch.int16)\n"
     ]
    }
   ],
   "source": [
    "batch2 = next(batch1)\n",
    "d_x,d_y = batch2\n",
    "print(\n",
    "    \"x:\",\n",
    "    d_x.shape,\n",
    "    d_x,\n",
    "    \"\\n\",\n",
    "    \"y:\",\n",
    "    d_y.shape,\n",
    "    d_y\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练 CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n",
      "Setting up a new session...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "visdom has started\n",
      "ep 1 tr_ac: 0.2595144081324036 ls: 117.44196057319641 te_ls: 1.9105627536773682 te_ac: 0.4127894156560088\n",
      "ep 2 tr_ac: 0.4272032260774595 ls: 84.8479015827179 te_ls: 1.3904591798782349 te_ac: 0.5369349503858876\n",
      "ep 3 tr_ac: 0.5120557842560699 ls: 71.51802432537079 te_ls: 1.3942406177520752 te_ac: 0.5715545755237045\n",
      "ep 4 tr_ac: 0.561287070486432 ls: 64.8975305557251 te_ls: 1.404667615890503 te_ac: 0.619845644983462\n",
      "ep 5 tr_ac: 0.5960682180962783 ls: 59.81566905975342 te_ls: 1.2508643865585327 te_ac: 0.6379272326350607\n",
      "ep 6 tr_ac: 0.6125346551289591 ls: 56.53174698352814 te_ls: 1.032187581062317 te_ac: 0.6557883131201764\n",
      "ep 7 tr_ac: 0.6310173905738049 ls: 53.47503846883774 te_ls: 0.7789742350578308 te_ac: 0.6670341786108048\n",
      "ep 8 tr_ac: 0.6484919768125682 ls: 51.13501167297363 te_ls: 1.0622671842575073 te_ac: 0.6710033076074973\n",
      "ep 9 tr_ac: 0.6631101403007645 ls: 49.174563229084015 te_ls: 1.2922528982162476 te_ac: 0.6820286659316428\n",
      "ep 10 tr_ac: 0.6719314458539863 ls: 47.335743725299835 te_ls: 1.0208261013031006 te_ac: 0.6866593164277839\n",
      "ep 11 tr_ac: 0.6827690498193733 ls: 45.520911037921906 te_ls: 1.104262113571167 te_ac: 0.6961411245865491\n",
      "ep 12 tr_ac: 0.6887339326220281 ls: 44.670967280864716 te_ls: 0.9139228463172913 te_ac: 0.7058434399117971\n",
      "ep 13 tr_ac: 0.6962110392338066 ls: 43.35689914226532 te_ls: 1.0190563201904297 te_ac: 0.718632855567806\n",
      "ep 14 tr_ac: 0.7086448794421575 ls: 41.47424805164337 te_ls: 1.0050795078277588 te_ac: 0.7239250275633958\n",
      "ep 15 tr_ac: 0.7154498865832143 ls: 40.78152972459793 te_ls: 1.1162101030349731 te_ac: 0.7334068357221609\n",
      "ep 16 tr_ac: 0.7195664958413845 ls: 39.76760846376419 te_ls: 0.705873966217041 te_ac: 0.7292171995589857\n",
      "ep 17 tr_ac: 0.727799714357725 ls: 38.5725154876709 te_ls: 0.7463703751564026 te_ac: 0.74090407938258\n",
      "ep 18 tr_ac: 0.7339326220280602 ls: 37.94444781541824 te_ls: 0.6965013742446899 te_ac: 0.7464167585446527\n",
      "ep 19 tr_ac: 0.736705032344787 ls: 36.766940236091614 te_ls: 0.5239903330802917 te_ac: 0.7521499448732084\n",
      "ep 20 tr_ac: 0.7436780643535243 ls: 36.43017280101776 te_ls: 1.0386587381362915 te_ac: 0.7567805953693495\n",
      "ep 21 tr_ac: 0.748382760648576 ls: 35.41873490810394 te_ls: 0.8419269323348999 te_ac: 0.7598676957001103\n",
      "ep 22 tr_ac: 0.7567840040325968 ls: 34.468712747097015 te_ls: 0.5570738911628723 te_ac: 0.7638368246968027\n",
      "ep 23 tr_ac: 0.7512391833991431 ls: 34.46788650751114 te_ls: 0.7086662650108337 te_ac: 0.7708930540242558\n",
      "ep 24 tr_ac: 0.7633369738721331 ls: 33.41787087917328 te_ls: 0.6642516255378723 te_ac: 0.7722160970231533\n",
      "ep 25 tr_ac: 0.7682937074687054 ls: 32.95039063692093 te_ls: 0.6049684286117554 te_ac: 0.7724366041896361\n",
      "ep 26 tr_ac: 0.7714861799546333 ls: 32.29871863126755 te_ls: 0.6490480303764343 te_ac: 0.7757442116868798\n",
      "ep 27 tr_ac: 0.7713181550869529 ls: 32.01210677623749 te_ls: 0.5417545437812805 te_ac: 0.7788313120176406\n",
      "ep 28 tr_ac: 0.7766949508527262 ls: 31.45455765724182 te_ls: 0.5783644914627075 te_ac: 0.7823594266813672\n",
      "ep 29 tr_ac: 0.7782911870956901 ls: 30.746824979782104 te_ls: 0.8364971876144409 te_ac: 0.7755237045203969\n",
      "ep 30 tr_ac: 0.7827438460892212 ls: 30.205359876155853 te_ls: 0.44300657510757446 te_ac: 0.787651598676957\n",
      "ep 31 tr_ac: 0.7824077963538604 ls: 30.209094524383545 te_ls: 0.6855136156082153 te_ac: 0.7887541345093716\n",
      "ep 32 tr_ac: 0.7852642191044275 ls: 29.379602253437042 te_ls: 0.4235065281391144 te_ac: 0.7885336273428887\n",
      "ep 33 tr_ac: 0.7900529278333194 ls: 29.093207001686096 te_ls: 0.38901248574256897 te_ac: 0.7883131201764058\n",
      "ep 34 tr_ac: 0.7905570024363606 ls: 28.75683543086052 te_ls: 0.6222308874130249 te_ac: 0.7969128996692393\n",
      "ep 35 tr_ac: 0.793161387885407 ls: 28.25866374373436 te_ls: 0.7946929931640625 te_ac: 0.7940463065049614\n",
      "ep 36 tr_ac: 0.7952616987314123 ls: 28.101817786693573 te_ls: 0.5487110614776611 te_ac: 0.7973539140022051\n",
      "ep 37 tr_ac: 0.7965218852390153 ls: 27.48499572277069 te_ls: 0.7258003354072571 te_ac: 0.8055126791620728\n",
      "ep 38 tr_ac: 0.8015626312694278 ls: 27.21699634194374 te_ls: 0.7549248337745667 te_ac: 0.7977949283351708\n",
      "ep 39 tr_ac: 0.8073594892044023 ls: 26.717864096164703 te_ls: 1.0709160566329956 te_ac: 0.8008820286659316\n",
      "ep 40 tr_ac: 0.8083676384104848 ls: 26.449507623910904 te_ls: 0.3619195818901062 te_ac: 0.8008820286659316\n",
      "ep 41 tr_ac: 0.809291775182727 ls: 26.440912008285522 te_ls: 0.5971084237098694 te_ac: 0.8092613009922822\n",
      "ep 42 tr_ac: 0.8053431907922373 ls: 26.189469814300537 te_ls: 0.727197527885437 te_ac: 0.8077177508269019\n",
      "ep 43 tr_ac: 0.8084516508443249 ls: 25.990819931030273 te_ls: 0.5753987431526184 te_ac: 0.8022050716648291\n",
      "ep 44 tr_ac: 0.8142485087792993 ls: 25.230742305517197 te_ls: 0.39566105604171753 te_ac: 0.8046306504961411\n",
      "ep 45 tr_ac: 0.8173569688313871 ls: 25.198933571577072 te_ls: 0.7095696330070496 te_ac: 0.8132304299889747\n",
      "ep 46 tr_ac: 0.8146685709485004 ls: 25.059476673603058 te_ls: 0.4365125298500061 te_ac: 0.8130099228224917\n",
      "ep 47 tr_ac: 0.81853314290515 ls: 24.764362305402756 te_ls: 0.3509044647216797 te_ac: 0.8205071664829107\n",
      "ep 48 tr_ac: 0.8224817272956397 ls: 24.436307817697525 te_ls: 0.45707547664642334 te_ac: 0.8143329658213891\n",
      "ep 49 tr_ac: 0.814920608250021 ls: 24.703610211610794 te_ls: 0.45527347922325134 te_ac: 0.8149944873208379\n",
      "ep 50 tr_ac: 0.8249180878770058 ls: 23.79487332701683 te_ls: 0.4048998951911926 te_ac: 0.8205071664829107\n",
      "ep 51 tr_ac: 0.8276904981937326 ls: 23.375164061784744 te_ls: 0.31004035472869873 te_ac: 0.8187431091510474\n",
      "ep 52 tr_ac: 0.8297908090397379 ls: 23.412324607372284 te_ls: 0.6762187480926514 te_ac: 0.8218302094818082\n",
      "ep 53 tr_ac: 0.8266823489876501 ls: 23.688372939825058 te_ls: 0.6546241044998169 te_ac: 0.8224917309812569\n",
      "ep 54 tr_ac: 0.8307149458119801 ls: 23.22404482960701 te_ls: 0.7502063512802124 te_ac: 0.8231532524807056\n",
      "ep 55 tr_ac: 0.8294547593043771 ls: 22.712322056293488 te_ls: 0.691177248954773 te_ac: 0.8260198456449834\n",
      "ep 56 tr_ac: 0.8367638410484751 ls: 22.56838870048523 te_ls: 0.32247185707092285 te_ac: 0.8169790518191842\n",
      "ep 57 tr_ac: 0.83063093337814 ls: 22.73872420191765 te_ls: 0.3072434961795807 te_ac: 0.8277839029768468\n",
      "ep 58 tr_ac: 0.838276064857599 ls: 21.981183171272278 te_ls: 0.5226235389709473 te_ac: 0.8240352811466373\n",
      "ep 59 tr_ac: 0.8327312442241451 ls: 21.995295852422714 te_ls: 0.3624742925167084 te_ac: 0.8310915104740904\n",
      "ep 60 tr_ac: 0.8349155675039905 ls: 22.012883454561234 te_ls: 0.6173861026763916 te_ac: 0.8288864388092613\n",
      "ep 61 tr_ac: 0.8397882886667227 ls: 21.656803786754608 te_ls: 0.42149943113327026 te_ac: 0.8361631753031974\n",
      "ep 62 tr_ac: 0.8452490968663362 ls: 21.21291694045067 te_ls: 0.4719821810722351 te_ac: 0.8308710033076075\n",
      "ep 63 tr_ac: 0.8418885995127279 ls: 21.314200460910797 te_ls: 0.46892428398132324 te_ac: 0.8326350606394708\n",
      "ep 64 tr_ac: 0.8434848357556919 ls: 20.983479738235474 te_ls: 0.5151953101158142 te_ac: 0.8348401323042999\n",
      "ep 65 tr_ac: 0.8446610098294548 ls: 21.159570157527924 te_ls: 0.2872373163700104 te_ac: 0.8357221609702316\n",
      "ep 66 tr_ac: 0.8430647735864908 ls: 20.882118344306946 te_ls: 0.574566125869751 te_ac: 0.8324145534729879\n",
      "ep 67 tr_ac: 0.844745022263295 ls: 20.574820578098297 te_ls: 0.37992697954177856 te_ac: 0.8306504961411246\n",
      "ep 68 tr_ac: 0.8424766865496094 ls: 20.45119097828865 te_ls: 0.32233062386512756 te_ac: 0.8414553472987872\n",
      "ep 69 tr_ac: 0.8492816936906662 ls: 20.17419809103012 te_ls: 0.28795433044433594 te_ac: 0.839470782800441\n",
      "ep 70 tr_ac: 0.8501218180290683 ls: 20.188542157411575 te_ls: 0.478494793176651 te_ac: 0.8414553472987872\n",
      "ep 71 tr_ac: 0.8533983029488365 ls: 19.880545377731323 te_ls: 0.23964181542396545 te_ac: 0.8410143329658214\n",
      "ep 72 tr_ac: 0.8518020667058724 ls: 19.938991397619247 te_ls: 0.36925655603408813 te_ac: 0.8359426681367145\n",
      "ep 73 tr_ac: 0.8499537931613879 ls: 19.769317626953125 te_ls: 0.3143908679485321 te_ac: 0.8357221609702316\n",
      "ep 74 tr_ac: 0.8543224397210787 ls: 19.41408398747444 te_ls: 0.5108159780502319 te_ac: 0.8487320837927232\n",
      "ep 75 tr_ac: 0.8551625640594808 ls: 19.281386882066727 te_ls: 0.4937281608581543 te_ac: 0.8390297684674752\n",
      "ep 76 tr_ac: 0.8540704024195581 ls: 19.38781914114952 te_ls: 0.29216280579566956 te_ac: 0.8405733186328556\n",
      "ep 77 tr_ac: 0.856422750567084 ls: 19.05029410123825 te_ls: 0.4321162700653076 te_ac: 0.8425578831312017\n",
      "ep 78 tr_ac: 0.8600352852222128 ls: 18.806194007396698 te_ls: 0.23785485327243805 te_ac: 0.8482910694597574\n",
      "ep 79 tr_ac: 0.857262874905486 ls: 19.117367684841156 te_ls: 0.7082064151763916 te_ac: 0.8480705622932745\n",
      "ep 80 tr_ac: 0.8577669495085273 ls: 18.75757646560669 te_ls: 0.3422738015651703 te_ac: 0.8480705622932745\n",
      "ep 81 tr_ac: 0.8563387381332437 ls: 18.9536654651165 te_ls: 0.5439799427986145 te_ac: 0.8533627342888643\n",
      "ep 82 tr_ac: 0.8628917079727799 ls: 18.565142571926117 te_ls: 0.6389056444168091 te_ac: 0.8476295479603088\n",
      "ep 83 tr_ac: 0.8645719566495841 ls: 18.10881307721138 te_ls: 0.4512856602668762 te_ac: 0.8482910694597574\n",
      "ep 84 tr_ac: 0.8607913971267748 ls: 18.462807059288025 te_ls: 0.6302201747894287 te_ac: 0.8460859977949283\n",
      "ep 85 tr_ac: 0.8633957825758212 ls: 18.198597252368927 te_ls: 0.7080954909324646 te_ac: 0.8447629547960309\n",
      "ep 86 tr_ac: 0.8654960934218264 ls: 17.89369884133339 te_ls: 0.5096496939659119 te_ac: 0.8449834619625137\n",
      "ep 87 tr_ac: 0.8671763420986306 ls: 17.57383319735527 te_ls: 0.5176903605461121 te_ac: 0.8476295479603088\n",
      "ep 88 tr_ac: 0.8662522053263884 ls: 17.610595285892487 te_ls: 0.3480662405490875 te_ac: 0.8560088202866594\n",
      "ep 89 tr_ac: 0.8630597328404603 ls: 17.928757667541504 te_ls: 0.4970048666000366 te_ac: 0.851819184123484\n",
      "ep 90 tr_ac: 0.865328068554146 ls: 17.660912543535233 te_ls: 0.3683173656463623 te_ac: 0.8522601984564498\n",
      "ep 91 tr_ac: 0.8682685037385534 ls: 17.5683151781559 te_ls: 0.562809407711029 te_ac: 0.8507166482910694\n",
      "ep 92 tr_ac: 0.8700327648491977 ls: 17.279156357049942 te_ls: 0.46787500381469727 te_ac: 0.8515986769570011\n",
      "ep 93 tr_ac: 0.8702848021507184 ls: 17.26940616965294 te_ls: 0.28864628076553345 te_ac: 0.8520396912899669\n",
      "ep 94 tr_ac: 0.8679324540031925 ls: 17.392538607120514 te_ls: 0.22346711158752441 te_ac: 0.8551267916207277\n",
      "ep 95 tr_ac: 0.8712929513568008 ls: 16.81158035993576 te_ls: 0.4946677088737488 te_ac: 0.851819184123484\n",
      "ep 96 tr_ac: 0.8728891875997647 ls: 17.03084483742714 te_ls: 0.30678972601890564 te_ac: 0.8571113561190739\n",
      "ep 97 tr_ac: 0.8721330756952029 ls: 16.675631642341614 te_ls: 0.4665452241897583 te_ac: 0.8522601984564498\n",
      "ep 98 tr_ac: 0.8725531378644039 ls: 16.730032444000244 te_ls: 0.48600655794143677 te_ac: 0.8551267916207277\n",
      "ep 99 tr_ac: 0.8759976476518525 ls: 16.57478041946888 te_ls: 0.3280688226222992 te_ac: 0.856670341786108\n",
      "ep 100 tr_ac: 0.8733092497689658 ls: 16.63941466808319 te_ls: 0.1333387941122055 te_ac: 0.8549062844542448\n",
      "ep 101 tr_ac: 0.8772578341594556 ls: 16.03794015944004 te_ls: 0.18172839283943176 te_ac: 0.8593164277839029\n",
      "ep 102 tr_ac: 0.8796941947408217 ls: 15.831276804208755 te_ls: 0.18177199363708496 te_ac: 0.8584343991179714\n",
      "ep 103 tr_ac: 0.8849029656389146 ls: 15.389935478568077 te_ls: 0.48340100049972534 te_ac: 0.8637265711135612\n",
      "ep 104 tr_ac: 0.8821305553221876 ls: 15.828448921442032 te_ls: 0.2698178291320801 te_ac: 0.8650496141124586\n",
      "ep 105 tr_ac: 0.8811224061161052 ls: 15.516586169600487 te_ls: 0.19666752219200134 te_ac: 0.8630650496141125\n",
      "ep 106 tr_ac: 0.8844829034697135 ls: 15.243198230862617 te_ls: 0.18673913180828094 te_ac: 0.86438809261301\n",
      "ep 107 tr_ac: 0.8851550029404351 ls: 15.315051883459091 te_ls: 0.19634421169757843 te_ac: 0.8610804851157663\n",
      "ep 108 tr_ac: 0.8852390153742754 ls: 15.154161423444748 te_ls: 0.4544445276260376 te_ac: 0.86438809261301\n",
      "ep 109 tr_ac: 0.8837267915651517 ls: 15.3067846596241 te_ls: 0.3726281523704529 te_ac: 0.8630650496141125\n",
      "ep 110 tr_ac: 0.8889355624632446 ls: 14.984690591692924 te_ls: 0.22315536439418793 te_ac: 0.8639470782800441\n",
      "ep 111 tr_ac: 0.8841468537343526 ls: 15.287227123975754 te_ls: 0.451894611120224 te_ac: 0.8648291069459757\n",
      "ep 112 tr_ac: 0.8872553137864404 ls: 15.110040113329887 te_ls: 0.3343490660190582 te_ac: 0.8639470782800441\n",
      "ep 113 tr_ac: 0.8879274132571621 ls: 15.061782211065292 te_ls: 0.32247573137283325 te_ac: 0.8637265711135612\n",
      "ep 114 tr_ac: 0.8885155002940435 ls: 14.719240620732307 te_ls: 0.2357618659734726 te_ac: 0.8599779492833517\n",
      "ep 115 tr_ac: 0.8890195748970847 ls: 14.734889581799507 te_ls: 0.5142638683319092 te_ac: 0.8639470782800441\n",
      "ep 116 tr_ac: 0.890363773838528 ls: 14.676656693220139 te_ls: 0.2538127899169922 te_ac: 0.8661521499448732\n",
      "ep 117 tr_ac: 0.8884314878602033 ls: 14.84542478621006 te_ls: 0.2813211679458618 te_ac: 0.86438809261301\n",
      "ep 118 tr_ac: 0.8912879106107704 ls: 14.801887720823288 te_ls: 0.275876522064209 te_ac: 0.8705622932745314\n",
      "ep 119 tr_ac: 0.8926321095522137 ls: 14.586759015917778 te_ls: 0.4540961980819702 te_ac: 0.8610804851157663\n",
      "ep 120 tr_ac: 0.8909518608754096 ls: 14.84986887872219 te_ls: 0.3468174636363983 te_ac: 0.8665931642778391\n",
      "ep 121 tr_ac: 0.8886835251617239 ls: 14.727329269051552 te_ls: 0.4298169016838074 te_ac: 0.8621830209481808\n",
      "ep 122 tr_ac: 0.8925480971183736 ls: 14.42804691195488 te_ls: 0.1849190592765808 te_ac: 0.8672546857772878\n",
      "ep 123 tr_ac: 0.8913719230446105 ls: 14.546614602208138 te_ls: 0.2948428690433502 te_ac: 0.8705622932745314\n",
      "ep 124 tr_ac: 0.8885995127278837 ls: 14.404465943574905 te_ls: 0.2487790733575821 te_ac: 0.8672546857772878\n",
      "ep 125 tr_ac: 0.8890195748970847 ls: 14.752817437052727 te_ls: 0.3659116327762604 te_ac: 0.864167585446527\n",
      "ep 126 tr_ac: 0.8907838360077291 ls: 14.415214762091637 te_ls: 0.25937291979789734 te_ac: 0.8674751929437706\n",
      "ep 127 tr_ac: 0.8922120473830127 ls: 14.231589928269386 te_ls: 0.440134733915329 te_ac: 0.866813671444322\n",
      "ep 128 tr_ac: 0.8926321095522137 ls: 14.14754730463028 te_ls: 0.2271163910627365 te_ac: 0.8685777287761852\n",
      "ep 129 tr_ac: 0.8954885323027808 ls: 14.189910680055618 te_ls: 0.41139402985572815 te_ac: 0.8694597574421169\n",
      "ep 130 tr_ac: 0.8952364950012602 ls: 14.112105086445808 te_ls: 0.37502962350845337 te_ac: 0.8617420066152149\n",
      "ep 131 tr_ac: 0.8964966815088633 ls: 14.027066066861153 te_ls: 0.48508840799331665 te_ac: 0.8683572216097023\n",
      "ep 132 tr_ac: 0.8938922960598169 ls: 14.22278867661953 te_ls: 0.5211097598075867 te_ac: 0.86438809261301\n",
      "ep 133 tr_ac: 0.8928001344198941 ls: 13.88310994207859 te_ls: 0.2598877549171448 te_ac: 0.8670341786108049\n",
      "ep 134 tr_ac: 0.8952364950012602 ls: 14.076389417052269 te_ls: 0.32065561413764954 te_ac: 0.8694597574421169\n",
      "ep 135 tr_ac: 0.8965806939427036 ls: 13.852228656411171 te_ls: 0.39419424533843994 te_ac: 0.8665931642778391\n",
      "ep 136 tr_ac: 0.8930521717214148 ls: 14.026396200060844 te_ls: 0.637997031211853 te_ac: 0.8661521499448732\n",
      "ep 137 tr_ac: 0.8980929177518273 ls: 13.721270486712456 te_ls: 0.6360607743263245 te_ac: 0.866813671444322\n",
      "ep 138 tr_ac: 0.8941443333613375 ls: 13.856916800141335 te_ls: 0.4422544836997986 te_ac: 0.8683572216097023\n",
      "ep 139 tr_ac: 0.8912038981769301 ls: 14.087540328502655 te_ls: 0.19462500512599945 te_ac: 0.8665931642778391\n",
      "ep 140 tr_ac: 0.8975888431487861 ls: 13.510312676429749 te_ls: 0.2965056598186493 te_ac: 0.8674751929437706\n",
      "ep 141 tr_ac: 0.8944803830966983 ls: 13.866090282797813 te_ls: 0.2727743983268738 te_ac: 0.869239250275634\n",
      "ep 142 tr_ac: 0.8978408804503066 ls: 13.778117582201958 te_ls: 0.49763187766075134 te_ac: 0.8743109151047409\n",
      "ep 143 tr_ac: 0.8942283457951777 ls: 13.663028359413147 te_ls: 0.4704148471355438 te_ac: 0.871664829106946\n",
      "ep 144 tr_ac: 0.9001932285978325 ls: 13.433254659175873 te_ls: 0.5908235311508179 te_ac: 0.8705622932745314\n",
      "ep 145 tr_ac: 0.8980929177518273 ls: 13.336267590522766 te_ls: 0.2788988947868347 te_ac: 0.8652701212789415\n",
      "ep 146 tr_ac: 0.8956565571704612 ls: 13.672964930534363 te_ls: 0.38280919194221497 te_ac: 0.8705622932745314\n",
      "ep 147 tr_ac: 0.8992690918255902 ls: 13.183809369802475 te_ls: 0.24001361429691315 te_ac: 0.8723263506063947\n",
      "ep 148 tr_ac: 0.8998571788624716 ls: 13.354544788599014 te_ls: 0.548646092414856 te_ac: 0.8694597574421169\n",
      "ep 149 tr_ac: 0.8990170545240695 ls: 13.35647839307785 te_ls: 0.37128201127052307 te_ac: 0.8683572216097023\n",
      "ep 150 tr_ac: 0.9001932285978325 ls: 13.0739875882864 te_ls: 0.2918151319026947 te_ac: 0.869239250275634\n",
      "ep 151 tr_ac: 0.9002772410316727 ls: 13.279850855469704 te_ls: 0.4584583342075348 te_ac: 0.8703417861080485\n",
      "ep 152 tr_ac: 0.8999411912963119 ls: 13.265478745102882 te_ls: 0.26801490783691406 te_ac: 0.8694597574421169\n",
      "ep 153 tr_ac: 0.9004452658993531 ls: 13.150536939501762 te_ls: 0.394218385219574 te_ac: 0.874090407938258\n",
      "ep 154 tr_ac: 0.9001932285978325 ls: 13.031363010406494 te_ls: 0.2949303090572357 te_ac: 0.8714443219404631\n",
      "ep 155 tr_ac: 0.898008905317987 ls: 13.145206168293953 te_ls: 0.2861120402812958 te_ac: 0.8701212789415655\n",
      "ep 156 tr_ac: 0.900025203730152 ls: 13.241462260484695 te_ls: 0.22434395551681519 te_ac: 0.8729878721058434\n",
      "ep 157 tr_ac: 0.9022095270099975 ls: 13.101454421877861 te_ls: 0.4171535074710846 te_ac: 0.8705622932745314\n",
      "ep 158 tr_ac: 0.9053179870620852 ls: 12.868116185069084 te_ls: 0.24136154353618622 te_ac: 0.8751929437706726\n",
      "ep 159 tr_ac: 0.9010333529362345 ls: 13.086852744221687 te_ls: 0.268751323223114 te_ac: 0.8751929437706726\n",
      "ep 160 tr_ac: 0.9024615643115181 ls: 12.917683348059654 te_ls: 0.23157507181167603 te_ac: 0.8710033076074972\n",
      "ep 161 tr_ac: 0.902797614046879 ls: 12.955382317304611 te_ls: 0.29319441318511963 te_ac: 0.8723263506063947\n",
      "ep 162 tr_ac: 0.901201377803915 ls: 12.793233573436737 te_ls: 0.21136151254177094 te_ac: 0.8729878721058434\n",
      "ep 163 tr_ac: 0.902797614046879 ls: 12.682193979620934 te_ls: 0.585148811340332 te_ac: 0.8727673649393606\n",
      "ep 164 tr_ac: 0.9043098378560027 ls: 12.46444109082222 te_ls: 0.1700996458530426 te_ac: 0.8707828004410143\n",
      "ep 165 tr_ac: 0.9022095270099975 ls: 12.953030347824097 te_ls: 0.17007923126220703 te_ac: 0.8712238147739801\n",
      "ep 166 tr_ac: 0.9057380492312862 ls: 12.452360391616821 te_ls: 0.4688313603401184 te_ac: 0.8751929437706726\n",
      "ep 167 tr_ac: 0.9075863227757708 ls: 12.473206490278244 te_ls: 0.2555224597454071 te_ac: 0.8743109151047409\n",
      "ep 168 tr_ac: 0.9037217508191212 ls: 12.828321680426598 te_ls: 0.2956242561340332 te_ac: 0.8718853362734289\n",
      "ep 169 tr_ac: 0.9048979248928841 ls: 12.739206179976463 te_ls: 0.5123195648193359 te_ac: 0.8723263506063947\n",
      "ep 170 tr_ac: 0.9044778627236831 ls: 12.67552787065506 te_ls: 0.2709081172943115 te_ac: 0.8712238147739801\n",
      "ep 171 tr_ac: 0.9069982357388894 ls: 12.572906509041786 te_ls: 0.575609564781189 te_ac: 0.8762954796030871\n",
      "ep 172 tr_ac: 0.9044778627236831 ls: 12.672668293118477 te_ls: 0.3151140809059143 te_ac: 0.8712238147739801\n",
      "ep 173 tr_ac: 0.904813912459044 ls: 12.364296481013298 te_ls: 0.5467072129249573 te_ac: 0.8749724366041897\n",
      "ep 174 tr_ac: 0.9080063849449719 ls: 12.2010867446661 te_ls: 0.11106482148170471 te_ac: 0.8721058434399118\n",
      "ep 175 tr_ac: 0.9106947828278585 ls: 12.109185561537743 te_ls: 0.32905763387680054 te_ac: 0.8745314222712238\n",
      "ep 176 tr_ac: 0.9069982357388894 ls: 12.416463151574135 te_ls: 0.18320593237876892 te_ac: 0.8754134509371555\n",
      "ep 177 tr_ac: 0.9070822481727295 ls: 12.057849556207657 te_ls: 0.3506976366043091 te_ac: 0.8756339581036384\n",
      "ep 178 tr_ac: 0.9067461984373687 ls: 12.254980564117432 te_ls: 0.41092485189437866 te_ac: 0.8743109151047409\n",
      "ep 179 tr_ac: 0.9057380492312862 ls: 12.069634184241295 te_ls: 0.40825197100639343 te_ac: 0.8773980154355017\n",
      "ep 180 tr_ac: 0.9085944719818533 ls: 12.249717190861702 te_ls: 0.3909083902835846 te_ac: 0.8729878721058434\n",
      "ep 181 tr_ac: 0.9082584222464924 ls: 12.057994857430458 te_ls: 0.5349982976913452 te_ac: 0.8754134509371555\n",
      "ep 182 tr_ac: 0.907670335209611 ls: 11.938861027359962 te_ls: 0.13759231567382812 te_ac: 0.874090407938258\n",
      "ep 183 tr_ac: 0.9059900865328069 ls: 12.28491124510765 te_ls: 0.4667002558708191 te_ac: 0.8687982359426681\n",
      "ep 184 tr_ac: 0.908846509283374 ls: 12.018628746271133 te_ls: 0.457396924495697 te_ac: 0.8776185226019846\n",
      "ep 185 tr_ac: 0.9047299000252037 ls: 12.234012514352798 te_ls: 0.6929723024368286 te_ac: 0.87651598676957\n",
      "ep 186 tr_ac: 0.9067461984373687 ls: 12.040993764996529 te_ls: 0.38493627309799194 te_ac: 0.8760749724366041\n",
      "ep 187 tr_ac: 0.904813912459044 ls: 12.198633298277855 te_ls: 0.3295312821865082 te_ac: 0.8756339581036384\n",
      "ep 188 tr_ac: 0.9106947828278585 ls: 11.83055393397808 te_ls: 0.348513662815094 te_ac: 0.8767364939360529\n",
      "ep 189 tr_ac: 0.9100226833571369 ls: 11.864731773734093 te_ls: 0.3562695384025574 te_ac: 0.8714443219404631\n",
      "ep 190 tr_ac: 0.9110308325632194 ls: 11.774131804704666 te_ls: 0.46305280923843384 te_ac: 0.8756339581036384\n",
      "ep 191 tr_ac: 0.9075023103419306 ls: 11.968432873487473 te_ls: 0.14545553922653198 te_ac: 0.8734288864388092\n",
      "ep 192 tr_ac: 0.9134671931445854 ls: 11.76736806333065 te_ls: 0.4673648774623871 te_ac: 0.8778390297684675\n",
      "ep 193 tr_ac: 0.9114508947324204 ls: 11.609520390629768 te_ls: 0.3350375294685364 te_ac: 0.8767364939360529\n",
      "ep 194 tr_ac: 0.9087624968495337 ls: 12.01680475473404 te_ls: 0.19229388236999512 te_ac: 0.8800441014332966\n",
      "ep 195 tr_ac: 0.913719230446106 ls: 11.429402828216553 te_ls: 0.22047041356563568 te_ac: 0.8756339581036384\n",
      "ep 196 tr_ac: 0.9135512055784256 ls: 11.423496052622795 te_ls: 0.5338913202285767 te_ac: 0.874090407938258\n",
      "ep 197 tr_ac: 0.912122994203142 ls: 11.366604521870613 te_ls: 0.10617782920598984 te_ac: 0.8745314222712238\n",
      "ep 198 tr_ac: 0.913719230446106 ls: 11.54587572813034 te_ls: 0.2606387138366699 te_ac: 0.8769570011025358\n",
      "ep 199 tr_ac: 0.9116189196001008 ls: 11.696745619177818 te_ls: 0.22460976243019104 te_ac: 0.8743109151047409\n",
      "ep 200 tr_ac: 0.9130471309753844 ls: 11.442665100097656 te_ls: 0.18856337666511536 te_ac: 0.8769570011025358\n",
      "ep 201 tr_ac: 0.9172477526673948 ls: 11.044394791126251 te_ls: 0.22466306388378143 te_ac: 0.8780595369349504\n",
      "ep 202 tr_ac: 0.9201881878518021 ls: 10.871266975998878 te_ls: 0.48646530508995056 te_ac: 0.8785005512679162\n",
      "ep 203 tr_ac: 0.9165756531966731 ls: 11.0334233045578 te_ls: 0.5130907893180847 te_ac: 0.8776185226019846\n",
      "ep 204 tr_ac: 0.9194320759472402 ls: 10.849844321608543 te_ls: 0.28521478176116943 te_ac: 0.8785005512679162\n",
      "ep 205 tr_ac: 0.9191800386457196 ls: 11.019960790872574 te_ls: 0.2946174740791321 te_ac: 0.87651598676957\n",
      "ep 206 tr_ac: 0.9171637402335545 ls: 10.791404411196709 te_ls: 0.16688814759254456 te_ac: 0.8782800441014333\n",
      "ep 207 tr_ac: 0.918171889439637 ls: 10.723457843065262 te_ls: 0.32048389315605164 te_ac: 0.8807056229327453\n",
      "ep 208 tr_ac: 0.9180878770057969 ls: 10.81841541826725 te_ls: 0.49824029207229614 te_ac: 0.8798235942668137\n",
      "ep 209 tr_ac: 0.9208602873225238 ls: 10.671232774853706 te_ls: 0.11678047478199005 te_ac: 0.8820286659316428\n",
      "ep 210 tr_ac: 0.917751827270436 ls: 10.837974309921265 te_ls: 0.335707426071167 te_ac: 0.881367144432194\n",
      "ep 211 tr_ac: 0.9158195412921112 ls: 10.884326040744781 te_ls: 0.4362671971321106 te_ac: 0.8807056229327453\n",
      "ep 212 tr_ac: 0.9188439889103587 ls: 10.661086812615395 te_ls: 0.22750383615493774 te_ac: 0.8831312017640573\n",
      "ep 213 tr_ac: 0.9158195412921112 ls: 10.84648086130619 te_ls: 0.2867604196071625 te_ac: 0.8809261300992283\n",
      "ep 214 tr_ac: 0.9174997899689153 ls: 10.82527619600296 te_ls: 0.17728692293167114 te_ac: 0.8798235942668137\n",
      "ep 215 tr_ac: 0.9159035537259514 ls: 10.701115787029266 te_ls: 0.23414604365825653 te_ac: 0.8787210584343991\n",
      "ep 216 tr_ac: 0.9195160883810805 ls: 10.824555903673172 te_ls: 0.4130997657775879 te_ac: 0.8787210584343991\n",
      "ep 217 tr_ac: 0.9203562127194825 ls: 10.94125247001648 te_ls: 0.3185802400112152 te_ac: 0.8800441014332966\n",
      "ep 218 tr_ac: 0.921784424094766 ls: 10.45025822520256 te_ls: 0.19160549342632294 te_ac: 0.8796030871003307\n",
      "ep 219 tr_ac: 0.9191800386457196 ls: 11.023064211010933 te_ls: 0.5232475399971008 te_ac: 0.8791620727673649\n",
      "ep 220 tr_ac: 0.9209442997563639 ls: 10.710654199123383 te_ls: 0.4498792588710785 te_ac: 0.8800441014332966\n",
      "ep 221 tr_ac: 0.9173317651012349 ls: 10.8966795951128 te_ls: 0.3580653965473175 te_ac: 0.8800441014332966\n",
      "ep 222 tr_ac: 0.9195160883810805 ls: 10.503493577241898 te_ls: 0.2544894814491272 te_ac: 0.8776185226019846\n",
      "ep 223 tr_ac: 0.9206082500210031 ls: 10.606456011533737 te_ls: 0.24020707607269287 te_ac: 0.8820286659316428\n",
      "ep 224 tr_ac: 0.9178358397042762 ls: 10.521264061331749 te_ls: 0.7325429916381836 te_ac: 0.8798235942668137\n",
      "ep 225 tr_ac: 0.9185919516088381 ls: 10.86877492070198 te_ls: 0.3478817641735077 te_ac: 0.8809261300992283\n",
      "ep 226 tr_ac: 0.9166596656305133 ls: 10.707041546702385 te_ls: 0.31594318151474 te_ac: 0.881367144432194\n",
      "ep 227 tr_ac: 0.9207762748886835 ls: 10.650924548506737 te_ls: 0.45008352398872375 te_ac: 0.8791620727673649\n",
      "ep 228 tr_ac: 0.9230446106023692 ls: 10.577624961733818 te_ls: 0.2558680772781372 te_ac: 0.8822491730981257\n",
      "ep 229 tr_ac: 0.9179198521381164 ls: 10.711343199014664 te_ls: 0.18824023008346558 te_ac: 0.8851157662624035\n",
      "ep 230 tr_ac: 0.9193480635134 ls: 10.708628669381142 te_ls: 0.1791910082101822 te_ac: 0.8802646085997795\n",
      "ep 231 tr_ac: 0.9207762748886835 ls: 10.306040972471237 te_ls: 0.3133947551250458 te_ac: 0.8837927232635061\n",
      "ep 232 tr_ac: 0.9193480635134 ls: 10.518928050994873 te_ls: 0.21188296377658844 te_ac: 0.8862183020948181\n",
      "ep 233 tr_ac: 0.9196001008149206 ls: 10.6923146545887 te_ls: 0.26001179218292236 te_ac: 0.8804851157662624\n",
      "ep 234 tr_ac: 0.9224565235654877 ls: 10.333158791065216 te_ls: 0.34599044919013977 te_ac: 0.878941565600882\n",
      "ep 235 tr_ac: 0.918171889439637 ls: 10.440958142280579 te_ls: 0.05267046019434929 te_ac: 0.8835722160970232\n",
      "ep 236 tr_ac: 0.9216163992270856 ls: 10.309663102030754 te_ls: 0.1632460057735443 te_ac: 0.8815876515986769\n",
      "ep 237 tr_ac: 0.919768125682601 ls: 10.687341511249542 te_ls: 0.5842216610908508 te_ac: 0.8807056229327453\n",
      "ep 238 tr_ac: 0.9211963370578846 ls: 10.413489922881126 te_ls: 0.2773003876209259 te_ac: 0.8802646085997795\n",
      "ep 239 tr_ac: 0.9210283121902042 ls: 10.244491547346115 te_ls: 0.12245043367147446 te_ac: 0.8829106945975744\n",
      "ep 240 tr_ac: 0.9194320759472402 ls: 10.548758625984192 te_ls: 0.23478423058986664 te_ac: 0.8820286659316428\n",
      "ep 241 tr_ac: 0.9194320759472402 ls: 10.386755183339119 te_ls: 0.31151849031448364 te_ac: 0.8826901874310915\n",
      "ep 242 tr_ac: 0.9237167100730909 ls: 10.301724925637245 te_ls: 0.39136803150177 te_ac: 0.8824696802646086\n",
      "ep 243 tr_ac: 0.9206082500210031 ls: 10.369994193315506 te_ls: 0.13154858350753784 te_ac: 0.8831312017640573\n",
      "ep 244 tr_ac: 0.923800722506931 ls: 10.32312273979187 te_ls: 0.39424341917037964 te_ac: 0.8800441014332966\n",
      "ep 245 tr_ac: 0.9261530706544568 ls: 9.927499637007713 te_ls: 0.3243969678878784 te_ac: 0.881367144432194\n",
      "ep 246 tr_ac: 0.9234646727715702 ls: 10.139163166284561 te_ls: 0.6416735053062439 te_ac: 0.8791620727673649\n",
      "ep 247 tr_ac: 0.9224565235654877 ls: 10.328647643327713 te_ls: 0.46895042061805725 te_ac: 0.8802646085997795\n",
      "ep 248 tr_ac: 0.923800722506931 ls: 9.997555777430534 te_ls: 0.1811186522245407 te_ac: 0.8815876515986769\n",
      "ep 249 tr_ac: 0.921364361925565 ls: 10.409261643886566 te_ls: 0.2744874060153961 te_ac: 0.8804851157662624\n",
      "ep 250 tr_ac: 0.9199361505502814 ls: 10.448725566267967 te_ls: 0.3189062476158142 te_ac: 0.8826901874310915\n",
      "ep 251 tr_ac: 0.9246408468453331 ls: 10.098638772964478 te_ls: 0.41614508628845215 te_ac: 0.8844542447629548\n",
      "ep 252 tr_ac: 0.92338066033773 ls: 10.257198676466942 te_ls: 0.34746524691581726 te_ac: 0.8848952590959206\n",
      "ep 253 tr_ac: 0.9191800386457196 ls: 10.411752179265022 te_ls: 0.43503841757774353 te_ac: 0.8833517089305403\n",
      "ep 254 tr_ac: 0.9252289338822146 ls: 9.930040776729584 te_ls: 0.3654136061668396 te_ac: 0.8815876515986769\n",
      "ep 255 tr_ac: 0.9235486852054104 ls: 10.011464238166809 te_ls: 0.34455275535583496 te_ac: 0.8820286659316428\n",
      "ep 256 tr_ac: 0.9239687473746114 ls: 10.095815122127533 te_ls: 0.1863923817873001 te_ac: 0.8815876515986769\n",
      "ep 257 tr_ac: 0.9228765857346888 ls: 10.132523760199547 te_ls: 0.2470468282699585 te_ac: 0.8842337375964718\n",
      "ep 258 tr_ac: 0.9241367722422918 ls: 10.091859892010689 te_ls: 0.21967752277851105 te_ac: 0.8833517089305403\n",
      "ep 259 tr_ac: 0.9226245484331681 ls: 10.202951654791832 te_ls: 0.4738380014896393 te_ac: 0.8862183020948181\n",
      "ep 260 tr_ac: 0.9241367722422918 ls: 10.038626700639725 te_ls: 0.20663127303123474 te_ac: 0.8829106945975744\n",
      "ep 261 tr_ac: 0.9243888095438125 ls: 9.971519440412521 te_ls: 0.09933388978242874 te_ac: 0.8802646085997795\n",
      "ep 262 tr_ac: 0.923800722506931 ls: 10.072263911366463 te_ls: 0.3025655150413513 te_ac: 0.8824696802646086\n",
      "ep 263 tr_ac: 0.9230446106023692 ls: 10.080318748950958 te_ls: 0.5881377458572388 te_ac: 0.8833517089305403\n",
      "ep 264 tr_ac: 0.9254809711837352 ls: 10.015893206000328 te_ls: 0.5126603245735168 te_ac: 0.8826901874310915\n",
      "ep 265 tr_ac: 0.9248928841468538 ls: 9.991729572415352 te_ls: 0.28299614787101746 te_ac: 0.8822491730981257\n",
      "ep 266 tr_ac: 0.9269931949928589 ls: 9.86701475083828 te_ls: 0.3752852976322174 te_ac: 0.8820286659316428\n",
      "ep 267 tr_ac: 0.9243888095438125 ls: 10.009080022573471 te_ls: 0.2520335614681244 te_ac: 0.8855567805953694\n",
      "ep 268 tr_ac: 0.925817020919096 ls: 9.873198986053467 te_ls: 0.46041345596313477 te_ac: 0.8837927232635061\n",
      "ep 269 tr_ac: 0.9242207846761321 ls: 9.62391710281372 te_ls: 0.313824862241745 te_ac: 0.8820286659316428\n",
      "ep 270 tr_ac: 0.9285054188019827 ls: 9.65833830833435 te_ls: 0.45071229338645935 te_ac: 0.8811466372657112\n",
      "ep 271 tr_ac: 0.9227085608670084 ls: 9.904649965465069 te_ls: 0.19545480608940125 te_ac: 0.8835722160970232\n",
      "ep 272 tr_ac: 0.921364361925565 ls: 10.054419293999672 te_ls: 0.514003574848175 te_ac: 0.8831312017640573\n",
      "ep 273 tr_ac: 0.9270772074266992 ls: 9.711424306035042 te_ls: 0.19340315461158752 te_ac: 0.8846747519294377\n",
      "ep 274 tr_ac: 0.9235486852054104 ls: 9.812245771288872 te_ls: 0.35416728258132935 te_ac: 0.8844542447629548\n",
      "ep 275 tr_ac: 0.9217004116609259 ls: 10.045801252126694 te_ls: 0.3784680962562561 te_ac: 0.8833517089305403\n",
      "ep 276 tr_ac: 0.9252289338822146 ls: 9.75309719145298 te_ls: 0.887755811214447 te_ac: 0.8815876515986769\n",
      "ep 277 tr_ac: 0.9281693690666218 ls: 9.671380296349525 te_ls: 0.2054339200258255 te_ac: 0.8853362734288864\n",
      "ep 278 tr_ac: 0.9265731328236579 ls: 9.720787480473518 te_ls: 0.2280261516571045 te_ac: 0.8855567805953694\n",
      "ep 279 tr_ac: 0.9290094934050239 ls: 9.64017729461193 te_ls: 0.22464875876903534 te_ac: 0.8829106945975744\n",
      "ep 280 tr_ac: 0.9256489960514156 ls: 9.68149022758007 te_ls: 0.27473184466362 te_ac: 0.8879823594266814\n",
      "ep 281 tr_ac: 0.9268251701251785 ls: 9.687909856438637 te_ls: 0.0933985710144043 te_ac: 0.8842337375964718\n",
      "ep 282 tr_ac: 0.9275812820297404 ls: 9.701301231980324 te_ls: 0.43669191002845764 te_ac: 0.8842337375964718\n",
      "ep 283 tr_ac: 0.9234646727715702 ls: 9.682862177491188 te_ls: 0.23518970608711243 te_ac: 0.8831312017640573\n",
      "ep 284 tr_ac: 0.9220364613962867 ls: 9.97379495203495 te_ls: 0.5768941640853882 te_ac: 0.8818081587651598\n",
      "ep 285 tr_ac: 0.9295135680080652 ls: 9.459568440914154 te_ls: 0.4458155632019043 te_ac: 0.8831312017640573\n",
      "ep 286 tr_ac: 0.9277493068974209 ls: 9.611564889550209 te_ls: 0.37371566891670227 te_ac: 0.8844542447629548\n",
      "ep 287 tr_ac: 0.9266571452574981 ls: 9.751793891191483 te_ls: 0.18907718360424042 te_ac: 0.8857772877618523\n",
      "ep 288 tr_ac: 0.9242207846761321 ls: 9.930337265133858 te_ls: 0.3143167197704315 te_ac: 0.881367144432194\n",
      "ep 289 tr_ac: 0.9297656053095859 ls: 9.460027024149895 te_ls: 0.255497008562088 te_ac: 0.8835722160970232\n",
      "ep 290 tr_ac: 0.9306897420818281 ls: 9.552784338593483 te_ls: 0.2905850112438202 te_ac: 0.8837927232635061\n",
      "ep 291 tr_ac: 0.9263210955221373 ls: 9.733503617346287 te_ls: 0.3224037289619446 te_ac: 0.886438809261301\n",
      "ep 292 tr_ac: 0.9312778291187096 ls: 9.475001372396946 te_ls: 0.20742912590503693 te_ac: 0.8859977949283352\n",
      "ep 293 tr_ac: 0.9259850457867764 ls: 9.671040147542953 te_ls: 0.1767418533563614 te_ac: 0.881367144432194\n",
      "ep 294 tr_ac: 0.9307737545156684 ls: 9.434437930583954 te_ls: 0.20296335220336914 te_ac: 0.8844542447629548\n",
      "ep 295 tr_ac: 0.9283373939343023 ls: 9.48454188555479 te_ls: 0.46431493759155273 te_ac: 0.8829106945975744\n",
      "ep 296 tr_ac: 0.9273292447282198 ls: 9.652567654848099 te_ls: 0.9788845777511597 te_ac: 0.8846747519294377\n",
      "ep 297 tr_ac: 0.9263210955221373 ls: 9.481476105749607 te_ls: 0.3751212954521179 te_ac: 0.8818081587651598\n",
      "ep 298 tr_ac: 0.929849617743426 ls: 9.435441635549068 te_ls: 0.304262638092041 te_ac: 0.8818081587651598\n",
      "ep 299 tr_ac: 0.9272452322943796 ls: 9.495625913143158 te_ls: 0.3209344446659088 te_ac: 0.881367144432194\n",
      "ep 300 tr_ac: 0.9274972695959002 ls: 9.462026253342628 te_ls: 0.26241031289100647 te_ac: 0.8871003307607497\n",
      "ep 301 tr_ac: 0.9285894312358229 ls: 9.326079897582531 te_ls: 0.23952968418598175 te_ac: 0.884013230429989\n",
      "ep 302 tr_ac: 0.9313618415525498 ls: 9.279116593301296 te_ls: 0.18967553973197937 te_ac: 0.8835722160970232\n",
      "ep 303 tr_ac: 0.9289254809711838 ls: 9.380200833082199 te_ls: 0.30927351117134094 te_ac: 0.8871003307607497\n",
      "ep 304 tr_ac: 0.9332941275308746 ls: 9.188666373491287 te_ls: 0.22505593299865723 te_ac: 0.8848952590959206\n",
      "ep 305 tr_ac: 0.9316138788540704 ls: 9.21825036406517 te_ls: 0.204037144780159 te_ac: 0.8873208379272326\n",
      "ep 306 tr_ac: 0.933882214567756 ls: 9.118612796068192 te_ls: 0.3197533190250397 te_ac: 0.8871003307607497\n",
      "ep 307 tr_ac: 0.9295975804419054 ls: 9.367218121886253 te_ls: 0.13122303783893585 te_ac: 0.8871003307607497\n",
      "ep 308 tr_ac: 0.9289254809711838 ls: 9.3534537255764 te_ls: 0.37307262420654297 te_ac: 0.8877618522601984\n",
      "ep 309 tr_ac: 0.9323699907586322 ls: 8.997114814817905 te_ls: 0.5241081118583679 te_ac: 0.8882028665931643\n",
      "ep 310 tr_ac: 0.9330420902293539 ls: 8.93930747359991 te_ls: 0.3460807800292969 te_ac: 0.8884233737596472\n",
      "ep 311 tr_ac: 0.93144585398639 ls: 9.214371711015701 te_ls: 0.578170895576477 te_ac: 0.8886438809261301\n",
      "ep 312 tr_ac: 0.9289254809711838 ls: 9.17040640115738 te_ls: 0.27116119861602783 te_ac: 0.8855567805953694\n",
      "ep 313 tr_ac: 0.9343022767369571 ls: 8.843914315104485 te_ls: 0.11534783244132996 te_ac: 0.8875413450937155\n",
      "ep 314 tr_ac: 0.9272452322943796 ls: 9.364072144031525 te_ls: 0.2892412841320038 te_ac: 0.8857772877618523\n",
      "ep 315 tr_ac: 0.931865916155591 ls: 8.962047219276428 te_ls: 0.31566059589385986 te_ac: 0.8868798235942668\n",
      "ep 316 tr_ac: 0.9290935058388642 ls: 9.307830050587654 te_ls: 0.49033409357070923 te_ac: 0.8871003307607497\n",
      "ep 317 tr_ac: 0.9286734436696631 ls: 9.411715656518936 te_ls: 0.23326851427555084 te_ac: 0.8857772877618523\n",
      "ep 318 tr_ac: 0.9337141897000756 ls: 9.120181486010551 te_ls: 0.33215081691741943 te_ac: 0.8859977949283352\n",
      "ep 319 tr_ac: 0.9301016550449467 ls: 9.327626645565033 te_ls: 0.44354310631752014 te_ac: 0.886659316427784\n",
      "ep 320 tr_ac: 0.9327060404939931 ls: 8.886516436934471 te_ls: 0.32257217168807983 te_ac: 0.8859977949283352\n",
      "ep 321 tr_ac: 0.9304377047803075 ls: 9.13792222738266 te_ls: 0.26315614581108093 te_ac: 0.886659316427784\n",
      "ep 322 tr_ac: 0.9319499285894313 ls: 9.021472960710526 te_ls: 0.35443028807640076 te_ac: 0.8837927232635061\n",
      "ep 323 tr_ac: 0.9316138788540704 ls: 9.220179505646229 te_ls: 0.37413203716278076 te_ac: 0.8868798235942668\n",
      "ep 324 tr_ac: 0.9343022767369571 ls: 8.790678389370441 te_ls: 0.3615601062774658 te_ac: 0.8877618522601984\n",
      "ep 325 tr_ac: 0.9346383264723179 ls: 8.754040867090225 te_ls: 0.10395532101392746 te_ac: 0.886659316427784\n",
      "ep 326 tr_ac: 0.9293455431403848 ls: 9.179241053760052 te_ls: 0.22749044001102448 te_ac: 0.8886438809261301\n",
      "ep 327 tr_ac: 0.9275812820297404 ls: 9.371767438948154 te_ls: 0.28929105401039124 te_ac: 0.8859977949283352\n",
      "ep 328 tr_ac: 0.9330420902293539 ls: 8.970795907080173 te_ls: 0.350801944732666 te_ac: 0.8857772877618523\n",
      "ep 329 tr_ac: 0.931865916155591 ls: 9.197003111243248 te_ls: 0.32435616850852966 te_ac: 0.886659316427784\n",
      "ep 330 tr_ac: 0.9323699907586322 ls: 9.133292824029922 te_ls: 0.29467225074768066 te_ac: 0.8875413450937155\n",
      "ep 331 tr_ac: 0.9312778291187096 ls: 9.108002722263336 te_ls: 0.43952250480651855 te_ac: 0.8846747519294377\n",
      "ep 332 tr_ac: 0.9275812820297404 ls: 9.273490726947784 te_ls: 0.2957412600517273 te_ac: 0.888864388092613\n",
      "ep 333 tr_ac: 0.9286734436696631 ls: 9.163368195295334 te_ls: 0.2129763811826706 te_ac: 0.886659316427784\n",
      "ep 334 tr_ac: 0.929429555574225 ls: 9.1884550973773 te_ls: 0.6367645263671875 te_ac: 0.8848952590959206\n",
      "ep 335 tr_ac: 0.9311098042510292 ls: 9.12476646900177 te_ls: 0.2085476815700531 te_ac: 0.8868798235942668\n",
      "ep 336 tr_ac: 0.9287574561035034 ls: 9.36316479742527 te_ls: 0.247575044631958 te_ac: 0.8871003307607497\n",
      "ep 337 tr_ac: 0.9299336301772663 ls: 9.190327122807503 te_ls: 0.2911140024662018 te_ac: 0.884013230429989\n",
      "ep 338 tr_ac: 0.9313618415525498 ls: 8.926476672291756 te_ls: 0.3584761619567871 te_ac: 0.8893054024255789\n",
      "ep 339 tr_ac: 0.9316138788540704 ls: 9.255570985376835 te_ls: 0.41584309935569763 te_ac: 0.8862183020948181\n",
      "ep 340 tr_ac: 0.9329580777955138 ls: 8.849633276462555 te_ls: 0.42901891469955444 te_ac: 0.8873208379272326\n",
      "ep 341 tr_ac: 0.9347223389061581 ls: 8.796789579093456 te_ls: 0.3467418849468231 te_ac: 0.8837927232635061\n",
      "ep 342 tr_ac: 0.9336301772662354 ls: 8.711528904736042 te_ls: 0.39178401231765747 te_ac: 0.888864388092613\n",
      "ep 343 tr_ac: 0.9319499285894313 ls: 8.968543469905853 te_ls: 0.37379521131515503 te_ac: 0.8899669239250275\n",
      "ep 344 tr_ac: 0.93547845081072 ls: 8.95904915034771 te_ls: 0.24895599484443665 te_ac: 0.8857772877618523\n",
      "ep 345 tr_ac: 0.9332101150970343 ls: 8.864485882222652 te_ls: 0.21290916204452515 te_ac: 0.8871003307607497\n",
      "ep 346 tr_ac: 0.93547845081072 ls: 8.692179962992668 te_ls: 0.18400080502033234 te_ac: 0.8877618522601984\n",
      "ep 347 tr_ac: 0.9321179534571117 ls: 9.065967135131359 te_ls: 0.31376954913139343 te_ac: 0.886438809261301\n",
      "ep 348 tr_ac: 0.9316978912879106 ls: 8.89015044271946 te_ls: 0.17940247058868408 te_ac: 0.8904079382579934\n",
      "ep 349 tr_ac: 0.9324540031924725 ls: 8.895065397024155 te_ls: 0.28555941581726074 te_ac: 0.8848952590959206\n",
      "ep 350 tr_ac: 0.9328740653616735 ls: 8.841271176934242 te_ls: 0.10520372539758682 te_ac: 0.8875413450937155\n",
      "ep 351 tr_ac: 0.9299336301772663 ls: 9.126453399658203 te_ls: 0.3325831890106201 te_ac: 0.8895259095920618\n",
      "ep 352 tr_ac: 0.9311938166848693 ls: 9.18070600926876 te_ls: 0.3891502618789673 te_ac: 0.886659316427784\n",
      "ep 353 tr_ac: 0.933462152398555 ls: 8.8866171464324 te_ls: 0.16960322856903076 te_ac: 0.8875413450937155\n",
      "ep 354 tr_ac: 0.9359825254137613 ls: 8.722273364663124 te_ls: 0.3579186499118805 te_ac: 0.8875413450937155\n",
      "ep 355 tr_ac: 0.9303536923464673 ls: 8.957356914877892 te_ls: 0.7366511821746826 te_ac: 0.8901874310915104\n",
      "ep 356 tr_ac: 0.9329580777955138 ls: 8.868465021252632 te_ls: 0.3192313611507416 te_ac: 0.889084895259096\n",
      "ep 357 tr_ac: 0.9353944383768797 ls: 8.823661908507347 te_ls: 0.37451255321502686 te_ac: 0.8893054024255789\n",
      "ep 358 tr_ac: 0.9287574561035034 ls: 8.882431581616402 te_ls: 0.4832395315170288 te_ac: 0.8875413450937155\n",
      "ep 359 tr_ac: 0.9332941275308746 ls: 8.911744609475136 te_ls: 0.33153194189071655 te_ac: 0.8868798235942668\n",
      "ep 360 tr_ac: 0.9341342518692767 ls: 8.67500576376915 te_ls: 0.18179446458816528 te_ac: 0.888864388092613\n",
      "ep 361 tr_ac: 0.93144585398639 ls: 8.94639490544796 te_ls: 0.32370850443840027 te_ac: 0.884013230429989\n",
      "ep 362 tr_ac: 0.9308577669495085 ls: 8.84914942085743 te_ls: 0.2232639044523239 te_ac: 0.8873208379272326\n",
      "ep 363 tr_ac: 0.9341342518692767 ls: 8.716117590665817 te_ls: 0.39517155289649963 te_ac: 0.8871003307607497\n",
      "ep 364 tr_ac: 0.93547845081072 ls: 8.772994600236416 te_ls: 0.4657578468322754 te_ac: 0.8899669239250275\n",
      "ep 365 tr_ac: 0.9333781399647147 ls: 8.945925325155258 te_ls: 0.12018220126628876 te_ac: 0.8873208379272326\n",
      "ep 366 tr_ac: 0.9341342518692767 ls: 8.844573631882668 te_ls: 0.15230059623718262 te_ac: 0.8895259095920618\n",
      "ep 367 tr_ac: 0.9362345627152818 ls: 8.822375111281872 te_ls: 0.391611248254776 te_ac: 0.8859977949283352\n",
      "ep 368 tr_ac: 0.9322019658909518 ls: 8.944183476269245 te_ls: 0.13778091967105865 te_ac: 0.8873208379272326\n",
      "ep 369 tr_ac: 0.9328740653616735 ls: 8.836878344416618 te_ls: 0.39608606696128845 te_ac: 0.8871003307607497\n",
      "ep 370 tr_ac: 0.9343862891707972 ls: 8.790818303823471 te_ls: 0.28112518787384033 te_ac: 0.8897464167585446\n",
      "ep 371 tr_ac: 0.9327900529278333 ls: 8.93340265750885 te_ls: 0.28172409534454346 te_ac: 0.8875413450937155\n",
      "ep 372 tr_ac: 0.9339662270015963 ls: 8.762138426303864 te_ls: 0.15213237702846527 te_ac: 0.8879823594266814\n",
      "ep 373 tr_ac: 0.9381668486936067 ls: 8.557584956288338 te_ls: 0.22015568614006042 te_ac: 0.889084895259096\n",
      "ep 374 tr_ac: 0.9360665378476014 ls: 8.56897947192192 te_ls: 0.6253166198730469 te_ac: 0.8862183020948181\n",
      "ep 375 tr_ac: 0.9356464756784004 ls: 8.64280430227518 te_ls: 0.3870181739330292 te_ac: 0.8875413450937155\n",
      "ep 376 tr_ac: 0.931865916155591 ls: 8.608163267374039 te_ls: 0.2478047013282776 te_ac: 0.8862183020948181\n",
      "ep 377 tr_ac: 0.9366546248844829 ls: 8.54125565290451 te_ls: 0.1560208797454834 te_ac: 0.889084895259096\n",
      "ep 378 tr_ac: 0.9344703016046375 ls: 8.708697602152824 te_ls: 0.39460885524749756 te_ac: 0.8873208379272326\n",
      "ep 379 tr_ac: 0.9369066621860035 ls: 8.493659272789955 te_ls: 0.22190579771995544 te_ac: 0.8871003307607497\n",
      "ep 380 tr_ac: 0.9348903637738385 ls: 8.654480062425137 te_ls: 0.33202260732650757 te_ac: 0.8857772877618523\n",
      "ep 381 tr_ac: 0.935898512979921 ls: 8.67777270078659 te_ls: 0.12762762606143951 te_ac: 0.886438809261301\n",
      "ep 382 tr_ac: 0.9322859783247921 ls: 8.701805502176285 te_ls: 0.17271283268928528 te_ac: 0.8829106945975744\n",
      "ep 383 tr_ac: 0.9341342518692767 ls: 8.69159571826458 te_ls: 0.340336412191391 te_ac: 0.8862183020948181\n",
      "ep 384 tr_ac: 0.9342182643031168 ls: 8.65574823319912 te_ls: 0.40716129541397095 te_ac: 0.8875413450937155\n",
      "ep 385 tr_ac: 0.9325380156263127 ls: 8.738283909857273 te_ls: 0.3503817319869995 te_ac: 0.886659316427784\n",
      "ep 386 tr_ac: 0.9390069730320088 ls: 8.541833534836769 te_ls: 1.0133146047592163 te_ac: 0.8859977949283352\n",
      "ep 387 tr_ac: 0.9328740653616735 ls: 8.655612960457802 te_ls: 0.08842440694570541 te_ac: 0.8871003307607497\n",
      "ep 388 tr_ac: 0.9373267243552046 ls: 8.48068905621767 te_ls: 0.4124261438846588 te_ac: 0.8884233737596472\n",
      "ep 389 tr_ac: 0.9364025875829622 ls: 8.428731769323349 te_ls: 0.6512918472290039 te_ac: 0.8886438809261301\n",
      "ep 390 tr_ac: 0.9365706124506427 ls: 8.643551290035248 te_ls: 0.16935117542743683 te_ac: 0.8879823594266814\n",
      "ep 391 tr_ac: 0.9369906746198438 ls: 8.414199657738209 te_ls: 0.34057652950286865 te_ac: 0.8882028665931643\n",
      "ep 392 tr_ac: 0.9337982021339158 ls: 8.582366853952408 te_ls: 0.23507444560527802 te_ac: 0.8884233737596472\n",
      "ep 393 tr_ac: 0.9350583886415189 ls: 8.714331313967705 te_ls: 0.26611512899398804 te_ac: 0.8882028665931643\n",
      "ep 394 tr_ac: 0.9343862891707972 ls: 8.772132009267807 te_ls: 0.30120620131492615 te_ac: 0.886438809261301\n",
      "ep 395 tr_ac: 0.933462152398555 ls: 8.738972842693329 te_ls: 0.23008494079113007 te_ac: 0.8862183020948181\n",
      "ep 396 tr_ac: 0.9386709232966479 ls: 8.484950885176659 te_ls: 0.527788519859314 te_ac: 0.8877618522601984\n",
      "ep 397 tr_ac: 0.9362345627152818 ls: 8.673284217715263 te_ls: 0.26973459124565125 te_ac: 0.8859977949283352\n",
      "ep 398 tr_ac: 0.9341342518692767 ls: 8.53733941912651 te_ls: 0.5044485330581665 te_ac: 0.8901874310915104\n",
      "ep 399 tr_ac: 0.9376627740905654 ls: 8.276747092604637 te_ls: 0.2658495604991913 te_ac: 0.8910694597574421\n",
      "ep 400 tr_ac: 0.9343862891707972 ls: 8.58784917742014 te_ls: 0.07598306983709335 te_ac: 0.888864388092613\n",
      "ep 401 tr_ac: 0.9390909854658489 ls: 8.41763436794281 te_ls: 0.20448945462703705 te_ac: 0.8886438809261301\n",
      "ep 402 tr_ac: 0.9342182643031168 ls: 8.380684077739716 te_ls: 0.24742770195007324 te_ac: 0.8899669239250275\n",
      "ep 403 tr_ac: 0.9347223389061581 ls: 8.54049727320671 te_ls: 0.054544832557439804 te_ac: 0.8884233737596472\n",
      "ep 404 tr_ac: 0.9349743762076788 ls: 8.554340355098248 te_ls: 0.21611709892749786 te_ac: 0.8875413450937155\n",
      "ep 405 tr_ac: 0.937494749222885 ls: 8.496395960450172 te_ls: 0.1374291628599167 te_ac: 0.889084895259096\n",
      "ep 406 tr_ac: 0.9365706124506427 ls: 8.59233096241951 te_ls: 0.16815117001533508 te_ac: 0.8899669239250275\n",
      "ep 407 tr_ac: 0.9389229605981685 ls: 8.404147483408451 te_ls: 0.22732587158679962 te_ac: 0.8871003307607497\n",
      "ep 408 tr_ac: 0.9360665378476014 ls: 8.620804034173489 te_ls: 0.2867582440376282 te_ac: 0.886438809261301\n",
      "ep 409 tr_ac: 0.9349743762076788 ls: 8.593520104885101 te_ls: 0.3530249297618866 te_ac: 0.8904079382579934\n",
      "ep 410 tr_ac: 0.9379988238259263 ls: 8.434251837432384 te_ls: 0.4966629147529602 te_ac: 0.886438809261301\n",
      "ep 411 tr_ac: 0.9346383264723179 ls: 8.480081491172314 te_ls: 0.07507796585559845 te_ac: 0.8886438809261301\n",
      "ep 412 tr_ac: 0.9376627740905654 ls: 8.370198518037796 te_ls: 0.3617422580718994 te_ac: 0.8882028665931643\n",
      "ep 413 tr_ac: 0.9361505502814417 ls: 8.40017756074667 te_ls: 0.32376059889793396 te_ac: 0.8877618522601984\n",
      "ep 414 tr_ac: 0.9343862891707972 ls: 8.524003088474274 te_ls: 0.3918795585632324 te_ac: 0.8886438809261301\n",
      "ep 415 tr_ac: 0.9371586994875242 ls: 8.340456008911133 te_ls: 0.3037723898887634 te_ac: 0.888864388092613\n",
      "ep 416 tr_ac: 0.933882214567756 ls: 8.41690193861723 te_ls: 0.48943814635276794 te_ac: 0.8877618522601984\n",
      "ep 417 tr_ac: 0.9396790725027304 ls: 8.295730024576187 te_ls: 0.494841605424881 te_ac: 0.889084895259096\n",
      "ep 418 tr_ac: 0.9368226497521633 ls: 8.49669624865055 te_ls: 0.20727622509002686 te_ac: 0.8875413450937155\n",
      "ep 419 tr_ac: 0.9373267243552046 ls: 8.338349491357803 te_ls: 0.43769627809524536 te_ac: 0.8886438809261301\n",
      "ep 420 tr_ac: 0.9375787616567252 ls: 8.292454563081264 te_ls: 0.1516154408454895 te_ac: 0.8868798235942668\n",
      "ep 421 tr_ac: 0.9384188859951272 ls: 8.301033653318882 te_ls: 0.1284627616405487 te_ac: 0.8879823594266814\n",
      "ep 422 tr_ac: 0.9360665378476014 ls: 8.487835958600044 te_ls: 0.18781155347824097 te_ac: 0.8886438809261301\n",
      "ep 423 tr_ac: 0.9379988238259263 ls: 8.339152999222279 te_ls: 0.18490269780158997 te_ac: 0.888864388092613\n",
      "ep 424 tr_ac: 0.9391749978996892 ls: 8.231908053159714 te_ls: 0.2281758040189743 te_ac: 0.8893054024255789\n",
      "ep 425 tr_ac: 0.9383348735612871 ls: 8.143463782966137 te_ls: 0.3630977272987366 te_ac: 0.8886438809261301\n",
      "ep 426 tr_ac: 0.9369906746198438 ls: 8.338974140584469 te_ls: 0.2318604290485382 te_ac: 0.8884233737596472\n",
      "ep 427 tr_ac: 0.9363185751491221 ls: 8.22509914636612 te_ls: 0.3721143901348114 te_ac: 0.8904079382579934\n",
      "ep 428 tr_ac: 0.937494749222885 ls: 8.34287216514349 te_ls: 0.33257731795310974 te_ac: 0.8877618522601984\n",
      "ep 429 tr_ac: 0.9367386373183231 ls: 8.285238161683083 te_ls: 0.38137078285217285 te_ac: 0.8875413450937155\n",
      "ep 430 tr_ac: 0.9364866000168025 ls: 8.531126163899899 te_ls: 0.2188214510679245 te_ac: 0.8906284454244763\n",
      "ep 431 tr_ac: 0.9368226497521633 ls: 8.591652572154999 te_ls: 0.5773148536682129 te_ac: 0.8904079382579934\n",
      "ep 432 tr_ac: 0.9393430227673696 ls: 8.250281304121017 te_ls: 0.36565661430358887 te_ac: 0.8875413450937155\n",
      "ep 433 tr_ac: 0.9356464756784004 ls: 8.242182411253452 te_ls: 0.15710711479187012 te_ac: 0.8893054024255789\n",
      "ep 434 tr_ac: 0.9400991346719314 ls: 8.2532689422369 te_ls: 0.12119320780038834 te_ac: 0.8904079382579934\n",
      "ep 435 tr_ac: 0.9375787616567252 ls: 8.254320412874222 te_ls: 0.3889676332473755 te_ac: 0.8904079382579934\n",
      "ep 436 tr_ac: 0.9398470973704108 ls: 8.302520409226418 te_ls: 0.2883654534816742 te_ac: 0.889084895259096\n",
      "ep 437 tr_ac: 0.9390069730320088 ls: 8.072447806596756 te_ls: 0.1257738322019577 te_ac: 0.8908489525909592\n",
      "ep 438 tr_ac: 0.937914811392086 ls: 8.477547012269497 te_ls: 0.26568588614463806 te_ac: 0.8884233737596472\n",
      "ep 439 tr_ac: 0.9348063513399983 ls: 8.635647490620613 te_ls: 0.4694841802120209 te_ac: 0.8873208379272326\n",
      "ep 440 tr_ac: 0.9382508611274468 ls: 8.247311808168888 te_ls: 0.2042299211025238 te_ac: 0.8879823594266814\n",
      "ep 441 tr_ac: 0.9349743762076788 ls: 8.643970362842083 te_ls: 0.20464058220386505 te_ac: 0.891510474090408\n",
      "ep 442 tr_ac: 0.937914811392086 ls: 8.283257022500038 te_ls: 0.48605313897132874 te_ac: 0.889084895259096\n",
      "ep 443 tr_ac: 0.9358145005460808 ls: 8.221950829029083 te_ls: 0.26213905215263367 te_ac: 0.889084895259096\n",
      "ep 444 tr_ac: 0.9371586994875242 ls: 8.391063772141933 te_ls: 0.5254583358764648 te_ac: 0.889084895259096\n",
      "ep 445 tr_ac: 0.9373267243552046 ls: 8.231724932789803 te_ls: 0.3487323820590973 te_ac: 0.8901874310915104\n",
      "ep 446 tr_ac: 0.9414433336133747 ls: 8.145322434604168 te_ls: 0.2548028528690338 te_ac: 0.8906284454244763\n",
      "ep 447 tr_ac: 0.9379988238259263 ls: 8.371687516570091 te_ls: 0.32091665267944336 te_ac: 0.8901874310915104\n",
      "ep 448 tr_ac: 0.9376627740905654 ls: 8.367534041404724 te_ls: 0.23206105828285217 te_ac: 0.8886438809261301\n",
      "ep 449 tr_ac: 0.9365706124506427 ls: 8.366278693079948 te_ls: 0.6218258142471313 te_ac: 0.8879823594266814\n",
      "ep 450 tr_ac: 0.9389229605981685 ls: 8.288588061928749 te_ls: 0.3045737147331238 te_ac: 0.8893054024255789\n",
      "ep 451 tr_ac: 0.9365706124506427 ls: 8.317345291376114 te_ls: 0.2699946165084839 te_ac: 0.8904079382579934\n",
      "ep 452 tr_ac: 0.9383348735612871 ls: 8.296773739159107 te_ls: 0.6214717030525208 te_ac: 0.8871003307607497\n",
      "ep 453 tr_ac: 0.9381668486936067 ls: 8.270188063383102 te_ls: 0.1303795427083969 te_ac: 0.8884233737596472\n",
      "ep 454 tr_ac: 0.9367386373183231 ls: 8.432683929800987 te_ls: 0.44246676564216614 te_ac: 0.8893054024255789\n",
      "ep 455 tr_ac: 0.9381668486936067 ls: 8.177827529609203 te_ls: 0.44388484954833984 te_ac: 0.888864388092613\n",
      "ep 456 tr_ac: 0.9352264135091993 ls: 8.419434063136578 te_ls: 0.2851394712924957 te_ac: 0.8906284454244763\n",
      "ep 457 tr_ac: 0.9360665378476014 ls: 8.457584574818611 te_ls: 0.7001725435256958 te_ac: 0.8886438809261301\n",
      "ep 458 tr_ac: 0.9366546248844829 ls: 8.289544098079205 te_ls: 0.8288483619689941 te_ac: 0.8882028665931643\n",
      "ep 459 tr_ac: 0.9353944383768797 ls: 8.274281114339828 te_ls: 0.2643580138683319 te_ac: 0.8873208379272326\n",
      "ep 460 tr_ac: 0.9375787616567252 ls: 8.331582695245743 te_ls: 0.4630793035030365 te_ac: 0.8882028665931643\n",
      "ep 461 tr_ac: 0.9366546248844829 ls: 8.62557628750801 te_ls: 0.3215848505496979 te_ac: 0.8877618522601984\n",
      "ep 462 tr_ac: 0.937494749222885 ls: 8.162285923957825 te_ls: 0.3423839211463928 te_ac: 0.8873208379272326\n",
      "ep 463 tr_ac: 0.9374107367890447 ls: 8.506033763289452 te_ls: 0.5588567852973938 te_ac: 0.8906284454244763\n",
      "ep 464 tr_ac: 0.9397630849365706 ls: 8.141314752399921 te_ls: 0.3616846799850464 te_ac: 0.8893054024255789\n",
      "ep 465 tr_ac: 0.93951104763505 ls: 8.043791398406029 te_ls: 0.17142464220523834 te_ac: 0.891289966923925\n",
      "ep 466 tr_ac: 0.93951104763505 ls: 8.272934429347515 te_ls: 0.11861202865839005 te_ac: 0.8884233737596472\n",
      "ep 467 tr_ac: 0.9383348735612871 ls: 8.117311805486679 te_ls: 0.2853424549102783 te_ac: 0.8886438809261301\n",
      "ep 468 tr_ac: 0.9374107367890447 ls: 8.347689092159271 te_ls: 0.5997275114059448 te_ac: 0.8886438809261301\n",
      "ep 469 tr_ac: 0.9367386373183231 ls: 8.129539385437965 te_ls: 0.3767518103122711 te_ac: 0.888864388092613\n",
      "ep 470 tr_ac: 0.937494749222885 ls: 8.371551468968391 te_ls: 0.11556645482778549 te_ac: 0.8895259095920618\n",
      "ep 471 tr_ac: 0.9381668486936067 ls: 8.375505149364471 te_ls: 0.1798993945121765 te_ac: 0.888864388092613\n",
      "ep 472 tr_ac: 0.9377467865244056 ls: 8.129072606563568 te_ls: 0.2290191948413849 te_ac: 0.8893054024255789\n",
      "ep 473 tr_ac: 0.9376627740905654 ls: 8.21725980937481 te_ls: 0.16085511445999146 te_ac: 0.8895259095920618\n",
      "ep 474 tr_ac: 0.9388389481643283 ls: 8.225086331367493 te_ls: 0.35928088426589966 te_ac: 0.8899669239250275\n",
      "ep 475 tr_ac: 0.9359825254137613 ls: 8.242824219167233 te_ls: 0.31206363439559937 te_ac: 0.8893054024255789\n",
      "ep 476 tr_ac: 0.9387549357304881 ls: 8.095185920596123 te_ls: 0.44649186730384827 te_ac: 0.8877618522601984\n",
      "ep 477 tr_ac: 0.9369906746198438 ls: 8.334586054086685 te_ls: 0.232929527759552 te_ac: 0.8886438809261301\n",
      "ep 478 tr_ac: 0.9377467865244056 ls: 8.168389692902565 te_ls: 0.48781612515449524 te_ac: 0.888864388092613\n",
      "ep 479 tr_ac: 0.9390069730320088 ls: 8.17867486178875 te_ls: 0.09793995320796967 te_ac: 0.8895259095920618\n",
      "ep 480 tr_ac: 0.9374107367890447 ls: 8.298519603908062 te_ls: 0.270731657743454 te_ac: 0.889084895259096\n",
      "ep 481 tr_ac: 0.9400991346719314 ls: 7.998034782707691 te_ls: 0.40041452646255493 te_ac: 0.888864388092613\n",
      "ep 482 tr_ac: 0.9387549357304881 ls: 8.160567797720432 te_ls: 0.11793011426925659 te_ac: 0.8901874310915104\n",
      "ep 483 tr_ac: 0.9379988238259263 ls: 8.055425822734833 te_ls: 0.562928318977356 te_ac: 0.8904079382579934\n",
      "ep 484 tr_ac: 0.9397630849365706 ls: 8.05761131644249 te_ls: 0.07230998575687408 te_ac: 0.8882028665931643\n",
      "ep 485 tr_ac: 0.9387549357304881 ls: 8.292099677026272 te_ls: 0.1275862604379654 te_ac: 0.8908489525909592\n",
      "ep 486 tr_ac: 0.9364866000168025 ls: 8.439870998263359 te_ls: 0.389311283826828 te_ac: 0.8901874310915104\n",
      "ep 487 tr_ac: 0.9412753087456943 ls: 8.200680255889893 te_ls: 0.15543366968631744 te_ac: 0.8886438809261301\n",
      "ep 488 tr_ac: 0.9364025875829622 ls: 8.29696387797594 te_ls: 0.5403732061386108 te_ac: 0.8897464167585446\n",
      "ep 489 tr_ac: 0.9381668486936067 ls: 8.394308187067509 te_ls: 0.16613107919692993 te_ac: 0.8901874310915104\n",
      "ep 490 tr_ac: 0.9368226497521633 ls: 8.225655123591423 te_ls: 0.21892979741096497 te_ac: 0.8904079382579934\n",
      "ep 491 tr_ac: 0.9410232714441737 ls: 8.088117226958275 te_ls: 0.3388517498970032 te_ac: 0.8904079382579934\n",
      "ep 492 tr_ac: 0.9391749978996892 ls: 8.167872041463852 te_ls: 0.09360753744840622 te_ac: 0.8862183020948181\n",
      "ep 493 tr_ac: 0.9386709232966479 ls: 8.3280578404665 te_ls: 0.12001213431358337 te_ac: 0.8901874310915104\n",
      "ep 494 tr_ac: 0.9393430227673696 ls: 8.171383239328861 te_ls: 0.6443938612937927 te_ac: 0.8895259095920618\n",
      "ep 495 tr_ac: 0.9416953709148954 ls: 8.014989621937275 te_ls: 0.3558088541030884 te_ac: 0.8895259095920618\n",
      "ep 496 tr_ac: 0.9387549357304881 ls: 7.90896138548851 te_ls: 0.4565517008304596 te_ac: 0.8899669239250275\n",
      "ep 497 tr_ac: 0.9374107367890447 ls: 8.039822742342949 te_ls: 0.2532137632369995 te_ac: 0.8875413450937155\n",
      "ep 498 tr_ac: 0.9406032092749727 ls: 7.908032588660717 te_ls: 0.40693399310112 te_ac: 0.8906284454244763\n",
      "ep 499 tr_ac: 0.9411072838780139 ls: 7.99384418129921 te_ls: 0.12480859458446503 te_ac: 0.888864388092613\n",
      "ep 500 tr_ac: 0.9379988238259263 ls: 8.279860720038414 te_ls: 0.13038867712020874 te_ac: 0.8895259095920618\n",
      "ep 501 tr_ac: 0.9412753087456943 ls: 7.976040191948414 te_ls: 0.23932236433029175 te_ac: 0.8895259095920618\n",
      "ep 502 tr_ac: 0.9405191968411325 ls: 7.9519005343317986 te_ls: 0.19748780131340027 te_ac: 0.889084895259096\n",
      "ep 503 tr_ac: 0.9386709232966479 ls: 8.026440531015396 te_ls: 0.3276423513889313 te_ac: 0.888864388092613\n",
      "ep 504 tr_ac: 0.9410232714441737 ls: 7.9039401933550835 te_ls: 0.23297661542892456 te_ac: 0.889084895259096\n",
      "ep 505 tr_ac: 0.9437116693270604 ls: 7.775478884577751 te_ls: 0.43839001655578613 te_ac: 0.8875413450937155\n",
      "ep 506 tr_ac: 0.9391749978996892 ls: 8.176887661218643 te_ls: 0.3637005388736725 te_ac: 0.8893054024255789\n",
      "ep 507 tr_ac: 0.9382508611274468 ls: 8.166214473545551 te_ls: 0.45382851362228394 te_ac: 0.889084895259096\n",
      "ep 508 tr_ac: 0.9406872217088129 ls: 7.968969970941544 te_ls: 0.34862077236175537 te_ac: 0.889084895259096\n",
      "ep 509 tr_ac: 0.9408552465764933 ls: 7.941877879202366 te_ls: 0.7288974523544312 te_ac: 0.8899669239250275\n",
      "ep 510 tr_ac: 0.9401831471057717 ls: 7.867905505001545 te_ls: 0.22170206904411316 te_ac: 0.8886438809261301\n",
      "ep 511 tr_ac: 0.9363185751491221 ls: 8.412382572889328 te_ls: 0.46329355239868164 te_ac: 0.8908489525909592\n",
      "ep 512 tr_ac: 0.9395950600688902 ls: 8.26890541613102 te_ls: 0.2759254574775696 te_ac: 0.888864388092613\n",
      "ep 513 tr_ac: 0.9385869108628077 ls: 8.122376054525375 te_ls: 0.40169933438301086 te_ac: 0.888864388092613\n",
      "ep 514 tr_ac: 0.9423674703856171 ls: 7.822934716939926 te_ls: 0.2758455276489258 te_ac: 0.8895259095920618\n",
      "ep 515 tr_ac: 0.9387549357304881 ls: 8.05955634266138 te_ls: 0.10994322597980499 te_ac: 0.8904079382579934\n",
      "ep 516 tr_ac: 0.937914811392086 ls: 8.298902705311775 te_ls: 0.43619832396507263 te_ac: 0.8895259095920618\n",
      "ep 517 tr_ac: 0.9400151222380912 ls: 8.067903831601143 te_ls: 0.2560632526874542 te_ac: 0.8895259095920618\n",
      "ep 518 tr_ac: 0.9406032092749727 ls: 8.1027155444026 te_ls: 0.3742338716983795 te_ac: 0.8901874310915104\n",
      "ep 519 tr_ac: 0.9407712341426531 ls: 8.071343801915646 te_ls: 0.19457656145095825 te_ac: 0.8886438809261301\n",
      "ep 520 tr_ac: 0.9410232714441737 ls: 7.905799314379692 te_ls: 0.35834577679634094 te_ac: 0.8886438809261301\n",
      "ep 521 tr_ac: 0.9396790725027304 ls: 8.078481212258339 te_ls: 0.17308920621871948 te_ac: 0.8893054024255789\n",
      "ep 522 tr_ac: 0.9433756195916996 ls: 7.699916690587997 te_ls: 0.5974834561347961 te_ac: 0.8904079382579934\n",
      "ep 523 tr_ac: 0.937914811392086 ls: 8.272853396832943 te_ls: 0.2227170169353485 te_ac: 0.8901874310915104\n",
      "ep 524 tr_ac: 0.9396790725027304 ls: 7.978889726102352 te_ls: 0.12929920852184296 te_ac: 0.8906284454244763\n",
      "ep 525 tr_ac: 0.9411072838780139 ls: 7.870587408542633 te_ls: 0.4695552885532379 te_ac: 0.8895259095920618\n",
      "ep 526 tr_ac: 0.9382508611274468 ls: 8.11194035410881 te_ls: 0.3634742796421051 te_ac: 0.8893054024255789\n",
      "ep 527 tr_ac: 0.9400991346719314 ls: 7.880038395524025 te_ls: 0.2527223229408264 te_ac: 0.8904079382579934\n",
      "ep 528 tr_ac: 0.9400991346719314 ls: 8.138637006282806 te_ls: 0.19757969677448273 te_ac: 0.8882028665931643\n",
      "ep 529 tr_ac: 0.9395950600688902 ls: 8.159971587359905 te_ls: 0.35511958599090576 te_ac: 0.8895259095920618\n",
      "ep 530 tr_ac: 0.9382508611274468 ls: 8.233698315918446 te_ls: 0.32558512687683105 te_ac: 0.889084895259096\n",
      "ep 531 tr_ac: 0.9401831471057717 ls: 7.978115051984787 te_ls: 0.1614626944065094 te_ac: 0.8882028665931643\n",
      "ep 532 tr_ac: 0.9422834579517768 ls: 7.882231950759888 te_ls: 0.3564971387386322 te_ac: 0.888864388092613\n",
      "ep 533 tr_ac: 0.9372427119213643 ls: 8.21833810210228 te_ls: 0.5124183297157288 te_ac: 0.8893054024255789\n",
      "ep 534 tr_ac: 0.9397630849365706 ls: 7.99327078461647 te_ls: 0.5837704539299011 te_ac: 0.8908489525909592\n",
      "ep 535 tr_ac: 0.9421994455179367 ls: 7.766585156321526 te_ls: 0.34442687034606934 te_ac: 0.8895259095920618\n",
      "ep 536 tr_ac: 0.939931109804251 ls: 7.873119004070759 te_ls: 0.26247459650039673 te_ac: 0.8910694597574421\n",
      "ep 537 tr_ac: 0.9403511719734521 ls: 8.016971051692963 te_ls: 0.1240554228425026 te_ac: 0.8899669239250275\n",
      "ep 538 tr_ac: 0.9384188859951272 ls: 8.267784610390663 te_ls: 0.21639347076416016 te_ac: 0.8895259095920618\n",
      "ep 539 tr_ac: 0.9407712341426531 ls: 8.021146520972252 te_ls: 0.2785262167453766 te_ac: 0.8893054024255789\n",
      "ep 540 tr_ac: 0.9416113584810551 ls: 7.839665308594704 te_ls: 0.536676824092865 te_ac: 0.8895259095920618\n",
      "ep 541 tr_ac: 0.9421154330840964 ls: 7.934512622654438 te_ls: 0.6046066284179688 te_ac: 0.8897464167585446\n",
      "ep 542 tr_ac: 0.9390909854658489 ls: 8.143601216375828 te_ls: 0.1811782568693161 te_ac: 0.8886438809261301\n",
      "ep 543 tr_ac: 0.9417793833487356 ls: 7.983364470303059 te_ls: 0.18112653493881226 te_ac: 0.8897464167585446\n",
      "ep 544 tr_ac: 0.9388389481643283 ls: 8.04149378836155 te_ls: 0.44952043890953064 te_ac: 0.888864388092613\n",
      "ep 545 tr_ac: 0.9420314206502562 ls: 7.962861441075802 te_ls: 0.2042219638824463 te_ac: 0.8895259095920618\n",
      "ep 546 tr_ac: 0.9385028984289675 ls: 7.9573139026761055 te_ls: 0.18475341796875 te_ac: 0.8908489525909592\n",
      "ep 547 tr_ac: 0.9423674703856171 ls: 7.7046922743320465 te_ls: 0.4652799367904663 te_ac: 0.8886438809261301\n",
      "ep 548 tr_ac: 0.9407712341426531 ls: 7.881596051156521 te_ls: 0.3455357253551483 te_ac: 0.8899669239250275\n",
      "ep 549 tr_ac: 0.9405191968411325 ls: 8.088901109993458 te_ls: 0.17142044007778168 te_ac: 0.8895259095920618\n",
      "ep 550 tr_ac: 0.9420314206502562 ls: 7.7848313227295876 te_ls: 0.5014711618423462 te_ac: 0.889084895259096\n",
      "ep 551 tr_ac: 0.9377467865244056 ls: 8.26859700679779 te_ls: 0.2575274407863617 te_ac: 0.8904079382579934\n",
      "ep 552 tr_ac: 0.9395950600688902 ls: 7.998423188924789 te_ls: 0.316957950592041 te_ac: 0.8921719955898567\n",
      "ep 553 tr_ac: 0.9409392590103335 ls: 7.756181389093399 te_ls: 0.2038494348526001 te_ac: 0.8899669239250275\n",
      "ep 554 tr_ac: 0.9395950600688902 ls: 7.941884763538837 te_ls: 0.3389569818973541 te_ac: 0.8897464167585446\n",
      "ep 555 tr_ac: 0.9403511719734521 ls: 7.9548384770751 te_ls: 0.5490962266921997 te_ac: 0.8904079382579934\n",
      "ep 556 tr_ac: 0.9406032092749727 ls: 7.919868037104607 te_ls: 0.20643053948879242 te_ac: 0.8899669239250275\n",
      "ep 557 tr_ac: 0.9409392590103335 ls: 8.038012862205505 te_ls: 0.2251727133989334 te_ac: 0.8901874310915104\n",
      "ep 558 tr_ac: 0.9427035201209779 ls: 7.684813521802425 te_ls: 0.182419553399086 te_ac: 0.888864388092613\n",
      "ep 559 tr_ac: 0.9436276568932201 ls: 7.677847243845463 te_ls: 0.3600810170173645 te_ac: 0.888864388092613\n",
      "ep 560 tr_ac: 0.9386709232966479 ls: 7.976451151072979 te_ls: 0.32367706298828125 te_ac: 0.8897464167585446\n",
      "ep 561 tr_ac: 0.9396790725027304 ls: 7.990766994655132 te_ls: 0.4600394070148468 te_ac: 0.8886438809261301\n",
      "ep 562 tr_ac: 0.9405191968411325 ls: 8.037521414458752 te_ls: 0.1768127977848053 te_ac: 0.8908489525909592\n",
      "ep 563 tr_ac: 0.9375787616567252 ls: 8.098482258617878 te_ls: 0.42881059646606445 te_ac: 0.8877618522601984\n",
      "ep 564 tr_ac: 0.9384188859951272 ls: 8.074623107910156 te_ls: 0.2864845395088196 te_ac: 0.8897464167585446\n",
      "ep 565 tr_ac: 0.937494749222885 ls: 8.111149124801159 te_ls: 0.38278716802597046 te_ac: 0.8904079382579934\n",
      "ep 566 tr_ac: 0.9369906746198438 ls: 8.31408716738224 te_ls: 0.33010050654411316 te_ac: 0.8901874310915104\n",
      "ep 567 tr_ac: 0.9409392590103335 ls: 7.900197736918926 te_ls: 0.15020301938056946 te_ac: 0.8904079382579934\n",
      "ep 568 tr_ac: 0.9409392590103335 ls: 8.000925123691559 te_ls: 0.544522762298584 te_ac: 0.8895259095920618\n",
      "ep 569 tr_ac: 0.9381668486936067 ls: 8.15066510438919 te_ls: 0.12792788445949554 te_ac: 0.8893054024255789\n",
      "ep 570 tr_ac: 0.9426195076871376 ls: 7.945581272244453 te_ls: 0.20753590762615204 te_ac: 0.8886438809261301\n",
      "ep 571 tr_ac: 0.9405191968411325 ls: 7.811407312750816 te_ls: 0.11487247049808502 te_ac: 0.8886438809261301\n",
      "ep 572 tr_ac: 0.93951104763505 ls: 7.971494227647781 te_ls: 0.27444061636924744 te_ac: 0.8895259095920618\n",
      "ep 573 tr_ac: 0.9382508611274468 ls: 8.148099794983864 te_ls: 0.13016125559806824 te_ac: 0.8884233737596472\n",
      "ep 574 tr_ac: 0.93951104763505 ls: 8.005517967045307 te_ls: 0.13042059540748596 te_ac: 0.889084895259096\n",
      "ep 575 tr_ac: 0.9398470973704108 ls: 7.853968232870102 te_ls: 0.4017253816127777 te_ac: 0.889084895259096\n",
      "ep 576 tr_ac: 0.9407712341426531 ls: 7.864282481372356 te_ls: 0.41506412625312805 te_ac: 0.8895259095920618\n",
      "ep 577 tr_ac: 0.9398470973704108 ls: 7.781334236264229 te_ls: 0.25949060916900635 te_ac: 0.8895259095920618\n",
      "ep 578 tr_ac: 0.9406032092749727 ls: 7.801097318530083 te_ls: 0.20163904130458832 te_ac: 0.8879823594266814\n",
      "ep 579 tr_ac: 0.9432916071578593 ls: 7.873460866510868 te_ls: 0.31857582926750183 te_ac: 0.8897464167585446\n",
      "ep 580 tr_ac: 0.9382508611274468 ls: 8.018996641039848 te_ls: 0.2901188135147095 te_ac: 0.8884233737596472\n",
      "ep 581 tr_ac: 0.9365706124506427 ls: 8.227553591132164 te_ls: 0.23138289153575897 te_ac: 0.8886438809261301\n",
      "ep 582 tr_ac: 0.9412753087456943 ls: 7.734546832740307 te_ls: 0.14989522099494934 te_ac: 0.8895259095920618\n",
      "ep 583 tr_ac: 0.9411912963118542 ls: 7.9536367431283 te_ls: 0.36840128898620605 te_ac: 0.8899669239250275\n",
      "ep 584 tr_ac: 0.9389229605981685 ls: 8.255995869636536 te_ls: 0.20207488536834717 te_ac: 0.8908489525909592\n",
      "ep 585 tr_ac: 0.9397630849365706 ls: 7.97223974019289 te_ls: 0.39449188113212585 te_ac: 0.8886438809261301\n",
      "ep 586 tr_ac: 0.9383348735612871 ls: 8.020132720470428 te_ls: 0.32168954610824585 te_ac: 0.8897464167585446\n",
      "ep 587 tr_ac: 0.9390069730320088 ls: 8.174897529184818 te_ls: 0.3255702555179596 te_ac: 0.889084895259096\n",
      "ep 588 tr_ac: 0.941527346047215 ls: 7.863224819302559 te_ls: 0.178651362657547 te_ac: 0.8893054024255789\n",
      "ep 589 tr_ac: 0.943963706628581 ls: 7.829686731100082 te_ls: 0.17512589693069458 te_ac: 0.8899669239250275\n",
      "ep 590 tr_ac: 0.9386709232966479 ls: 8.180408596992493 te_ls: 0.42063745856285095 te_ac: 0.8893054024255789\n",
      "ep 591 tr_ac: 0.9400991346719314 ls: 7.7709430903196335 te_ls: 0.32784709334373474 te_ac: 0.8895259095920618\n",
      "ep 592 tr_ac: 0.9397630849365706 ls: 7.918782211840153 te_ls: 0.2120254784822464 te_ac: 0.8904079382579934\n",
      "ep 593 tr_ac: 0.9400151222380912 ls: 7.905532270669937 te_ls: 0.3982371687889099 te_ac: 0.8886438809261301\n",
      "ep 594 tr_ac: 0.9392590103335293 ls: 8.19327450543642 te_ls: 0.31154870986938477 te_ac: 0.889084895259096\n",
      "ep 595 tr_ac: 0.9409392590103335 ls: 7.880895495414734 te_ls: 0.294731080532074 te_ac: 0.8901874310915104\n",
      "ep 596 tr_ac: 0.9373267243552046 ls: 8.211078837513924 te_ls: 0.6598615646362305 te_ac: 0.891510474090408\n",
      "ep 597 tr_ac: 0.9390069730320088 ls: 7.771966725587845 te_ls: 0.17590460181236267 te_ac: 0.8901874310915104\n",
      "ep 598 tr_ac: 0.9398470973704108 ls: 7.807472109794617 te_ls: 0.18747718632221222 te_ac: 0.889084895259096\n",
      "ep 599 tr_ac: 0.941527346047215 ls: 8.02597077935934 te_ls: 0.30433517694473267 te_ac: 0.888864388092613\n",
      "ep 600 tr_ac: 0.9409392590103335 ls: 7.777589537203312 te_ls: 0.21793699264526367 te_ac: 0.8895259095920618\n"
     ]
    }
   ],
   "source": [
    "# CNN 训练\n",
    "# 损失\n",
    "# criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "# 加载数据，设置优化器\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=256, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=100, shuffle=True)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.0002)\n",
    "lr_schedule = torch.optim.lr_scheduler.StepLR(optimizer, 100, gamma=0.5, last_epoch=-1)\n",
    "# 初始化 visdom\n",
    "viz.close()\n",
    "viz = viz_init()\n",
    "if not os.path.exists(vislogDir):\n",
    "    os.makedirs(vislogDir)\n",
    "viz = Visdom(\n",
    "    env=viz_acnt.evns, log_to_filename=vislogDir + \"vislog_\" + get_current_time()\n",
    ")\n",
    "vizx = 0\n",
    "\n",
    "total_test_acc = 0\n",
    "total_test_correct = 0\n",
    "totaltest = 0\n",
    "# 训练过程\n",
    "epoch_num = 600\n",
    "net.to(device)\n",
    "for epoch in range(epoch_num):\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    curr_total_correct = 0\n",
    "    total_traintnum = 0\n",
    "    for batch in train_loader:\n",
    "        # 载入本批次数据\n",
    "        batch = batch\n",
    "        images, labels = batch\n",
    "        images = images.to(torch.float32).to(device)\n",
    "        labels = labels.long().to(device)\n",
    "        net.train()\n",
    "        preds = net(images)\n",
    "        trainloss = F.cross_entropy(preds, labels)  # 真实数据的鉴别器损失\n",
    "        optimizer.zero_grad()\n",
    "        trainloss.backward()  # Calculate Gradients\n",
    "        optimizer.step()  # Update Weight\n",
    "        total_loss += trainloss.item()\n",
    "        curr_total_correct = get_num_correct(preds, labels)\n",
    "        total_correct += curr_total_correct\n",
    "        total_traintnum += labels.size(0)\n",
    "    total_train_acc = total_correct / total_traintnum\n",
    "    total_correct = 0\n",
    "    # 测试\n",
    "    net.eval()\n",
    "    total_testnum = 0\n",
    "    for testemgdatas, testemglabels in test_loader:  # Get Batch\n",
    "        testemgdatas = testemgdatas.to(torch.float32).to(device)\n",
    "        testemglabels = testemglabels.long().to(device)\n",
    "        predstest = net(testemgdatas)\n",
    "        testloss = F.cross_entropy(\n",
    "            predstest, testemglabels\n",
    "        )  # Calculate Loss\n",
    "        curr_test_correct = get_num_correct(\n",
    "            predstest, testemglabels\n",
    "        )\n",
    "        total_testnum += testemglabels.size(0)\n",
    "        total_test_correct += curr_test_correct\n",
    "        # totaltest += testemglabels.size(0)\n",
    "    # total_test_acc = total_test_correct/(trainlabel.size)\n",
    "    total_test_acc = total_test_correct / total_testnum\n",
    "    print(\n",
    "        \"ep\",\n",
    "        epoch+1,\n",
    "        \"tr_ac:\",\n",
    "        total_train_acc,\n",
    "        \"ls:\",\n",
    "        total_loss,\n",
    "        \"te_ls:\",\n",
    "        float(testloss),\n",
    "        \"te_ac:\",\n",
    "        total_test_acc,\n",
    "    )\n",
    "    total_test_correct = 0\n",
    "    # 可视化，每 epoch 更新\n",
    "    viz.line(\n",
    "        [float(trainloss)],\n",
    "        [epoch],\n",
    "        win=\"loss_perEpoch\",\n",
    "        name=\"train_loss\",\n",
    "        update=\"append\",\n",
    "        opts=dict(title=\"loss_perEpoch\", xlabel=\"epoch\", ylabel=\"loss\"),\n",
    "    )\n",
    "    viz.line(\n",
    "        [float(testloss)], [epoch], win=\"loss_perEpoch\", name=\"D_loss\", update=\"append\"\n",
    "    )\n",
    "    viz.line(\n",
    "        [float(total_train_acc)],\n",
    "        [epoch],\n",
    "        win=\"acc_perEpoch\",\n",
    "        name=\"train_acc\",\n",
    "        update=\"append\",\n",
    "        opts=dict(title=\"acc_perEpoch\", xlabel=\"epoch\", ylabel=\"acc\"),\n",
    "    )\n",
    "    viz.line(\n",
    "        [float(total_test_acc)],\n",
    "        [epoch],\n",
    "        win=\"acc_perEpoch\",\n",
    "        name=\"test_acc\",\n",
    "        update=\"append\",\n",
    "    )\n",
    "\n",
    "    viz.line(\n",
    "        [float(optimizer.state_dict()[\"param_groups\"][0][\"lr\"])],\n",
    "        [epoch],\n",
    "        win=\"lr_perEpoch\",\n",
    "        name=\"lr\",\n",
    "        update=\"append\",\n",
    "        opts=dict(title=\"lr_perEpoch\", xlabel=\"epoch\", ylabel=\"lr\"),\n",
    "    )\n",
    "    # 更新学习率\n",
    "    lr_schedule.step()\n",
    "    # viz.text('updating weights',win='Monitor',append=True,opts = {'title':'ProcessMonitor',},)\n",
    "    # 定期保存\n",
    "    if (epoch + 1) % 300 == 0:\n",
    "        timeForSave = datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "        checkpointPath = (\n",
    "            ckpDir\n",
    "            + \"c_\"\n",
    "            + str(num_kn)\n",
    "            + \"kc_ep\"\n",
    "            + str(epoch + 1)\n",
    "            + \"_acc\"\n",
    "            + str(int(total_test_acc * 10000))\n",
    "            + \"_\"\n",
    "            + timeForSave\n",
    "            + \".pth\"\n",
    "        )\n",
    "        c_state = {\n",
    "            \"model\": net.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "            \"epoch\": epoch,\n",
    "        }\n",
    "        torch.save(c_state, checkpointPath)\n",
    "        viz.text(\n",
    "            \"epoch \" + str(epoch + 1) + \" model saved\",\n",
    "            win=\"Monitor\",\n",
    "            append=True,\n",
    "            opts={\n",
    "                \"title\": \"ProcessMonitor\",\n",
    "            },\n",
    "        )\n",
    "\n",
    "\n",
    "checkpointPath_model = (\n",
    "    model_Dir\n",
    "    + \"c_final_\"                \n",
    "    + str(num_kn)\n",
    "    + \"kc_\"\n",
    "    + \"acc\"\n",
    "    + str(int(total_test_acc * 10000))\n",
    "    + \"_\"\n",
    "    + timeForSave\n",
    "    + \".pth\"\n",
    ")\n",
    "torch.save(net.state_dict(), checkpointPath_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "           Flatten-1                     [1, 4]               0\n",
      "            Linear-2                    [1, 64]             320\n",
      "       BatchNorm1d-3                    [1, 64]             128\n",
      "              ReLU-4                    [1, 64]               0\n",
      "         Dropout2d-5                    [1, 64]               0\n",
      "            Linear-6                   [1, 128]           8,320\n",
      "       BatchNorm1d-7                   [1, 128]             256\n",
      "              ReLU-8                   [1, 128]               0\n",
      "         Dropout2d-9                   [1, 128]               0\n",
      "           Linear-10                     [1, 4]             516\n",
      "================================================================\n",
      "Total params: 9,540\n",
      "Trainable params: 9,540\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.01\n",
      "Params size (MB): 0.04\n",
      "Estimated Total Size (MB): 0.04\n",
      "----------------------------------------------------------------\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "           Flatten-1                     [1, 4]               0\n",
      "            Linear-2                   [1, 128]             640\n",
      "       BatchNorm1d-3                   [1, 128]             256\n",
      "         LeakyReLU-4                   [1, 128]               0\n",
      "         Dropout2d-5                   [1, 128]               0\n",
      "            Linear-6                    [1, 64]           8,256\n",
      "       BatchNorm1d-7                    [1, 64]             128\n",
      "         LeakyReLU-8                    [1, 64]               0\n",
      "         Dropout2d-9                    [1, 64]               0\n",
      "           Linear-10                     [1, 1]              65\n",
      "================================================================\n",
      "Total params: 9,345\n",
      "Trainable params: 9,345\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.01\n",
      "Params size (MB): 0.04\n",
      "Estimated Total Size (MB): 0.04\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Flatten(start_dim=1, end_dim=-1)\n",
       "  (1): Linear(in_features=4, out_features=128, bias=True)\n",
       "  (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (3): LeakyReLU(negative_slope=0.2)\n",
       "  (4): Dropout2d(p=0.2, inplace=False)\n",
       "  (5): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (6): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (7): LeakyReLU(negative_slope=0.2)\n",
       "  (8): Dropout2d(p=0.2, inplace=False)\n",
       "  (9): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# EMG需要转化成6维特征，用到刚刚训练的模型c_ep_200_acc9763_2022_02_26_23_18_44,\n",
    "# 并重组为新数据集OpenganDatafea_smr_10cl_220227.npy，步骤见 emgDataprocess.ipynb\n",
    "num_kc = 4\n",
    "# opGAN部分开始\n",
    "# GAN 模型搭建\n",
    "# 潜在张量大小,32，4是因为 cnn feature 只有6个，不好比它大\n",
    "latent_size = 4\n",
    "# 输出通道数\n",
    "n_channel = num_kc\n",
    "# 生成网络隐藏层大小\n",
    "n_g_feature = 64\n",
    "\n",
    "gnet = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(latent_size, n_g_feature * 1),  # 用线性变换将输入映射到64维\n",
    "    nn.BatchNorm1d(n_g_feature * 1),\n",
    "    nn.ReLU(True),  # relu激活\n",
    "    nn.Dropout2d(0.2),\n",
    "    nn.Linear(n_g_feature * 1, n_g_feature * 2),  # 线性变换\n",
    "    nn.BatchNorm1d(n_g_feature * 2),\n",
    "    nn.ReLU(True),  # relu激活\n",
    "    nn.Dropout2d(0.2),\n",
    "    nn.Linear(n_g_feature * 2, n_channel),  # 线性变换\n",
    ").to(device)\n",
    "gnet.to(\"cpu\")\n",
    "# print(gnet)\n",
    "summary(gnet, (1, 1, 4), batch_size=1, device=\"cpu\")\n",
    "gnet.to(device)\n",
    "# 鉴别网络隐藏层大小\n",
    "# 32,\n",
    "n_d_feature = 64\n",
    "dnet = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(n_channel, n_d_feature * 2),  # 输入特征数为784，输出为256\n",
    "    nn.BatchNorm1d(n_d_feature * 2),\n",
    "    nn.LeakyReLU(0.2),  # 进行非线性映射\n",
    "    nn.Dropout2d(0.2),\n",
    "    nn.Linear(n_d_feature * 2, n_d_feature),  # 进行一个线性映射\n",
    "    nn.BatchNorm1d(n_d_feature),\n",
    "    nn.LeakyReLU(0.2),\n",
    "    nn.Dropout2d(0.2),\n",
    "    nn.Linear(n_d_feature, 1),\n",
    "    # nn.Sigmoid()  # 也是一个激活函数，二分类问题中，\n",
    "    # sigmoid可以班实数映射到【0,1】，作为概率值，\n",
    "    # 多分类用softmax函数\n",
    ").to(device)\n",
    "# print(dnet)\n",
    "dnet.to(\"cpu\")\n",
    "summary(dnet, (1, 1, num_kc), batch_size=1, device=\"cpu\")\n",
    "dnet.to(device)\n",
    "# 初始化权重\n",
    "def weights_init(m):\n",
    "    if type(m) in [nn.ConvTranspose2d, nn.Conv2d]:\n",
    "        init.xavier_normal_(m.weight)\n",
    "    elif type(m) == nn.BatchNorm2d:\n",
    "        init.normal_(m.weight, 1.0, 0.02)\n",
    "        init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "gnet.apply(weights_init)\n",
    "dnet.apply(weights_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAN 数据加载，构建数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "(tensor([ 3.8041, -3.6928, -1.0038, -0.7952]), array([0], dtype=int16))\n",
      "(tensor([ 3.5913, -2.9987, -0.4952, -1.7472]), array([1], dtype=int16))\n",
      "(tensor([ 7.3668, -8.6500, -2.9457,  1.6768]), array([1], dtype=int16))\n"
     ]
    }
   ],
   "source": [
    "# 加载 openGAN 所需数据，这里只有训练、验证两部分，没有测试集，因为本次数据采集划分时只划出两部分，不影响最终结果\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),  # 将图片转换为Tensor,归一化至[0,1]\n",
    "    ]\n",
    ")\n",
    "GAN_dataarray = np.load(\"../../data/nina_db1/t03/s1/Fopg_s1_diff_ratio_vec_20220629.npy\", allow_pickle=True)\n",
    "GANdataset = GAN_dataarray.item()\n",
    "print(type(GANdataset))\n",
    "\n",
    "ktr_str_X = \"vec_ktr_X_\"+str(num_kc)+\"c\"\n",
    "ktr_str_Y = \"ktr_Y_\"+str(num_kc)+\"c\"\n",
    "val_str_X = \"vec_val_X_\"+str(num_kc)+\"c\"\n",
    "val_str_Y = \"val_Y_\"+str(num_kc)+\"c\"\n",
    "dte_str_X = \"vec_dte_X_\"+str(num_kc)+\"c\"\n",
    "dte_str_Y = \"dte_Y_\"+str(num_kc)+\"c\"\n",
    "\n",
    "traindata = GANdataset[ktr_str_X]\n",
    "trainlabel = GANdataset[ktr_str_Y]\n",
    "# 本方法中的 test 实际上是个大号的完整版openset,所以标签是01\n",
    "testdata = GANdataset[dte_str_X]\n",
    "testlabel = GANdataset[dte_str_Y]\n",
    "testlabel_for_auc = GANdataset[dte_str_Y]\n",
    "testlabel_for_auc = testlabel_for_auc.ravel()\n",
    "\n",
    "valdata = GANdataset[val_str_X]\n",
    "vallabel = GANdataset[val_str_Y]\n",
    "vallabel_for_auc = GANdataset[val_str_Y]\n",
    "vallabel_for_auc = vallabel_for_auc.ravel()\n",
    "\n",
    "# 注意这里 ktr，kte 的标签需要降维，其他的在数据制作阶段已经降过了\n",
    "# trainlabel = trainlabel[:, 0]\n",
    "# testlabel = testlabel[:,0]\n",
    "# vallabel = vallabel[:, 0]\n",
    "# trainunknownc_label = trainunknownc_label[:,0]\n",
    "# print(type(trainlabel))\n",
    "train_set = EMGDataset_1D(traindata, trainlabel)\n",
    "test_set = EMGDataset_1D(testdata, testlabel)\n",
    "val_set = EMGDataset_1D(valdata, vallabel)\n",
    "# train_unknown = EMGDataset(trainunknown_data,trainunknownc_label)\n",
    "# train_loader = torch.utils.data.DataLoader(train_set, batch_size=1, shuffle=True, pin_memory=True,\n",
    "#                                             num_workers=3)\n",
    "\n",
    "sample = next(iter(train_set))\n",
    "print(sample)\n",
    "sample = next(iter(test_set))\n",
    "print(sample)\n",
    "sample = next(iter(val_set))\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练 GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n",
      "Setting up a new session...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "visdom has started\n",
      "[1/2500]D loss:1.43097 G loss:1.8017真判真:0.510306 假判真:0.514018/0.368626\n",
      "[1/2500]D loss:2.71349 G loss:0.577476真判真:0.359214 假判真:0.369249/0.833689\n",
      "[1/2500]D loss:11.5147 G loss:2.51598真判真:0.826614 假判真:0.824923/0.508816\n",
      "[1/2500]D loss:3.90614 G loss:1.11309真判真:0.423899 假判真:0.509233/0.364595\n",
      "[1/2500]D loss:1.65759 G loss:0.528112真判真:0.368217 假判真:0.365113/0.612621\n",
      "[1/2500]D loss:1.63365 G loss:1.27846真判真:0.594696 假判真:0.604226/0.448074\n",
      "[1/2500]D loss:1.84085 G loss:0.626268真判真:0.446638 假判真:0.441419/0.543074\n",
      "[1/2500]D loss:1.41965 G loss:0.736316真判真:0.547314 假判真:0.548086/0.490191\n",
      "[1/2500]D loss:1.45341 G loss:0.793807真判真:0.496942 假判真:0.497094/0.457133\n",
      "[1/2500]D loss:1.39613 G loss:0.623823真判真:0.46433 假判真:0.45802/0.541754\n",
      "[1/2500]D loss:1.39353 G loss:0.757569真判真:0.558907 假判真:0.546428/0.473018\n",
      "[1/2500]D loss:1.37501 G loss:0.732931真判真:0.487363 假判真:0.471763/0.484022\n",
      "[1/2500]D loss:1.34127 G loss:0.66044真判真:0.503023 假判真:0.474399/0.522719\n",
      "[1/2500]D loss:1.37816 G loss:0.792295真判真:0.541545 假判真:0.526284/0.459975\n",
      "[1/2500]D loss:1.40393 G loss:0.732619真判真:0.507866 假判真:0.507673/0.483054\n",
      "[2/2500]D loss:1.41412 G loss:0.452918真判真:0.432649 假判真:0.419903/0.644318\n",
      "[2/2500]D loss:1.5666 G loss:1.42002真判真:0.635831 假判真:0.647752/0.256175\n",
      "[2/2500]D loss:1.69791 G loss:0.31509真判真:0.263759 假判真:0.256566/0.732399\n",
      "[2/2500]D loss:1.63056 G loss:0.895054真判真:0.74071 假判真:0.727651/0.411584\n",
      "[2/2500]D loss:1.45162 G loss:0.82783真判真:0.401752 假判真:0.409743/0.439388\n",
      "[2/2500]D loss:1.38545 G loss:0.573384真判真:0.447014 假判真:0.434273/0.565678\n",
      "[2/2500]D loss:1.41494 G loss:0.754359真判真:0.57378 假判真:0.572775/0.47393\n",
      "[2/2500]D loss:1.38334 G loss:0.756267真判真:0.482473 假判真:0.476378/0.472783\n",
      "[2/2500]D loss:1.36897 G loss:0.725467真判真:0.493071 假判真:0.479979/0.491314\n",
      "[2/2500]D loss:1.38174 G loss:0.659541真判真:0.507396 假判真:0.498496/0.519454\n",
      "[2/2500]D loss:1.37231 G loss:0.728834真判真:0.5309 假判真:0.516452/0.484448\n",
      "[2/2500]D loss:1.37057 G loss:0.775287真判真:0.496651 假判真:0.485096/0.467073\n",
      "[2/2500]D loss:1.36678 G loss:0.635559真判真:0.489238 假判真:0.468664/0.533337\n",
      "[2/2500]D loss:1.35368 G loss:0.704427真判真:0.548997 假判真:0.523725/0.497795\n",
      "[2/2500]D loss:1.44428 G loss:0.994779真判真:0.521543 假判真:0.54133/0.390284\n",
      "[3/2500]D loss:1.44273 G loss:0.337061真判真:0.422542 假判真:0.422098/0.727678\n",
      "[3/2500]D loss:1.9029 G loss:2.1593真判真:0.743398 假判真:0.746352/0.126257\n",
      "[3/2500]D loss:2.32474 G loss:0.496787真判真:0.122727 假判真:0.118436/0.625335\n",
      "[3/2500]D loss:1.51423 G loss:0.689724真判真:0.639084 假判真:0.631661/0.512052\n",
      "[3/2500]D loss:1.4186 G loss:0.988506真判真:0.513715 假判真:0.507931/0.390148\n",
      "[3/2500]D loss:1.49867 G loss:0.435724真判真:0.397987 假判真:0.396688/0.653249\n",
      "[3/2500]D loss:1.52414 G loss:0.918781真判真:0.65228 假判真:0.648406/0.40838\n",
      "[3/2500]D loss:1.44476 G loss:0.701157真判真:0.414859 假判真:0.410053/0.505161\n",
      "[3/2500]D loss:1.399 G loss:0.635844真判真:0.517788 假判真:0.508912/0.534756\n",
      "[3/2500]D loss:1.40912 G loss:0.774717真判真:0.531489 假判真:0.531116/0.467723\n",
      "[3/2500]D loss:1.39096 G loss:0.60219真判真:0.470734 假判真:0.458896/0.551682\n",
      "[3/2500]D loss:1.43379 G loss:0.809801真判真:0.553676 假判真:0.559708/0.450618\n",
      "[3/2500]D loss:1.39176 G loss:0.701045真判真:0.450571 假判真:0.439916/0.507051\n",
      "[3/2500]D loss:1.38328 G loss:0.729519真判真:0.531887 假判真:0.518278/0.48851\n",
      "[3/2500]D loss:1.33141 G loss:0.5108真判真:0.471682 假判真:0.436536/0.62706\n",
      "[4/2500]D loss:1.55456 G loss:1.06723真判真:0.6547 假判真:0.653556/0.350505\n",
      "[4/2500]D loss:1.4967 G loss:0.715474真判真:0.362194 假判真:0.364323/0.493424\n",
      "[4/2500]D loss:1.3789 G loss:0.544276真判真:0.509757 假判真:0.497723/0.584103\n",
      "[4/2500]D loss:1.4047 G loss:0.822728真判真:0.580951 假判真:0.571662/0.445411\n",
      "[4/2500]D loss:1.41944 G loss:0.725602真判真:0.450078 假判真:0.453223/0.486626\n",
      "[4/2500]D loss:1.39628 G loss:0.618945真判真:0.483221 假判真:0.480662/0.541221\n",
      "[4/2500]D loss:1.41084 G loss:0.767446真判真:0.541999 假判真:0.544301/0.466671\n",
      "[4/2500]D loss:1.40069 G loss:0.757899真判真:0.468605 假判真:0.468921/0.47106\n",
      "[4/2500]D loss:1.39557 G loss:0.640804真判真:0.478062 假判真:0.477437/0.530299\n",
      "[4/2500]D loss:1.41083 G loss:0.725374真判真:0.530036 假判真:0.534224/0.486097\n",
      "[4/2500]D loss:1.38668 G loss:0.731788真判真:0.497372 假判真:0.493873/0.482704\n",
      "[4/2500]D loss:1.37854 G loss:0.70027真判真:0.489886 假判真:0.481899/0.500065\n",
      "[4/2500]D loss:1.37427 G loss:0.726488真判真:0.515032 假判真:0.502273/0.486111\n",
      "[4/2500]D loss:1.35737 G loss:0.675058真判真:0.499833 假判真:0.481459/0.51593\n",
      "[4/2500]D loss:1.27184 G loss:1.22309真判真:0.546608 假判真:0.481589/0.407369\n",
      "[5/2500]D loss:1.63984 G loss:0.690322真判真:0.448732 假判真:0.460353/0.558221\n",
      "[5/2500]D loss:2.01449 G loss:1.14952真判真:0.573969 假判真:0.575116/0.325298\n",
      "[5/2500]D loss:1.58444 G loss:0.45628真判真:0.318201 假判真:0.331342/0.639888\n",
      "[5/2500]D loss:1.53638 G loss:1.03751真判真:0.635782 假判真:0.641318/0.359069\n",
      "[5/2500]D loss:1.45332 G loss:0.526892真判真:0.374416 假判真:0.364922/0.594059\n",
      "[5/2500]D loss:1.42237 G loss:0.810018真判真:0.591854 假判真:0.585538/0.449903\n",
      "[5/2500]D loss:1.39886 G loss:0.657671真判真:0.456731 假判真:0.449116/0.520301\n",
      "[5/2500]D loss:1.40663 G loss:0.647769真判真:0.513044 假判真:0.516635/0.525079\n",
      "[5/2500]D loss:1.41484 G loss:0.801033真判真:0.520996 假判真:0.529541/0.455128\n",
      "[5/2500]D loss:1.4182 G loss:0.570406真判真:0.451163 假判真:0.451715/0.566632\n",
      "[5/2500]D loss:1.42471 G loss:0.776147真判真:0.564537 假判真:0.571827/0.461748\n",
      "[5/2500]D loss:1.39336 G loss:0.676904真判真:0.460246 假判真:0.458056/0.509623\n",
      "[5/2500]D loss:1.38413 G loss:0.675323真判真:0.519388 假判真:0.51478/0.511317\n",
      "[5/2500]D loss:1.38641 G loss:0.746372真判真:0.516814 假判真:0.511925/0.475112\n",
      "[5/2500]D loss:1.38348 G loss:0.544742真判真:0.481659 假判真:0.475428/0.580579\n",
      "[6/2500]D loss:1.40829 G loss:0.921966真判真:0.562063 假判真:0.555816/0.400618\n",
      "[6/2500]D loss:1.4373 G loss:0.462538真判真:0.401513 假判真:0.402679/0.634909\n",
      "[6/2500]D loss:1.48282 G loss:0.983139真判真:0.63103 假判真:0.626673/0.405703\n",
      "[6/2500]D loss:1.52258 G loss:0.463644真判真:0.404064 假判真:0.408923/0.64071\n",
      "[6/2500]D loss:1.52794 G loss:1.6226真判真:0.66358 假判真:0.653125/0.21561\n",
      "[6/2500]D loss:1.87518 G loss:0.589308真判真:0.218363 假判真:0.228049/0.561191\n",
      "[6/2500]D loss:1.4079 G loss:1.10503真判真:0.571664 假判真:0.558191/0.359427\n",
      "[6/2500]D loss:1.53829 G loss:0.394862真判真:0.388158 假判真:0.375099/0.682453\n",
      "[6/2500]D loss:1.5886 G loss:1.16049真判真:0.673427 假判真:0.67159/0.33001\n",
      "[6/2500]D loss:1.63121 G loss:0.370859真判真:0.326079 假判真:0.338122/0.694349\n",
      "[6/2500]D loss:1.59769 G loss:0.851239真判真:0.697171 假判真:0.69682/0.432558\n",
      "[6/2500]D loss:1.43854 G loss:0.854589真判真:0.432311 假判真:0.440738/0.429658\n",
      "[6/2500]D loss:1.41484 G loss:0.621998真判真:0.435623 假判真:0.433183/0.541417\n",
      "[6/2500]D loss:1.37531 G loss:0.692517真判真:0.551839 假判真:0.535444/0.507384\n",
      "[6/2500]D loss:1.42029 G loss:0.630802真判真:0.506751 假判真:0.512036/0.534187\n",
      "[7/2500]D loss:1.39026 G loss:0.647432真判真:0.507778 假判真:0.500735/0.537728\n",
      "[7/2500]D loss:1.47531 G loss:0.794093真判真:0.537959 假判真:0.546841/0.484688\n",
      "[7/2500]D loss:1.52169 G loss:0.789631真判真:0.486155 假判真:0.482643/0.497737\n",
      "[7/2500]D loss:1.47735 G loss:0.596539真判真:0.496352 假判真:0.489654/0.552334\n",
      "[7/2500]D loss:1.41419 G loss:1.10034真判真:0.556981 假判真:0.560048/0.336071\n",
      "[7/2500]D loss:1.50502 G loss:0.287674真判真:0.333827 假判真:0.325909/0.751265\n",
      "[7/2500]D loss:1.69958 G loss:1.29928真判真:0.755415 假判真:0.754634/0.27953\n",
      "[7/2500]D loss:1.60951 G loss:0.381684真判真:0.287105 假判真:0.282488/0.688984\n",
      "[7/2500]D loss:1.61682 G loss:0.984085真判真:0.678201 假判真:0.687278/0.382613\n",
      "[7/2500]D loss:1.44768 G loss:0.613056真判真:0.386402 假判真:0.378245/0.547582\n",
      "[7/2500]D loss:1.41638 G loss:0.702673真判真:0.553162 假判真:0.553044/0.506371\n",
      "[7/2500]D loss:1.40309 G loss:0.851412真判真:0.515809 假判真:0.513445/0.432811\n",
      "[7/2500]D loss:1.42142 G loss:0.706578真判真:0.438211 假判真:0.442498/0.507226\n",
      "[7/2500]D loss:1.43698 G loss:0.775103真判真:0.510933 假判真:0.507462/0.482352\n",
      "[7/2500]D loss:1.45055 G loss:0.512574真判真:0.484748 假判真:0.49799/0.656009\n",
      "[8/2500]D loss:1.69062 G loss:1.36672真判真:0.647009 假判真:0.63485/0.259638\n",
      "[8/2500]D loss:1.70864 G loss:0.197479真判真:0.250996 假判真:0.256741/0.832498\n",
      "[8/2500]D loss:2.23684 G loss:1.97168真判真:0.829726 假判真:0.835372/0.216766\n",
      "[8/2500]D loss:2.25776 G loss:0.189592真判真:0.202032 假判真:0.208512/0.836608\n",
      "[8/2500]D loss:2.75965 G loss:1.399真判真:0.834484 假判真:0.834886/0.397391\n",
      "[8/2500]D loss:1.93206 G loss:0.837034真判真:0.399261 假判真:0.386196/0.470025\n",
      "[8/2500]D loss:1.65716 G loss:0.364295真判真:0.480143 假判真:0.480615/0.731847\n",
      "[8/2500]D loss:1.77256 G loss:1.9688真判真:0.736741 假判真:0.729978/0.156556\n",
      "[8/2500]D loss:2.12554 G loss:0.177538真判真:0.152784 假判真:0.150604/0.844895\n",
      "[8/2500]D loss:2.1387 G loss:1.32823真判真:0.829094 假判真:0.820807/0.317664\n",
      "[8/2500]D loss:1.76572 G loss:0.706341真判真:0.29816 假判真:0.302753/0.506436\n",
      "[8/2500]D loss:1.42888 G loss:0.898931真判真:0.532405 假判真:0.523823/0.418161\n",
      "[8/2500]D loss:1.46271 G loss:0.597457真判真:0.428088 假判真:0.432841/0.562069\n",
      "[8/2500]D loss:1.45473 G loss:0.964603真判真:0.570713 假判真:0.563165/0.396324\n",
      "[8/2500]D loss:1.46828 G loss:0.601422真判真:0.381137 假判真:0.383302/0.595654\n",
      "[9/2500]D loss:1.63157 G loss:1.10751真判真:0.642011 假判真:0.635668/0.351539\n",
      "[9/2500]D loss:1.62827 G loss:0.445865真判真:0.346488 假判真:0.372608/0.661927\n",
      "[9/2500]D loss:1.60522 G loss:1.10596真判真:0.688202 假判真:0.670376/0.343308\n",
      "[9/2500]D loss:1.5232 G loss:0.698884真判真:0.353063 假判真:0.353622/0.549326\n",
      "[9/2500]D loss:1.51314 G loss:0.495814真判真:0.540528 假判真:0.536548/0.625649\n",
      "[9/2500]D loss:1.61463 G loss:1.24631真判真:0.62098 假判真:0.623984/0.291648\n",
      "[9/2500]D loss:1.59826 G loss:0.562303真判真:0.294704 假判真:0.29774/0.572768\n",
      "[9/2500]D loss:1.42279 G loss:0.626232真判真:0.574315 假判真:0.574938/0.537169\n",
      "[9/2500]D loss:1.39117 G loss:0.783436真判真:0.536107 假判真:0.530265/0.460119\n",
      "[9/2500]D loss:1.40754 G loss:0.717079真判真:0.457611 假判真:0.459331/0.489765\n",
      "[9/2500]D loss:1.38082 G loss:0.634776真判真:0.497055 假判真:0.49187/0.531936\n",
      "[9/2500]D loss:1.40004 G loss:0.723989真判真:0.521763 假判真:0.524237/0.486746\n",
      "[9/2500]D loss:1.40308 G loss:0.717749真判真:0.481593 假判真:0.486495/0.489847\n",
      "[9/2500]D loss:1.38784 G loss:0.677899真判真:0.493981 假判真:0.491959/0.508613\n",
      "[9/2500]D loss:1.39967 G loss:0.78047真判真:0.52409 假判真:0.529084/0.461148\n",
      "[10/2500]D loss:1.39948 G loss:0.686904真判真:0.46133 假判真:0.462972/0.505112\n",
      "[10/2500]D loss:1.41568 G loss:0.641087真判真:0.501093 假判真:0.510891/0.52825\n",
      "[10/2500]D loss:1.40048 G loss:0.785106真判真:0.529642 假判真:0.532205/0.456534\n",
      "[10/2500]D loss:1.39594 G loss:0.665564真判真:0.456497 假判真:0.45645/0.514651\n",
      "[10/2500]D loss:1.38198 G loss:0.694235真判真:0.521663 假判真:0.517516/0.500503\n",
      "[10/2500]D loss:1.37411 G loss:0.725915真判真:0.504042 假判真:0.496843/0.485452\n",
      "[10/2500]D loss:1.3833 G loss:0.652026真判真:0.48776 假判真:0.483407/0.522444\n",
      "[10/2500]D loss:1.3987 G loss:0.68142真判真:0.515778 假判真:0.519359/0.506672\n",
      "[10/2500]D loss:1.40616 G loss:0.710409真判真:0.502094 假判真:0.510333/0.491989\n",
      "[10/2500]D loss:1.38529 G loss:0.709249真判真:0.493233 假判真:0.491641/0.492731\n",
      "[10/2500]D loss:1.37862 G loss:0.694036真判真:0.500027 假判真:0.494977/0.500672\n",
      "[10/2500]D loss:1.39782 G loss:0.726764真判真:0.50273 假判真:0.50653/0.484238\n",
      "[10/2500]D loss:1.38801 G loss:0.676495真判真:0.487419 假判真:0.486559/0.509034\n",
      "[10/2500]D loss:1.38977 G loss:0.690401真判真:0.509342 假判真:0.508998/0.502088\n",
      "[10/2500]D loss:1.41986 G loss:0.771686真判真:0.513112 假判真:0.52823/0.462721\n",
      "[11/2500]D loss:1.38908 G loss:0.618777真判真:0.457189 假判真:0.453557/0.543804\n",
      "[11/2500]D loss:1.43418 G loss:0.80193真判真:0.544702 假判真:0.555004/0.449603\n",
      "[11/2500]D loss:1.40001 G loss:0.618295真判真:0.447598 假判真:0.446641/0.541047\n",
      "[11/2500]D loss:1.38995 G loss:0.743267真判真:0.543183 假判真:0.536912/0.476478\n",
      "[11/2500]D loss:1.39932 G loss:0.712819真判真:0.475067 假判真:0.478866/0.491308\n",
      "[11/2500]D loss:1.38997 G loss:0.678215真判真:0.491691 假判真:0.491425/0.510621\n",
      "[11/2500]D loss:1.38464 G loss:0.743106真判真:0.517911 假判真:0.513356/0.477191\n",
      "[11/2500]D loss:1.39818 G loss:0.694758真判真:0.479799 假判真:0.483295/0.500164\n",
      "[11/2500]D loss:1.37974 G loss:0.705019真判真:0.50396 假判真:0.498917/0.495449\n",
      "[11/2500]D loss:1.4035 G loss:0.763051真判真:0.499254 假判真:0.505323/0.467043\n",
      "[11/2500]D loss:1.39423 G loss:0.637705真判真:0.468868 假判真:0.468896/0.529496\n",
      "[11/2500]D loss:1.39766 G loss:0.763934真判真:0.528998 假判真:0.530338/0.467022\n",
      "[11/2500]D loss:1.40024 G loss:0.619433真判真:0.461067 假判真:0.462988/0.539408\n",
      "[11/2500]D loss:1.38704 G loss:0.763027真判真:0.543624 假判真:0.538451/0.469665\n",
      "[11/2500]D loss:1.39031 G loss:0.752007真判真:0.477292 假判真:0.473998/0.473538\n",
      "[12/2500]D loss:1.3997 G loss:0.597103真判真:0.481674 假判真:0.480628/0.552189\n",
      "[12/2500]D loss:1.39693 G loss:0.869945真判真:0.560069 假判真:0.554685/0.420749\n",
      "[12/2500]D loss:1.4377 G loss:0.532184真判真:0.420612 假判真:0.430965/0.58922\n",
      "[12/2500]D loss:1.42141 G loss:0.907462真判真:0.591986 假判真:0.588474/0.404677\n",
      "[12/2500]D loss:1.43232 G loss:0.539393真判真:0.401452 假判真:0.40284/0.584591\n",
      "[12/2500]D loss:1.40404 G loss:0.792069真判真:0.586094 假判真:0.577313/0.453883\n",
      "[12/2500]D loss:1.39708 G loss:0.677617真判真:0.458658 假判真:0.459267/0.50883\n",
      "[12/2500]D loss:1.38159 G loss:0.645632真判真:0.513886 假判真:0.508795/0.525764\n",
      "[12/2500]D loss:1.37778 G loss:0.767696真判真:0.521328 假判真:0.512919/0.46789\n",
      "[12/2500]D loss:1.3932 G loss:0.668223真判真:0.474015 假判真:0.471027/0.514787\n",
      "[12/2500]D loss:1.3759 G loss:0.726979真判真:0.53232 假判真:0.520513/0.485446\n",
      "[12/2500]D loss:1.39442 G loss:0.715942真判真:0.488422 假判真:0.48834/0.490847\n",
      "[12/2500]D loss:1.37337 G loss:0.707393真判真:0.502468 假判真:0.49217/0.495884\n",
      "[12/2500]D loss:1.36283 G loss:0.699202真判真:0.507078 假判真:0.490632/0.50147\n",
      "[12/2500]D loss:1.38016 G loss:0.913481真判真:0.496216 假判真:0.485056/0.479002\n",
      "[13/2500]D loss:1.59228 G loss:0.568681真判真:0.492037 假判真:0.49237/0.581797\n",
      "[13/2500]D loss:1.59678 G loss:1.20858真判真:0.582745 假判真:0.587147/0.307235\n",
      "[13/2500]D loss:1.56436 G loss:0.34678真判真:0.310523 假判真:0.299715/0.713061\n",
      "[13/2500]D loss:1.65653 G loss:1.36495真判真:0.72229 假判真:0.718278/0.309492\n",
      "[13/2500]D loss:1.64629 G loss:0.581119真判真:0.337322 假判真:0.312803/0.63576\n",
      "[13/2500]D loss:1.84859 G loss:1.27004真判真:0.685447 假判真:0.663221/0.322213\n",
      "[13/2500]D loss:1.55826 G loss:0.367131真判真:0.352362 假判真:0.318431/0.70249\n",
      "[13/2500]D loss:1.68288 G loss:1.3938真判真:0.662329 假判真:0.687228/0.259659\n",
      "[13/2500]D loss:1.68034 G loss:0.297167真判真:0.269663 假判真:0.261868/0.745955\n",
      "[13/2500]D loss:1.72369 G loss:1.15252真判真:0.743907 假判真:0.748686/0.323499\n",
      "[13/2500]D loss:1.52768 G loss:0.764052真判真:0.326517 假判真:0.320855/0.484152\n",
      "[13/2500]D loss:1.58453 G loss:0.605428真判真:0.490673 假判真:0.495541/0.546967\n",
      "[13/2500]D loss:1.39077 G loss:0.62993真判真:0.549111 假判真:0.544562/0.533455\n",
      "[13/2500]D loss:1.39038 G loss:0.824956真判真:0.535481 假判真:0.533052/0.439309\n",
      "[13/2500]D loss:1.38224 G loss:0.906908真判真:0.455845 假判真:0.443001/0.418136\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-cdecea7dffe8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m     \u001b[1;31m# 可视化，每 epoch 更新\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m     viz.line(\n\u001b[0m\u001b[0;32m    113\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\cwdbo\\anaconda3\\envs\\torchlearn\\lib\\site-packages\\visdom\\__init__.py\u001b[0m in \u001b[0;36mwrapped_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    387\u001b[0m         \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0m_to_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    388\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0m_to_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 389\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    390\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    391\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwrapped_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\cwdbo\\anaconda3\\envs\\torchlearn\\lib\\site-packages\\visdom\\__init__.py\u001b[0m in \u001b[0;36mline\u001b[1;34m(self, Y, X, win, env, opts, update, name)\u001b[0m\n\u001b[0;32m   1712\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'F'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1713\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1714\u001b[1;33m         return self.scatter(X=linedata, Y=labels, opts=opts, win=win, env=env,\n\u001b[0m\u001b[0;32m   1715\u001b[0m                             update=update, name=name)\n\u001b[0;32m   1716\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\cwdbo\\anaconda3\\envs\\torchlearn\\lib\\site-packages\\visdom\\__init__.py\u001b[0m in \u001b[0;36mwrapped_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    387\u001b[0m         \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0m_to_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    388\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0m_to_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 389\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    390\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    391\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwrapped_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\cwdbo\\anaconda3\\envs\\torchlearn\\lib\\site-packages\\visdom\\__init__.py\u001b[0m in \u001b[0;36mscatter\u001b[1;34m(self, X, Y, win, env, opts, update, name)\u001b[0m\n\u001b[0;32m   1490\u001b[0m                     \u001b[0mupdate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1491\u001b[0m                 \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moffline\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1492\u001b[1;33m                     \u001b[0mexists\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwin_exists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1493\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mexists\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1494\u001b[0m                         \u001b[0mupdate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\cwdbo\\anaconda3\\envs\\torchlearn\\lib\\site-packages\\visdom\\__init__.py\u001b[0m in \u001b[0;36mwin_exists\u001b[1;34m(self, win, env)\u001b[0m\n\u001b[0;32m    829\u001b[0m         \"\"\"\n\u001b[0;32m    830\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 831\u001b[1;33m             \u001b[0me\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_win_exists_wrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    832\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    833\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Error connecting to Visdom server!\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\cwdbo\\anaconda3\\envs\\torchlearn\\lib\\site-packages\\visdom\\__init__.py\u001b[0m in \u001b[0;36m_win_exists_wrap\u001b[1;34m(self, win, env)\u001b[0m\n\u001b[0;32m    810\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mwin\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    811\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 812\u001b[1;33m         return self._send({\n\u001b[0m\u001b[0;32m    813\u001b[0m             \u001b[1;34m'win'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mwin\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    814\u001b[0m             \u001b[1;34m'eid'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\cwdbo\\anaconda3\\envs\\torchlearn\\lib\\site-packages\\visdom\\__init__.py\u001b[0m in \u001b[0;36m_send\u001b[1;34m(self, msg, endpoint, quiet, from_log, create)\u001b[0m\n\u001b[0;32m    706\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    707\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 708\u001b[1;33m             return self._handle_post(\n\u001b[0m\u001b[0;32m    709\u001b[0m                 \"{0}:{1}{2}/{3}\".format(self.server, self.port,\n\u001b[0;32m    710\u001b[0m                                         self.base_url, endpoint),\n",
      "\u001b[1;32mc:\\Users\\cwdbo\\anaconda3\\envs\\torchlearn\\lib\\site-packages\\visdom\\__init__.py\u001b[0m in \u001b[0;36m_handle_post\u001b[1;34m(self, url, data)\u001b[0m\n\u001b[0;32m    675\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    676\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 677\u001b[1;33m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    678\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\cwdbo\\anaconda3\\envs\\torchlearn\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mpost\u001b[1;34m(self, url, data, json, **kwargs)\u001b[0m\n\u001b[0;32m    588\u001b[0m         \"\"\"\n\u001b[0;32m    589\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 590\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'POST'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    591\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    592\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\cwdbo\\anaconda3\\envs\\torchlearn\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    540\u001b[0m         }\n\u001b[0;32m    541\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 542\u001b[1;33m         \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    543\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\cwdbo\\anaconda3\\envs\\torchlearn\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    653\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    654\u001b[0m         \u001b[1;31m# Send the request\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 655\u001b[1;33m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    656\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m         \u001b[1;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\cwdbo\\anaconda3\\envs\\torchlearn\\lib\\site-packages\\requests\\adapters.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    437\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    438\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mchunked\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 439\u001b[1;33m                 resp = conn.urlopen(\n\u001b[0m\u001b[0;32m    440\u001b[0m                     \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    441\u001b[0m                     \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\cwdbo\\anaconda3\\envs\\torchlearn\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    697\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    698\u001b[0m             \u001b[1;31m# Make the request on the httplib connection object.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 699\u001b[1;33m             httplib_response = self._make_request(\n\u001b[0m\u001b[0;32m    700\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\cwdbo\\anaconda3\\envs\\torchlearn\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    443\u001b[0m                     \u001b[1;31m# Python 3 (including for exceptions like SystemExit).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    444\u001b[0m                     \u001b[1;31m# Otherwise it looks like a bug in the code.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 445\u001b[1;33m                     \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    446\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSocketError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_raise_timeout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\cwdbo\\anaconda3\\envs\\torchlearn\\lib\\site-packages\\urllib3\\packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\cwdbo\\anaconda3\\envs\\torchlearn\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    438\u001b[0m                 \u001b[1;31m# Python 3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m                     \u001b[0mhttplib_response\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    441\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m                     \u001b[1;31m# Remove the TypeError from the exception chain in\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\cwdbo\\anaconda3\\envs\\torchlearn\\lib\\http\\client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1347\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1348\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1349\u001b[1;33m                 \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1350\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1351\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\cwdbo\\anaconda3\\envs\\torchlearn\\lib\\http\\client.py\u001b[0m in \u001b[0;36mbegin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    314\u001b[0m         \u001b[1;31m# read until we get a non-100 response\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    315\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 316\u001b[1;33m             \u001b[0mversion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    317\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    318\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\cwdbo\\anaconda3\\envs\\torchlearn\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    276\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 277\u001b[1;33m         \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"iso-8859-1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    278\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    279\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"status line\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\cwdbo\\anaconda3\\envs\\torchlearn\\lib\\socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    702\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    703\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 704\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    705\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    706\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# GAN 训练,120\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=128)\n",
    "# 损失\n",
    "criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "# criterion = F.cross_entropy()\n",
    "# 优化器\n",
    "goptimizer = torch.optim.Adam(gnet.parameters(), lr=0.0008 * 1.5, betas=(0.5, 0.999))\n",
    "doptimizer = torch.optim.Adam(dnet.parameters(), lr=0.4 * 1, betas=(0.5, 0.999))\n",
    "g_lr_schedule = torch.optim.lr_scheduler.StepLR(\n",
    "    goptimizer, 1000, gamma=0.95, last_epoch=-1\n",
    ")\n",
    "d_lr_schedule = torch.optim.lr_scheduler.StepLR(\n",
    "    doptimizer, 1000, gamma=0.95, last_epoch=-1\n",
    ")\n",
    "# 用于测试的固定噪声,用来查看相同的潜在张量在训练过程中生成图片的变换\n",
    "# batch_size = 512\n",
    "# fixed_noises = torch.randn(batch_size, latent_size, 1, 1).to(device)\n",
    "# 初始化 visdom\n",
    "viz.close()\n",
    "viz = viz_init()\n",
    "viz = Visdom(\n",
    "    env=viz_acnt.evns, log_to_filename=vislogDir + \"vislog_\" + get_current_time()\n",
    ")\n",
    "vizx = 0\n",
    "# viz.text(\n",
    "#     \"MONITOR: Show train process~~\",\n",
    "#     win=\"Monitor\",\n",
    "#     opts={\n",
    "#         \"title\": \"ProcessMonitor\",\n",
    "#     },\n",
    "# )\n",
    "# AUC值\n",
    "auc_current = 0\n",
    "auc_max = 0\n",
    "auc_current_te = 0\n",
    "auc_max_te = 0\n",
    "# 训练过程\n",
    "epoch_num = 2500\n",
    "for epoch in range(epoch_num):\n",
    "    # viz.text(\n",
    "    #     \"ep\" + str(epoch + 1) + \" start\",\n",
    "    #     win=\"Monitor\",\n",
    "    #     opts={\n",
    "    #         \"title\": \"ProcessMonitor\",\n",
    "    #     },\n",
    "    # )\n",
    "    dnet.to(device)\n",
    "    for batch in train_loader:\n",
    "        # 载入本批次数据\n",
    "        # real_images, _ = data\n",
    "        # viz.text('load data',win='Monitor',append=True,opts = {'title':'ProcessMonitor',},)\n",
    "        rimages, rlabels = batch\n",
    "        rimages = rimages.to(device)\n",
    "        rlabels = rlabels.to(device)\n",
    "        rimages = rimages.to(torch.float32)\n",
    "        rlabels = rlabels.long()\n",
    "        batch_size = rimages.size(0)\n",
    "        # batch_size = 1344\n",
    "        # viz.text('training',win='Monitor',append=True,opts = {'title':'ProcessMonitor',},)\n",
    "        # 训练鉴别网络\n",
    "        dnet.train()\n",
    "        gnet.train()\n",
    "        labels = torch.ones(batch_size)  # 真实数据对应标签为1\n",
    "        labels = labels.to(device)\n",
    "        preds = dnet(rimages)  # 对真实数据进行判别\n",
    "        preds = preds.reshape(-1)\n",
    "\n",
    "        dloss_real = criterion(preds, labels)  # 真实数据的鉴别器损失\n",
    "        dmean_real = preds.sigmoid().mean()\n",
    "        # 计算鉴别器将多少比例的真数据判定为真,仅用于输出显示\n",
    "\n",
    "        noises = torch.randn(batch_size, latent_size, 1, 1)  # 潜在噪声\n",
    "        noises = noises.to(device)\n",
    "        fake_images = gnet(noises)  # 生成假数据\n",
    "        labels = torch.zeros(batch_size)  # 假数据对应标签为0\n",
    "        labels = labels.to(device)\n",
    "        fake = fake_images.detach()\n",
    "        # 使得梯度的计算不回溯到生成网络,可用于加快训练速度.删去此步结果不变\n",
    "        preds = dnet(fake)  # 对假数据进行鉴别\n",
    "        preds = preds.view(-1)\n",
    "        dloss_fake = criterion(preds, labels)  # 假数据的鉴别器损失\n",
    "        dmean_fake = preds.sigmoid().mean()\n",
    "        # 计算鉴别器将多少比例的假数据判定为真,仅用于输出显示\n",
    "\n",
    "        dloss = dloss_real + dloss_fake  # 总的鉴别器损失\n",
    "        dnet.zero_grad()\n",
    "        dloss.backward()\n",
    "        doptimizer.step()\n",
    "        # now_dloss += dloss.item()\n",
    "        # 训练生成网络\n",
    "        # viz.text(' Generator training',win='Monitor',append=True,opts = {'title':'ProcessMonitor',},)\n",
    "        labels = torch.ones(batch_size)\n",
    "        labels = labels.to(device)\n",
    "        # 生成网络希望所有生成的数据都被认为是真数据\n",
    "        preds = dnet(fake_images)  # 把假数据通过鉴别网络\n",
    "        preds = preds.view(-1)\n",
    "        gloss = criterion(preds, labels)  # 真数据看到的损失\n",
    "        gmean_fake = preds.sigmoid().mean()\n",
    "        # 计算鉴别器将多少比例的假数据判定为真,仅用于输出显示\n",
    "        gnet.zero_grad()\n",
    "        gloss.backward()\n",
    "        goptimizer.step()\n",
    "\n",
    "        # viz.text('train Generator',win='Monitor',append=True,opts = {'title':'ProcessMonitor',},)\n",
    "        print(\n",
    "            \"[{}/{}]\".format(epoch + 1, epoch_num)\n",
    "            + \"D loss:{:g} G loss:{:g}\".format(dloss, gloss)\n",
    "            + \"真判真:{:g} 假判真:{:g}/{:g}\".format(dmean_real, dmean_fake, gmean_fake)\n",
    "        )\n",
    "\n",
    "    # 可视化，每 epoch 更新\n",
    "    viz.line(\n",
    "        [float(gloss)],\n",
    "        [epoch],\n",
    "        win=\"loss_perEpoch\",\n",
    "        name=\"G_loss\",\n",
    "        update=\"append\",\n",
    "        opts=dict(title=\"loss_perEpoch\", xlabel=\"epoch\", ylabel=\"loss\"),\n",
    "    )\n",
    "    viz.line(\n",
    "        [float(dloss)], [epoch], win=\"loss_perEpoch\", name=\"D_loss\", update=\"append\"\n",
    "    )\n",
    "    viz.line(\n",
    "        [float(goptimizer.state_dict()[\"param_groups\"][0][\"lr\"])],\n",
    "        [epoch],\n",
    "        win=\"lr_perEpoch\",\n",
    "        name=\"G_loss\",\n",
    "        update=\"append\",\n",
    "        opts=dict(title=\"lr_perEpoch\", xlabel=\"epoch\", ylabel=\"lr\"),\n",
    "    )\n",
    "    viz.line(\n",
    "        [float(doptimizer.state_dict()[\"param_groups\"][0][\"lr\"])],\n",
    "        [epoch],\n",
    "        win=\"lr_perEpoch\",\n",
    "        name=\"D_loss\",\n",
    "        update=\"append\",\n",
    "    )\n",
    "    # viz.line(\n",
    "    #     [float(dmean_real)],\n",
    "    #     [epoch],\n",
    "    #     win=\"D real_perEpoch\",\n",
    "    #     update=\"append\",\n",
    "    #     opts=dict(title=\"D real真判真_perEpoch\", xlabel=\"epoch\"),\n",
    "    # )\n",
    "    viz.line(\n",
    "        [float(gmean_fake)],\n",
    "        [epoch],\n",
    "        win=\"D fake_perEpoch\",\n",
    "        update=\"append\",\n",
    "        opts=dict(title=\"D fake假判真_perEpoch\", xlabel=\"epoch\"),\n",
    "    )\n",
    "    # 更新学习率\n",
    "    g_lr_schedule.step()\n",
    "    d_lr_schedule.step()\n",
    "    viz.text(\n",
    "        \"updating weights\",\n",
    "        win=\"Monitor\",\n",
    "        append=True,\n",
    "        opts={\n",
    "            \"title\": \"ProcessMonitor\",\n",
    "        },\n",
    "    )\n",
    "    # 定期保存\n",
    "    if (epoch + 1) % 500 == 0:\n",
    "        timeForSave = datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "        checkpointPath_g = (\n",
    "            ckpDir + \"g_ep_\" + str(epoch + 1) + \"_\" + timeForSave + \".pth\"\n",
    "        )\n",
    "        checkpointPath_d = (\n",
    "            ckpDir + \"d_ep_\" + str(epoch + 1) + \"_\" + timeForSave + \".pth\"\n",
    "        )\n",
    "        d_state = {\n",
    "            \"model\": dnet.state_dict(),\n",
    "            \"optimizer\": doptimizer.state_dict(),\n",
    "            \"epoch\": epoch,\n",
    "        }\n",
    "        torch.save(d_state, checkpointPath_d)\n",
    "        g_state = {\n",
    "            \"model\": gnet.state_dict(),\n",
    "            \"optimizer\": goptimizer.state_dict(),\n",
    "            \"epoch\": epoch,\n",
    "        }\n",
    "        torch.save(g_state, checkpointPath_g)\n",
    "        viz.text(\n",
    "            \"epoch \" + str(epoch + 1) + \" model saved\",\n",
    "            win=\"Monitor\",\n",
    "            append=True,\n",
    "            opts={\n",
    "                \"title\": \"ProcessMonitor\",\n",
    "            },\n",
    "        )\n",
    "\n",
    "    #     计算验证集上的 AUC 值\n",
    "    viz.text(\n",
    "        \"calculate auc start\",\n",
    "        win=\"Monitor\",\n",
    "        opts={\n",
    "            \"title\": \"ProcessMonitor\",\n",
    "        },\n",
    "    )\n",
    "    dnet.eval()\n",
    "    dnet.to(\"cpu\")\n",
    "    emg_vec = emgdata_to_net_preds_sigmoid(data_set=val_set, net_vector=dnet)\n",
    "    emg_vec = emg_vec.ravel()\n",
    "    auc_current = metrics.roc_auc_score(vallabel_for_auc, emg_vec, average=\"micro\")\n",
    "    # ceshiji\n",
    "    emg_vec_te = emgdata_to_net_preds_sigmoid(data_set=test_set, net_vector=dnet)\n",
    "    emg_vec_te = emg_vec_te.ravel()\n",
    "    auc_current_te = metrics.roc_auc_score(testlabel_for_auc, emg_vec_te, average=\"micro\")\n",
    "\n",
    "    viz.line(\n",
    "        [auc_current],\n",
    "        [epoch],\n",
    "        win=\"AUC\",\n",
    "        update=\"append\",\n",
    "        name=\"valiate\",\n",
    "        opts=dict(title=\"AUC\", xlabel=\"epoch\", ylabel=\"AUC\"),\n",
    "    )\n",
    "        \n",
    "    #     保存策略：整个训练过程中最大的AUC及所有AUC值超过0.8的对应模型，由于最大值源于实时比较，刚开始会有一部分低AUC模型被保存，\n",
    "    #     无需在意，过后删去即可\n",
    "    if auc_current > auc_max:\n",
    "        auc_max = auc_current\n",
    "        viz.text(\n",
    "            \"max auc: \" + str(auc_max) + \", in ep \" + str(epoch + 1),\n",
    "            win=\"message\",\n",
    "            append=False,\n",
    "            opts={\n",
    "                \"title\": \"Max AUC\",\n",
    "            },\n",
    "        )\n",
    "                # ceshiji\n",
    "        emg_vec_te = emgdata_to_net_preds_sigmoid(data_set=test_set, net_vector=dnet)\n",
    "        emg_vec_te = emg_vec_te.ravel()\n",
    "        auc_current_te = metrics.roc_auc_score(testlabel_for_auc, emg_vec_te, average=\"micro\")\n",
    "        auc_max_te = auc_current_te\n",
    "        viz.text(\n",
    "            \"max auc_te: \" + str(auc_max_te) + \", in ep \" + str(epoch + 1),\n",
    "            win=\"message_te\",\n",
    "            append=False,\n",
    "            opts={\n",
    "                \"title\": \"Max AUC_te\",\n",
    "            },\n",
    "        )\n",
    "        max_auc_state = {\n",
    "            \"model\": dnet.state_dict(),\n",
    "            \"optimizer\": doptimizer.state_dict(),\n",
    "            \"epoch\": epoch,\n",
    "        }\n",
    "        checkpointPath_auc = (\n",
    "            ckpDir_auc\n",
    "            + \"auc\"\n",
    "            + str(int(auc_max * 1000))\n",
    "            + \"d_ep_\"\n",
    "            + str(epoch + 1)\n",
    "            + \"_\"\n",
    "            + timeForSave\n",
    "            + \".pth\"\n",
    "        )\n",
    "        torch.save(max_auc_state, checkpointPath_auc)\n",
    "        viz.text(\n",
    "            \"max auc: \"\n",
    "            + str(auc_max)\n",
    "            + \", in ep \"\n",
    "            + str(epoch + 1)\n",
    "            + \" saved at \"\n",
    "            + checkpointPath_auc,\n",
    "            win=\"message\",\n",
    "            append=True,\n",
    "            opts={\n",
    "                \"title\": \"Max AUC\",\n",
    "            },\n",
    "        )\n",
    "    elif auc_current >= 0.8:\n",
    "        checkpointPath_auc = (\n",
    "            ckpDir_auc\n",
    "            + \"auc\"\n",
    "            + str(int(auc_max * 1000))\n",
    "            + \"d_ep_\"\n",
    "            + str(epoch + 1)\n",
    "            + \"_\"\n",
    "            + timeForSave\n",
    "            + \".pth\"\n",
    "        )\n",
    "        torch.save(max_auc_state, checkpointPath_auc)\n",
    "        viz.text(\n",
    "            \"auc: \"\n",
    "            + str(auc_current)\n",
    "            + \", in ep \"\n",
    "            + str(epoch + 1)\n",
    "            + \" saved at \"\n",
    "            + checkpointPath_auc,\n",
    "            win=\"AUC_Monitor\",\n",
    "            append=True,\n",
    "            opts={\n",
    "                \"title\": \"Max AUC\",\n",
    "            },\n",
    "        )\n",
    "\n",
    "\n",
    "# 完整训练结束后，模型最终保存；意义不大，但以备不时之需\n",
    "checkpointPath_model = (\n",
    "    model_Dir\n",
    "    + \"g_final_\"\n",
    "    + \"acc\"\n",
    "    + str(int(total_test_acc * 10000))\n",
    "    + \"_\"\n",
    "    + timeForSave\n",
    "    + \".pth\"\n",
    ")\n",
    "torch.save(gnet.state_dict(), checkpointPath_model)\n",
    "checkpointPath_model = (\n",
    "    model_Dir\n",
    "    + \"d_final_\"\n",
    "    + \"acc\"\n",
    "    + str(int(total_test_acc * 10000))\n",
    "    + \"_\"\n",
    "    + timeForSave\n",
    "    + \".pth\"\n",
    ")\n",
    "torch.save(dnet.state_dict(), checkpointPath_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 验证结果\n",
    "绘制 ROC 曲线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '..//ckp//opg_0217a//auc//auc880d_ep_359_2022_02_28_09_16_29.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_25898/3824776269.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 加载挑出的模型，计算 ROC 曲线、最佳阈值及 AUC 值\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m checkpoint_net_fe = torch.load(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;34m\"..//ckp//opg_0217a//auc//auc880d_ep_359_2022_02_28_09_16_29.pth\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m )\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# net_fe = GAN_Dis_opengan_alpha(n_channel, n_d_feature)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ZP/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    595\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ZP/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ZP/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '..//ckp//opg_0217a//auc//auc880d_ep_359_2022_02_28_09_16_29.pth'"
     ]
    }
   ],
   "source": [
    "# 加载挑出的模型，计算 ROC 曲线、最佳阈值及 AUC 值\n",
    "checkpoint_net_fe = torch.load(\n",
    "    \"..//ckp//opg_0217a//auc//auc880d_ep_359_2022_02_28_09_16_29.pth\"\n",
    ")\n",
    "# net_fe = GAN_Dis_opengan_alpha(n_channel, n_d_feature)\n",
    "net_fe = dnet\n",
    "net_fe.to(\"cpu\")\n",
    "net_fe.load_state_dict(checkpoint_net_fe[\"model\"])\n",
    "net_fe.eval()\n",
    "# 计算模型对数据集的预测值，此函数还会通过sigmod函数以保持输出在0-1内\n",
    "emg_vec = emgdata_to_net_preds_sigmoid(data_set=val_set, net_vector=dnet)\n",
    "# 用来计算ROC,AUC 的数据需要同格式，直接全部展平至一维\n",
    "emg_vec = emg_vec.ravel()\n",
    "vallabel_for_auc = vallabel_for_auc.ravel()\n",
    "auc_test = metrics.roc_auc_score(vallabel_for_auc, emg_vec, average=\"micro\")\n",
    "# 计算roc曲线，以备后续绘制\n",
    "fpr, tpr, thresholds = metrics.roc_curve(vallabel_for_auc, emg_vec)\n",
    "auc_test_r = metrics.auc(fpr, tpr)\n",
    "# 按约登指数计算最佳阈值点\n",
    "optimals, points = Find_Optimal_Cutoff(tpr, fpr, thresholds)\n",
    "# 作ROC图\n",
    "plt.rcParams[\"figure.dpi\"] = 150\n",
    "plt.title(\"ROC(AUC:\" + str(\"%.5g\" % auc_test_r) + \")\")\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.plot(fpr, tpr, marker=\".\", label=\"ROC\")\n",
    "plt.plot(points[0], points[1], marker=\"*\", markersize=18, label=\"optimal cutoff\")\n",
    "# 在 ROC 图上标注最佳阈值点\n",
    "plt.annotate(\n",
    "    str(\"%.5g\" % points[0]) + \",\" + str(\"%.5g\" % points[1]),\n",
    "    xy=(points[0], points[1]),\n",
    "    xytext=(10, -20),\n",
    "    textcoords=\"offset points\",\n",
    "    bbox=dict(boxstyle=\"round,pad=0.5\", fc=\"yellow\", ec=\"k\", lw=1, alpha=0.5),\n",
    ")\n",
    "# 显示图例\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下一步应该让经过拒绝后的数据经过原始CNN分类器，再算一次准确率\n",
    "\n",
    "# 加载完整大数据集\n",
    "opg_data = np.load(\"../../data/OpgData_full_smr_10cl_220522_2.npy\", allow_pickle=True)\n",
    "opg_data = opg_data.item()\n",
    "Xtrain_known = opg_data[\"Xtrain_known\"]\n",
    "Xtest_known = opg_data[\"Xtest_known\"]\n",
    "Xval_opset = opg_data[\"Xval_opset\"]\n",
    "Ytrain_known = opg_data[\"Ytrain_known\"]\n",
    "Ytest_known = opg_data[\"Ytest_known\"]\n",
    "Yval_opset = opg_data[\"Yval_opset\"]\n",
    "X_train_Net_output = opg_data[\"X_train_Net_output\"]\n",
    "X_test_Net_output = opg_data[\"X_test_Net_output\"]\n",
    "X_val_Net_output = opg_data[\"X_val_Net_output\"]\n",
    "Yval_opset_mtv = opg_data[\"Yval_opset_mtv\"][:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 情形1，所有接受的样本中正确率变化\n",
    "# 加载CNN，过一遍验证集，中间加入 GAN 拒绝操作\n",
    "checkpoint_net_cnn = torch.load('..//ckp//opg_0217a//oth//c_ep_200_acc9763_2022_02_26_23_18_44.pth')\n",
    "net_cnn = Network_CNN_6ch_6cls_smr_v1()\n",
    "net_cnn.load_state_dict(checkpoint_net_cnn['model'])\n",
    "net_cnn.eval()\n",
    "# net_cnn.to(device)\n",
    "total_testnum = 0\n",
    "testdata = Xtest_known\n",
    "testlabel = Ytest_known[:,0]\n",
    "test_set = EMGDataset_2D(testdata, testlabel)\n",
    "test_val_on_cnn_set = EMGDataset_2D(Xval_opset, Yval_opset_mtv)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=2, shuffle=False)\n",
    "test_val_on_cnn_loader = torch.utils.data.DataLoader(test_val_on_cnn_set, batch_size=2, shuffle=False)\n",
    "batch1 = iter(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: torch.Size([2, 1, 200, 6]) tensor([[[[ 0.0195,  0.0015,  0.0049, -0.0190,  0.0638,  0.0233],\n",
      "          [ 0.0187, -0.0027,  0.0121,  0.0054,  0.0390,  0.0358],\n",
      "          [ 0.0098,  0.0011,  0.0121,  0.0240,  0.0028,  0.0296],\n",
      "          ...,\n",
      "          [ 0.0160,  0.0087,  0.0680, -0.0711,  0.0055,  0.0085],\n",
      "          [-0.0048,  0.0223,  0.0405, -0.0837,  0.0356,  0.0141],\n",
      "          [ 0.0116,  0.0266,  0.0162, -0.1236,  0.0647,  0.0280]]],\n",
      "\n",
      "\n",
      "        [[[-0.0173, -0.0149, -0.0623, -0.0209, -0.0621, -0.0923],\n",
      "          [-0.0231, -0.0156, -0.0641,  0.0186, -0.0390, -0.0488],\n",
      "          [-0.0206, -0.0148, -0.0535,  0.0628, -0.0144, -0.0224],\n",
      "          ...,\n",
      "          [-0.0247, -0.0351,  0.1067,  0.0336,  0.0373, -0.0140],\n",
      "          [-0.0192, -0.0440,  0.1082,  0.0433,  0.0340, -0.0670],\n",
      "          [-0.0185, -0.0384,  0.1060,  0.0619,  0.0261, -0.0861]]]],\n",
      "       dtype=torch.float64) \n",
      " y: torch.Size([2]) tensor([0, 0], dtype=torch.int16)\n"
     ]
    }
   ],
   "source": [
    "batch2 = next(batch1)\n",
    "d_x, d_y = batch2\n",
    "print(\n",
    "    \"x:\",\n",
    "    d_x.shape,\n",
    "    d_x,\n",
    "    \"\\n\",\n",
    "    \"y:\",\n",
    "    d_y.shape,\n",
    "    d_y,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 原始手势测试集，经过 CNN 后正确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin_acc: 0.976313640076232 \n",
      " opset acc: 0.5765273311897106\n"
     ]
    }
   ],
   "source": [
    "# 原始手势测试集，经过 CNN 后正确率\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=128, shuffle=False)\n",
    "test_val_on_cnn_loader = torch.utils.data.DataLoader(\n",
    "    test_val_on_cnn_set, batch_size=128, shuffle=False\n",
    ")\n",
    "# 是的，目前并没有必要使用 **不定参数，这只是一个练习，函数本身在reuse.py中\n",
    "kwargs = {\"device\": \"cuda:0\", \"data_set\": test_loader, \"network\": net_cnn}\n",
    "# 原始准确率\n",
    "origin_acc = test_network_accuracy_xy_only(**kwargs)\n",
    "kwargs = {\"device\": \"cuda:0\", \"data_set\": test_val_on_cnn_loader, \"network\": net_cnn}\n",
    "# 在包含未知类的验证集上的准确率\n",
    "opset_acc = test_network_accuracy_xy_only(**kwargs)\n",
    "print(\"origin_acc:\", origin_acc, \"\\n\", \"opset acc:\", opset_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 被接受的数据中，准确率是否生变"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Flatten(start_dim=1, end_dim=-1)\n",
       "  (1): Linear(in_features=6, out_features=128, bias=True)\n",
       "  (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (3): LeakyReLU(negative_slope=0.2)\n",
       "  (4): Dropout2d(p=0.2, inplace=False)\n",
       "  (5): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (6): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (7): LeakyReLU(negative_slope=0.2)\n",
       "  (8): Dropout2d(p=0.2, inplace=False)\n",
       "  (9): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 被接受的数据中，准确率是否生变\n",
    "# 加载判别器\n",
    "path_dnet = torch.load(\n",
    "    \"..//ckp//opg_0217a//auc//auc880d_ep_359_2022_02_28_09_16_29.pth\"\n",
    ")\n",
    "dnet_opset = GAN_Dis(n_channel = 6, n_d_feature = 64)\n",
    "dnet_opset.to(\"cpu\")\n",
    "dnet_opset.load_state_dict(path_dnet[\"model\"])\n",
    "dnet_opset.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算模型对数据集的预测值，此函数还会通过sigmod函数以保持输出在0-1内\n",
    "dnet_opset.to(\"cpu\")\n",
    "a_emg_vec = emgdata_to_net_preds_sigmoid(data_set=val_set, net_vector=dnet_opset)\n",
    "# 用来计算ROC,AUC 的数据需要同格式，直接全部展平至一维\n",
    "a_emg_vec = a_emg_vec.ravel()\n",
    "a_vallabel_for_auc = vallabel_for_auc.ravel()\n",
    "a_auc_test = metrics.roc_auc_score(vallabel_for_auc, emg_vec, average=\"micro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC before rejection: 0.5765273311897106 AER: 0.42347266881028944 \n",
      " ACC after rejection: 0.8107904642409034 AER: 0.1892095357590966\n"
     ]
    }
   ],
   "source": [
    "# 加载EMG原数据，让 CNN 判断，给出的特征值过一下判别器，通过的再计数\n",
    "# from collections import Counter\n",
    "test_val_on_cnn_loader = torch.utils.data.DataLoader(\n",
    "    test_val_on_cnn_set, batch_size=256, shuffle=False\n",
    ")\n",
    "optimal_cutoff = optimals\n",
    "total_test_correct_kn = 0\n",
    "total_testnum_kn = 0\n",
    "total_test_correct_un = 0\n",
    "total_testnum_un = 0\n",
    "a_vec_list = []\n",
    "a_vec_confidence_list = []\n",
    "net_cnn.to(device)\n",
    "dnet_opset.to(device)\n",
    "dnet_opset.eval()\n",
    "i = 0\n",
    "for testemgdatas, testemglabels in test_val_on_cnn_loader:\n",
    "    testemgdatas = testemgdatas.to(torch.float32)\n",
    "    testemgdatas = testemgdatas.to(device)\n",
    "    testemglabels = testemglabels.long()\n",
    "    testemglabels = testemglabels.to(device)\n",
    "    predstest = net_cnn(testemgdatas)\n",
    "    # 网络输出需要过一下 sigmoid ，我查了一天，果然是因为太久没看这个项目了\n",
    "    d_accept = dnet_opset(predstest).sigmoid()\n",
    "    # 下面代码，甚是妙极\n",
    "    aaa = d_accept >= optimal_cutoff\n",
    "    # d_reject = d_reject[aaa]\n",
    "    aaa_2 = aaa.squeeze()\n",
    "    predstest_2 = predstest[aaa_2, :]\n",
    "    preds_cpu = d_accept.to(\"cpu\")\n",
    "    a_vec_list.append((preds_cpu.detach().numpy().flatten()))\n",
    "    d_reject_label = testemglabels[aaa_2]\n",
    "    # 被接受的\n",
    "    curr_test_correct_kn = get_num_correct(predstest, testemglabels, dim=1)\n",
    "    total_testnum_kn += testemglabels.size(0)\n",
    "    total_test_correct_kn += curr_test_correct_kn\n",
    "    if aaa_2.sum() != 0:\n",
    "        curr_test_correct_un = get_num_correct(predstest_2, d_reject_label, dim=1)\n",
    "        total_testnum_un += d_reject_label.size(0)\n",
    "        total_test_correct_un += curr_test_correct_un\n",
    "# aaa_3 = Counter(a_vec_confidence_list)\n",
    "total_test_acc_kn = total_test_correct_kn / total_testnum_kn\n",
    "total_test_acc_un = total_test_correct_un / total_testnum_un\n",
    "print(\n",
    "    \"ACC before rejection:\",\n",
    "    total_test_acc_kn, \n",
    "    \"AER:\",1-total_test_acc_kn,\n",
    "    \"\\n\",\n",
    "    \"ACC after rejection:\",\n",
    "    total_test_acc_un,\n",
    "    \"AER:\",1-total_test_acc_un\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "696bc8a05a1eb808e62d18e9328544428937c8e11a4c703581cc2236b7d3d24e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 ('torchlearn')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
